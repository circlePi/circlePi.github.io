<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F01%2F17%2FTransformer%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%2F</url>
    <content type="text"><![CDATA[title:Transformer原理和实现，从入门到精通mathjax:Truedate:2019-01-17 23:30:08categories:自然语言处理 tags:TransformerTransformer原理和实现，从入门到精通概述所谓 ”工预善其事，必先利其器“， Bert之所以取得这么惊才绝艳的效果，很大一部分原因源自于Transformer。为了后面更好、更快地理解BERT模型，这一节从Transformer的开山鼻祖说起，先来跟着”Attention is All You Need[1]“ 这篇文章，走近transformer的世界，在这里你再也看不到熟悉的CNN、RNN的影子，取而代之的是，你将看到Attention机制是如何被发挥的淋漓尽致、妙至毫颠，以及它何以从一个为CNN、RNN跑龙套的配角实现华丽逆袭。对于Bert来说，transformer真可谓天纵神兵，出匣自鸣！ 看完本文，你大概能够： 掌握Encoder-Decoder框架 掌握残差网络 掌握BatchNormalization(批归一化)和LayerNormalization（层归一化） 掌握Position Embedding（位置编码） 当然，最重要的，你能了解Transformer的原理和代码实现。 Notes: 本文代码参考哈弗大学的The Annotated Transformer Encoder-Decoder框架Encoder-Decoder是为seq2seq（序列到序列）量身打造的一个深度学习框架，在机器翻译、机器问答等领域有着广泛的应用。这是一个抽象的框架，由两个组件：Encoder（编码器）和Decoder(解码器)组成。对于给定的输入source（x1, x2, x3, …,xn）, 首先编码器将其编码成一个中间表示向量z=（z1, z2, …, zn）。接着，解码器根据z和解码器自身前面的输出，来生成下一个单词（如Figure 1所示）。 Figure 1 1234567891011121314151617181920212223class EncoderDecoder(nn.Module): """ A standard Encoder-Decoder architecture. Base for this and many other models. """ def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): "Take in and process masked src and target sequences." return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) 上述代码呈现了一个标准的Encoder-Decoder框架。在实际应用中，编码器和解码器可以有多种组合，比如{RNN， RNN}、{CNN，RNN}等等，这就是传统的seq2seq框架。后来引入了attention机制，上述框架也被称为”分心模型“。为什么说他”分心“呢？因为对于解码器来说，他在生成每一个单词的时候，中间向量的每一个元素对当前生成词的贡献都是一样的。Attention的思想则是对于当前生成的单词，中间向量z的每个元素对其贡献的重要程度不同，跟其强相关的赋予更大的权重，无关的则给一个很小的权重。 举个栗子：假如我们要将 “knowledge is power” 翻译成 中文，在翻译”knowledge“这个单词时， 显然”knowledge“这个单词对翻译出来的”知识“贡献最大，其他两个单词贡献就很小了。这实际上让模型有个区分度，不会被无关的东西干扰到，翻译出来的准确度当然也就更高了。 在这里Attention其实还是一个小弟，主角仍然是RNN、CNN这些大佬。 我们不妨先顺着这个思路往下想，attention在这里充当了Encoder和Decoder的一个桥梁，事实证明有很好的效果。既然效果这么好，那在Encoder中是不是也可以用呢？文本自身对自身的编码进行有区分度的表示，事实上，这在以往的很多文本分类的工作中已被采用[2]。这看上去已经是个值得尝试的good idea了。继续开脑洞，Encoder都用了，Decoder能落后吗，好歹人家是一对CP，当然要妇唱夫随了。于是，Encoder和Decoder都用了自注意力（self-attention）。回想一下，到这里我们已经在三个地方用到了注意力机制了。这时候RNN大佬不愿意了，原本我的名声地盘都被你们分走了，散伙！Attention反正是初生牛犊不怕虎，说好，分分账分道扬镳吧，反正你的序列计算并行不起来一直让人诟病，没你我可能更潇洒。于是两兄弟就分开了。相见时难别亦难，RNN老大哥深谋远虑，临走时不忘嘱咐一句”苟富贵，勿相忘！“。于是一个故事的结束就成了另一个故事的开始，注意力就此开启创业之路，寒来暑往，春去秋来，在黑暗中不断寻找光亮，学习PPT技巧…终于有一天，它的PPT做完了，找到了融资，破茧成蝶，横空出道，并给自己取了个亮闪闪的名字：Transformer， 自此，一个新的时代开始了… 呃， 醒醒…黄粱熟了… 整体架构这部分我们来看看Transformer的架构。如Figure 2 所示， Transformer遵循了Encoder-Decoder的架构。在Encoder方面，6个编码器组件协同工作，组成一个大的编码器，解码器同样由6个解码器组件组成。我们先看Encoder。6个编码器组件依次排列，每个组件内部都是由一个多头attention加上一个前馈网络，attenion和前馈的输出都经过层归一化（LayerNormalization），并且都有各自的残差网络 。Decoder呢，组件的配置基本相同， 不同的是Decoder有两个多头attention机制，一个是其自身的mask自注意力机制，另一个则是从Encoder到Decoder的注意力机制，而且是Decoder内部先做一次attention后再接收Encoder的输出。说完了Encoder和Decoder，再说说输入，模型的输入部分由词向量（embedding）经位置编码（positional Encoding）后输入到Encoder和Decoder。编码器的输出由一个线性层和softmax组成，将浮点数映射成具体的符号输出。 Figure 2 那么，下面我们将结合原理和代码来逐一了解这些部分。 Encoder我们先来看下Encoder的实现。 123456789101112class Encoder(nn.Module): "Core encoder is a stack of N layers" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): "Pass the input (and mask) through each layer in turn." for layer in self.layers: x = layer(x, mask) return self.norm(x) 以上便是Encoder的核心实现。它由N个encoderLayer组成。输入一次通过每个encoderLayer，然后经过一个归一化层。下面来看下encoderLayer和LayerNorm是什么样子。 EncoderLayer和残差网络EncoderLayer如Figure 3所示。 Figure 3 123456789101112class EncoderLayer(nn.Module): "Encoder is made up of self-attn and feed forward (defined below)" def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) 12345678910111213class SublayerConnection(nn.Module): """ A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last. """ def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): "Apply residual connection to any sublayer with the same size." return x + self.dropout(sublayer(self.norm(x))) 这里的代码初看上去有点绕，不过没关系，听我娓娓道来。我们先看什么是残差网络（即代码中的SublayerConnection），见Figure 4 。其实非常简单，就是在正常的前向传播基础上开一个绿色通道，这个通道里x可以无损通过。这样做的好处不言而喻，避免了梯度消失（求导时多了一个常数项）。最终的输出结果就等于绿色通道里的x加上sublayer层的前向传播结果。注意，这里输入进来的时候做了个norm归一化，关于norm我们后面再说。 Figure 4 理解了残差网络，EncoderLayer的代码就很好看懂了。sublayer有两个，一个是多头self-attention层，另一个是前馈网络（feed_forward）。输入x先进入多头self-attention，用一个残差网络加成，接着通过前馈网络， 再用一个残差网络加成。 让我们从输入x开始，再从头理一遍这个过程： 输入x x做一个层归一化： x1 = norm(x) 进入多头self-attention: x2 = self_attn(x1) 残差加成：x3 = x + x2 再做个层归一化：x4 = norm(x3) 经过前馈网络: x5 = feed_forward(x4) 残差加成: x6 = x3 + x5 输出x6 以上就是一个Encoder组件所做的全部工作了。里面有两点暂未说明，一个是多头attention， 另一个是层归一化。喝杯茶之后精彩继续… 多头注意力机制多头注意力机制在敝人之前的博客已经做了详尽的原理和代码解析（请戳self-Attention/#more))，这里不再赘述，仅贴一下代码和注释，这里使用的是点乘attention，而不是加性（additive）attention。但是再提一点，在encoder和decoder的自注意力中，attention层的输入分为self_attn(x, x, x, mask)和self_attn(t, t, t, mask)， 这里的x和t分别为source和target输入。后面会看到，从encoder到decoder层的注意力输入时attn(t, m, m), 这里的m是Encoder的输出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778def attention(query, key, value, mask=None, dropout=None): """因子化的点乘Attention-矩阵形式 Query： 查询 （batch_size, heads, max_seq_len, d_k） Key: 键 (batch_size, heads, max_seq_len_d_k) Value: 值 （batch_size, heads, max_seq_len, d_v） d_v = d_k Q=K=V """ d_k = query.size(-1) # (batch_size, heads, max_seq_len, d_k) * (batch_size, heads, d_k, max_seq_len) # = (batch_size, heads, max_seq_len, max_seq_len) # 为了方便说明，只看矩阵的后两维 (max_seq_len, max_seq_len), 即 # How are you # How [[0.8, 0.2, 0.3] # are [0.2, 0.9, 0.6] # you [0.3, 0.6, 0.8]] # 矩阵中每个元素的含义是，他对其他单词的贡献（分数） # 例如，如果我们想得到所有单词对单词“How”的打分，取矩阵第一列[0.8, 0.2, 0.3], 然后做softmax scores = torch.matmul(query, key.transpose(-2, -1)) \ / math.sqrt(d_k) # 对于padding部分，赋予一个极大的负数，softmax后该项的分数就接近0了，表示贡献很小 if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim = -1) if dropout is not None: p_attn = dropout(p_attn) # 接着与Value做矩阵乘法: # (batch_size, heads, max_seq_len, max_seq_len) * (batch_size, heads, max_seq_len, d_k) # = (batch_size, heads, max_seq_len, d_k) return torch.matmul(p_attn, value), p_attnclass MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): "Take in model size and number of heads." super(MultiHeadedAttention, self).__init__() assert d_model % h == 0, "heads is not a multiple of the number of the in_features" # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): ''' 这里的query, key, value与attention函数中的含义有所不同，这里指的是原始的输入. 对于Encoder的自注意力来说，输入query=key=value=x 对于Decoder的自注意力来说，输入query=key=value=t 对于Encoder和Decoder之间的注意力来说， 输入query=t， key=value=m 其中m为Encoder的输出，即给定target，通过key计算出m中每个输出对当前target的分数，在乘上m ''' if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model =&gt; h x d_k query, key, value = \ [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] # 2) Apply attention on all the projected vectors in batch. ## x: (batch_size, heads, max_seq_len, d_k) x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # 3) "Concat" using a view and apply a final linear. ## x: (batch_size, max_seq_len, d_k*h) x = x.transpose(1, 2).contiguous() \ .view(nbatches, -1, self.h * self.d_k) ## output: (batch_size, max_seq_len, d_model) return self.linears[-1](x)def clones(module, N): "Produce N identical layers." return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) 注意下attention当中的mask。我们之前提到，在三个地方用到了attention。在Encoder的自注意力机制中，mask是用来过滤padding部分的作用，对于source中的每一个词来讲，其他的词对他都是可见的，都可以做出贡献的。但是在Decoder中，mask的作用就有所不同了。这可能又要从Encoder-Decoder框架说起。在这个框架下，解码器实际上可看成一个神经网络语言模型，预测的时候，target中的每一个单词是逐个生成的，当前词的生成依赖两方面：一是Encoder的输出，二是target的前面的单词。例如，在生成第一个单词是，不仅依赖于Encoder的输出，还依赖于起始标志[CLS]；生成第二个单词是，不仅依赖Encoder的输出，还依赖起始标志和第一个单词…依此类推。这其实是说，在翻译当前词的时候，是看不到后面的要翻译的词。由上可以看出，这里的mask是动态的。 1234567def subsequent_mask(size): "Mask out subsequent positions." # size: 序列长度 attn_shape = (1, size, size) # 生成一个上三角矩阵 subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') return torch.from_numpy(subsequent_mask) == 0 下面详细介绍下subsequent_mask是如何起作用的。函数的参数size指的是target句子的长度。以”[CLS] That is it“这个长度为4的target输入为例，这个函数的输出是什么呢？ 123456print(subsequent_mask(size=4))tensor([[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]]], dtype=torch.uint8) 可以看到，输出为一个下三角矩阵，维度为（1,4,4）。现在我们再来看下attention函数，mask起作用的地方是在Query和Key点乘后，结果矩阵的维度为（batch_size, heads, max_seq_len, max_seq_len）。为方便起见，我们只看一条数据，即batch_size=1。进入多头attention时，注意到对mask做了一步操作： 123456mask = mask.unsqueeze(1)mask：tensor([[[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]]]], dtype=torch.uint8) 这时mask的维度变成了（1,1,4,4）. 123456789101112target: CLS That is it CLS [[[[0.8, 0.2, 0.3, 0.9] That [0.2, 0.9, 0.6, 0.4] is [0.3, 0.6, 0.8, 0.7] it [1.2, 0.6, 2.1, 3.2]]]] mask： [[[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]]]] 写成了上面的样子，mask的作用就很显然了。例如，对于”CLS“来说，预测它下一个词时，只有”CLS“参与了attention，其他的词（相对于CLS为未来的词）都被mask_fill掉了，不起作用。后面的情况依此类推。 细心的小伙伴可能发现了，这里的解释并没有考虑padding部分。事实上，就算加了padding部分（为0），也不影响上述过程，有兴趣的话可以在上面it后面加上个0，下面的矩阵加一列[0 0 0 0 ]， 就可以一目了然。 层归一化123456789101112class LayerNorm(nn.Module): "Construct a layernorm module (See citation for details)." def __init__(self, features, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 在前面多次用到了层归一化（LayerNormalization），那么它是何方神圣呢？或许你对BatchNormalization比较熟悉，但千万不要在这里错以为是它。可以说层归一化是BatchNormalization的2.0版本，它是由Hinton神和他的学生提出的[3]。 BatchNormalizationBatchNormalization的出现无疑是广大AI调参侠的福音，将大家从繁琐的权重初始化、学习率调节中释放出来。它不仅能够大大加快收敛速度，还自带正则化功能，是Google 2015年提出的[4]。 机器学习的一个重要的假设是：数据是独立同分布的。训练集合测试集的数据是同分布的，这样模型才有好的泛化效果。神经网络其实也是在学习这个分布。在这个假设前提下，一旦我们知道了（x，y）的联合分布，很多问题就能通过条件概率P(x|y)计算出来了。但是在实际训练过程中，数据经过前一个隐藏层向后一个隐藏层传播（线性+非线性运算），分布通常会发生变化（作者称之为Internal Covariate Shift），这会导致网络学习变慢。我们从两个方面来稍微理解一下这个问题。 一方面：我们现在只看两个隐藏层（隐藏层A和隐藏层B）之间的传播。第一轮，来自A的数据经过线性操作和激活函数后到达B，反向传播时，B层为了学习到这个分布（满足A的需求），调整了权重W1。接着又进行第二轮传播了，A数据一到B，A说，现在需求变了，要这样这样。B一脸懵，盘算了一下，发现前面的白学了，没办法，换个方向重来…就这样，A一直在变，B就得跟着变，来来回回磨合…这听起来就是个非常耗时的工作。就好比A说今天要吃汤圆，B和好了面粉，准备了调料，A又说我要吃饭…虽然在B的不懈努力下A最后能吃上饭，但如果一开始A就告诉B我要吃饭不是更快一点？…网络越深，这个问题就越严重。 另一方面则是与激活函数有关，我们用sigmoid为例来说明一下。假设两层传播之间可表示为$$z = g（Wu+b）$$其中g是sigmoid函数，我们令$$x=Wu+b$$那么：$$z = sigmoid(x) = g(x) = \frac{1}{1+exp(-x)}$$计算下梯度：$$\frac{\partial z}{\partial W} = \frac{\partial z}{\partial g}\cdot\frac{\partial g}{\partial x}\cdot\frac{\partial x}{\partial u}$$我们关注一下中间那一项，是sigmoid函数的导数，它的分布如Figure 5 所示，可见随着|x|不断增大，该项趋近于0，这也就意味着整个梯度趋近于0，进入饱和区了，导致的结果就是收敛变慢！要想加快收敛怎么办，把|x|拉到靠近0的位置就行了，这里导数值最大。 Figure 5 BatchNormalization就是解决这两个问题的。首先，它将隐藏层的输入强行变换为同一分布（解决了第一个问题），这个分布就是正态分布（解决了第二个问题）。 它的具体做法如Figure 6所示。对每一个Mini-Batch的所有样本的每一维特征，计算两个统计量：均值和方差，然后做一个归一化操作，这样就变成了正态分布了。但是只这样做也有问题，首先，谁说数据一定是正态分布的，偏正态不行吗？第二，把数据全部拉到接近0的位置，sigmoid不就接近于一个线性函数了吗，没有起到激活的作用啊（线性激活函数+线性操作等价于一层线性操作）。 Figure 6 为了解决这两个问题，作者又做了一步操作，引入了两个参数gamma和beta（图中的最后一步）, 这两个参数是在训练过程中学习的！相当于将这个正态分布左右挪了挪，变胖或者变瘦，在加快收敛速度和保持非线性的之间找到一个平衡。当然这是臆测的，作者并没有明确这么说。如果你非要问为什么，那我只能告诉你深度学习是个实验领先于理论的学科。 上面说了一下训练过程。那么预测的时候呢？假如只预测一个样本，一个样本的均值…方差…怎么算?没意义是吧。事实上，预测的时候用的是全局的均值和方差，这个全局的均值和方差是怎么得到的呢？很简单，训练过程中记录下每个Mini-Batch的均值和方差，求个期望就是全局的均值和方差了。 LayerNormalizationBatchNormalization简直是个救世主啊，它令调参工作变得从未如此容易，让调参侠们不费吹灰之力，谈笑间到达收敛的彼岸。但毛主席曾经说过，万物都是辩证的，它同样存在两个问题： 对batch_size非常敏感。BatchNormalization的一个重要出发点是保持每层输入的数据同分布。回想下开始那个独立同分布的假设。假如取的batch_size很小，那显然有些Mini-Batch的数据分布就很可能与整个数据集的分布不一致了，又出现了那个问题，数据分布不一致…这就等于说没起到同分布的作用了，或者说同分布得不充分。实验也证明，batch_size取得大一点， 数据shuffle的好一点，BatchNormalization的效果就越好。 不能很方便地用于RNN。这其实是第一个问题的引申。我们再来看一下Figure 6中的均值和方差的计算公式。对所有样本求均值。对于图片这类等长的输入来说，这很容易操作，在每个维度加加除除就可以了，因为维度总是一致的。而对于不等长的文本来说，RNN中的每个time step共享了同一组权重。在应用BatchNormalization时，这就要求对每个time step的batch_size个输入计算一个均值和方差。那么问题就来了，假如有一个句子S非常长，那就意味着对S而言，总会有个time_step的batch_size为1，均值方差没意义，这就导致了BatchNormalization在RNN上无用武之地了。 为了避免这两个问题，LayerNormalization就应运而生了。 LayerNormalization的主要变化在于： 不再对Mini-Batch中的N的样本在各个维度做归一化，而是针对同一层的所有神经元做归一化。归一化公式为：$$\mu^l = \frac{1}{H}\Sigma_1^Ha_i^l$$ $$\sigma^l = \sqrt{\frac{1}{H}\Sigma_1^H(a_i^l-\mu^l)}$$ 其中，H指的是一层神经网络的神经元个数。我们再回想下BatchNormalization，其实它是在每个神经元上对batch_size个数据做归一化，每个神经元的均值和方差均不相同。而LayerNormalization则是对所有神经元做一个归一化，这就跟batch_size无关了。哪怕batch_size为1，这里的均值和方差只和神经元的个数有关系（如果读到这里仍然感到不是特别清楚，再读两遍…还困惑也没关系，待会看Figure 7）。 测试的时候可以直接利用LN，所以训练时不用保存均值和方差，这节省了内存空间。 Figure 7示意了两种方式的区别。假设有N个样本，每个样本的特征维度为4，图中每个小圆代表一个特征，特征1，特征2，…，特征4。BatchNormalization是在N个同一特征（如特征1）上求均值和方差，这里要对每个特征求1次，共4次。对照一下上面说的，万一有个样本有5个特征，是不是就没法玩了。LayerNormalization呢，别的样本都和我没啥关系，有多少个特征我把这些特征求个均值方差就好了。这也就是为什么一个叫”批归一化“，另一个叫”层归一化“了。理解了这一点，也就理解了为什么Transformer中使用LN而不是BN。 Figure 7 当然BatchNormalization也不是吃素的，虽然它在处理不等长序列上存在天生的缺陷，但是除此之外，它的效果都要好于其他Normalization方式（比如LN，WN，IN）。直觉上，BN貌似更好理解一点，LN似乎有种胡子眉毛一把抓的感觉… 回到层归一化的代码中，注意到这里的求均值和方差均是应用在x的最后一维上。这一维其实就是in_features，即神经元个数，同BN一样，这里也引入两个参数，参与训练。如果你想问这样做为什么也会有效，我愿意再次告诉你深度学习是个实验领先于理论的学科^_^，当然，不排除某位数学大神能够看出两者之间存在某种等价性。 前馈网络每个encoderLayer中，多头attention后会接一个前馈网络。这个前馈网络其实是两个全连接层，进行了如下操作：$$\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2$$ 123456789101112131415class PositionwiseFeedForward(nn.Module): '''Implements FFN equation. d_model=512 d_ff=2048 ''' def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) # self.w_1 = nn.Conv1d(in_features=d_model, out_features=d_ff, kenerl_size=1) self.w_2 = nn.Linear(d_ff, d_model) # self.w_2 = nn.Conv1d(in_features=d_ff, out_features=d_model, kenerl_size=1) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) 这两层的作用等价于两个 kenerl_size=1的一维卷积操作。 词向量这里就是普通的不能再普通的词向量，将词语变成d_model维的向量。 12345678class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) 位置编码由于Transformer没有用到CNN和RNN，因此，句子单词之间的位置信息就没有利用到。显然，这些信息对于翻译来说是非常有用的，同样一句话，每个单词的意思能够准确的翻译出来，但如果顺序不对，表达出来的意思就截然不同了。举个栗子感受一下，原句：”A man went through the Big Buddhist Temple“, 翻译成：”人过大佛寺“和”寺佛大过人“，意思就完全不同了。 那么如何表达一个序列的位置信息呢？对于某一个单词来说，他的位置信息主要有两个方面：一是绝对位置，二是相对位置。绝对位置决定了单词在一个序列中的第几个位置，相对位置决定了序列的流向。作者利用了正弦函数和余弦函数来进行位置编码：$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})$$ $$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})$$ 其中pos是单词处于句子的第几个位置。我们来考察一下第一个公式，看是否每个位置都能得到一个唯一的值作为编码。为简单起见，不妨令i=0，那么：$$PE_{(pos,0)} = sin(pos)$$我们反过来想，假如存在位置j和k的编码值相同，那么就有：$$sin(i)=sin(j)$$ $$i, j 为非负整数且i不等于j$$ 以上两式需要同时满足，可等价为：$$i=(-1)^k\cdot j+k\cdot{\pi}$$ $$i, j 为非负整数且i不等于j且k为整数$$ 同时成立，这就意味着：$$\pi=\frac{[i-(-1)^k\cdot j]}{k}$$这显然是不可能的，因为左边是个无理数（无限不循环小数），而右边是个有理数。通过反证法就证明了在这种表示下，每个位置确实有唯一的编码。 上面的讨论并未考虑i的作用。i决定了频率的大小，不同的i可以看成是不同的频率空间中的编码，是相互正交的，通过改变i的值，就能得到多维度的编码，类似于词向量的维度。这里2i&lt;=512（d_model）, 一共512维。想象一下，当2i大于d_model时会出现什么情况，这时sin函数的周期会变得非常大，函数值会非常接近于0，这显然不是我们希望看到的，因为这样和词向量就不在一个量级了，位置编码的作用被削弱了。另外，值得注意的是，位置编码是不参与训练的，而词向量是参与训练的。作者通过实验发现，位置编码参与训练与否对最终的结果并无影响。 1234567891011121314151617181920class PositionalEncoding(nn.Module): "Implement the PE function." def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 之所以对奇偶位置分别编码，是因为编码前一个位置是可以由另一个位置线性表示的（公差为1的等差数列），在编码之后也希望能保留这种线性。我们以第1个位置和第k+1个位置为例，还是令i=0：$$PE(1, 0)=cos(1)$$ $$PE(k,0)=sin(1+k)=sin(1)cos(k)+cos(1)sin(k)=A+B\cdot PE(1,0)$$ 至此，我们就把Encoder部分的细节介绍完了，下面来看下Decoder部分 Deocder我们先在看一眼刚开始的那张框架图。左半部分是Encoder，右半部分是Decoder。不难看出，Decoder和Encoder极其相似。 首先，Decoder也是由6个相同的decoder组件构成。 1234567891011class Decoder(nn.Module): "Generic N layer decoder with masking." def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) 每个组件长什么样子呢？首先输入经过词向量和位置编码，进入target的自注意力层，这里和Encoder一样，也是用了残差和层归一化。然后呢，这个输出再和Encoder的输出做一次context attention，相当于把上面的那层重复了一次，唯一不同的是，这次的attention有点不一样的，不再是自注意力，所有的技术细节都可以参照Encoder部分，这里不再复述。 123456789101112131415class DecoderLayer(nn.Module): "Decoder is made of self-attn, src-attn, and feed forward (defined below)" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) 线性层和softmax这是整个模型的最后一步了。从Decoder拿到的输出是维度为（batch_size, max_seq_len, d_model）的浮点型张量，我们希望得到最终每个单词预测的结果，首先用一个线性层将d_model映射到vocab的维度，得到每个单词的可能性，然后送入softmax，找到最可能的单词。 线性层的参数个数为d_model vocab_size， 一般来说，vocab_size会比较大，拿20000为例，那么只这层的参数就有51220000个，约为10的8次方，非常惊人。而在词向量那一层，同样也是这个数值，所以，一种比较好的做法是将这两个全连接层的参数共享，会节省不少内存，而且效果也不会差。 12345678class Generator(nn.Module): "Define standard linear + softmax generation step." def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return F.log_softmax(self.proj(x), dim=-1) 结语至此，整个Transformer模型的介绍就告一段落了，希望读者能够有所收获。以上恭疏短引，已竭鄙怀。请洒潘江，各倾陆海云尔！ 参考文献 Attention is all your need Multilingual Hierarchical Attention Networks for Document Classification Layer Normalization Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]]></content>
  </entry>
  <entry>
    <title><![CDATA[BERT解读(一) self——attention]]></title>
    <url>%2F2019%2F01%2F06%2FBERT%E7%B3%BB%E5%88%97(%E4%B8%80)Self-attention%2F</url>
    <content type="text"><![CDATA[BERT解读（一）self-Attention概述本系列文章希望对google BERT模型做一点解读。打算采取一种由点到线到面的方式，从基本的元素讲起，逐渐展开。 讲到BERT就不能不提到Transformer，而self-attention则是Transformer的精髓所在。简单来说，可以将Transformer看成和RNN类似的特征提取器，而其有别于RNN、CNN这些传统特征提取器的是，它另辟蹊径，采用的是attention机制对文本序列进行特征提取。 所以我们从self-Attention出发。 文章内容参考https://jalammar.github.io/illustrated-transformer/，jalammar的博客十分通俗易懂，且切中要害，本系列内容不乏很多翻译自jalammar的博客。 Attention is all your need尽管attention机制由来已久，但真正令其声名大噪的是google 2017年的这篇名为《attention is all your need》的论文。 让我们从一个简单的例子看起： 假设我们想用机器翻译的手段将下面这句话翻译成中文： “The animal didn’t cross the street because it was too tired” 当机器读到“it”时，“it”代表“animal”还是“street”呢？对于人类来讲，这是一个极其简单的问题，但是对于机器或者说算法来讲却十分不容易。 self-Attention则是处理此类问题的一个解决方案，也是目前看起来一个比较好的方案。当模型处理到“it”时，self-Attention可以将“it”和“animal‘联系到一起。 它是怎么做到的呢？ 通俗地讲，当模型处理一句话中某一个位置的单词时，self-Attention允许它看一看这句话中其他位置的单词，看是否能够找到能够一些线索，有助于更好地表示（或者说编码）这个单词。 如果你对RNN比较熟悉的话，我们不妨做一个比较。RNN通过保存一个隐藏态，将前面词的信息编码后依次往后面传递，达到利用前面词的信息来编码当前词的目的。而self-Attention仿佛有个上帝之眼，纵观全局，看看上下文中每个词对当前词的贡献。 下面来看下具体是怎么实现的。 Self-Attention in Detail首先，来看下怎样使用向量来计算self-attention，紧接着看如何用矩阵来计算self-attention。 使用向量如下图所示，一般而言，输入的句子进入模型的第一步是对单词进行embedding，每个单词对应一个embedding。对于每个embedding，我们创建三个向量，Query、Key和Value向量。我们如何来创建三个向量呢？如图，我们假设embedding的维度为4，我们希望得到一个维度为3的Query、Key和Value向量，只需将每个embedding乘上一个维度为4*3的矩阵即可。这些矩阵就是训练过程中要学习的。 那么，Query、Key和Value向量代表什么呢？他们在attention的计算中发挥了什么样的作用呢？ 我们用一个例子来说明： 首先要明确一点，self-attention其实是在计算每个单词对当前单词的贡献，也就是对每个单词对当前单词的贡献进行打分score。假设我们现在要计算下图中所有单词对第一个单词”Thinking”的打分。那么分分数如何计算呢，只需要将该单词的Query向量和待打分单词的Key向量做点乘即可。比如，第一个单词对第一个单词的分数为q1× k1，第二个单词对第一个单词的分数为q1×k2。 我们现在得到了两个单词对第一个单词的打分（分数是个数字了），然后将其进行softmax归一化。需要注意的是，在BERT模型中，作者在softmax之前将分数除以了Key的维度的平方根（据说可以保持梯度稳定）。softmax得到的是每个单词在Thinking这个单词上的贡献的权重。显然，当前单词对其自身的贡献肯定是最大的。 接着就是Value向量登场的地方了。将上面的分数分别和Value向量相乘，注意这里是对应位置相乘。 最后，将相乘的结果求和，这就得到了self-attention层对当前位置单词的输出。对每个单词进行如上操作，就能得到整个句子的attention输出了。在实际使用过程中，一般采用矩阵计算使整个过程更加高效。 在开始矩阵计算之前，先回顾总结一下上面的步骤： 创建Query、Key、Value向量 计算每个单词在当前单词的分数 将分数归一化后与Value相乘 求和 值得注意的是，上面阐述的过程实际上是Attention机制的计算流程，对于self-Attention，Query=Key=Value。 Matrix Calculation of Self-Attention其实矩阵计算就是将上面的向量放在一起，同时参与计算。 首先，将embedding向量pack成一个矩阵X。假设我们有一句话有长度为10，embedding维度为4，那么X的维度为（10 × 4）. 假设我们设定Q、K、V的维度为3.第二步我们构造一个维度为（4×3）的权值矩阵。将其与X做矩阵乘法，得到一个10×3的矩阵，这就能得到Query了。依样画葫芦，同样可以得到Key和Value。 最后，将Query和Key相乘，得到打分，然后经过softmax，接着乘上V的到最终的输出。]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Torchtext读取JSON数据]]></title>
    <url>%2F2018%2F12%2F31%2FTORCHTEXT%E8%AF%BB%E5%8F%96JSON%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[概述在文本预处理一节，介绍了如何利用torchtext读取tsv格式的文本数据。对于分类问题，这是足够的。但是在处理如NER和机器翻译等问题时，我们构造的输入通常就不是（类别，序列）这样的结构了，而是（序列，序列）。另一方面，在搭建混合网络时，有时我们希望能够给模型多个输入（例如cnn-bilstm-crf中，既需要字符又需要单词输入），这超过了tsv所能。因此要另辟蹊径。 尽管Torchtext封装了一个SequenceTaggingDataset类用于构造NER数据，但是在实际使用中发现，十分不方便生成batch。 json格式是采取字典的方式存储数据，这带来了很大的灵活性。但是关于其完整使用的相关文章非常有限，官方文档也未给出详细案例（不得不吐槽一下torchtext的说明文档）。 使用下面以NER任务为例， 进行json使用说明，以抛砖引玉。事实上，这种方法可以无缝地拓展到其他的任务上去，如文本分类，机器翻译等。 对于NER任务，标签(target)和输入(source)都是同样长度的序列。例如： 1234source: 人 民 网 1 月 1 日 讯 据 《 纽 约 时 报 》 报 道 , 美 国 华 尔 街 股 市 在 2 0 1 3 年 的 最 后 一 天 继 续 上 涨 , 和 全 球 股 市 一 样 , 都 以 最 高 纪 录 或 接 近 最 高 纪 录 结 束 本 年 的 交 易 。target:O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O O O O B_LOC I_LOC I_LOC I_LOC I_LOC O O O B_T I_T I_T I_T I_T O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O 文本预处理一般来说，为了节省内存，会先将其做个字符-ID映射(这里只是举例说明，和上面不是同一段文本)： 1234source:5627 5580 5550 5636 4509 5192 5466 5463 5624 5520 4871 5637 5607 5411 5313 5251 5528 5628 5580 5612 5292 5636 5626 5637 4810target:9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 1 构造Json格式的文件因为我们最终是通过torchtext来生成batch，所以首先要把数据存储为torchtext能够读取的json格式。值得注意的是，torchtext能够读取的json文件和我们一般意义上的json文件格式是不同的（这也是比较坑的地方），我们需要把上面的数据处理成如下格式： 12345&#123;"source": "10 111 2 3", "target": "1 1 2 2"&#125;&#123;"source": "10 111 2 3", "target": "1 1 2 2"&#125;&#123;"source": "10 111 2 3", "target": "1 1 2 2"&#125;&#123;"source": "10 111 2 3", "target": "1 1 2 2"&#125;&#123;"source": "10 111 2 3", "target": "1 1 2 2"&#125; 可以看到，里面的内容和通常的Json并无区别，每个字段采用字典的格式存储。不同的是，多个json序列中间是以换行符隔开的，而且最外面没有列表。 那么怎么构造这样的数据呢？ 我们知道python中的json模块是用来处理json文件的，但是对所有序列进行json.dump()后的结果并非我们想要的（序列之间不是以换行符隔开的），几经尝试，找到如下的处理方式，仅做参考，可能还有更好的处理方法： 12345678with open(config.TRAIN_FILE, 'w') as fw: for sent, label in train: sent = ' '.join([str(w) for w in sent]) label = ' '.join([str(l) for l in label]) df = &#123;"source": sent, "target": label&#125; encode_json = json.dumps(df) # 一行一行写入，并且采用print到文件的方式 print(encode_json, file=fw) 这里采用的是一行一行进行dumps， 然后print到文件，就能得到我们想要的格式了。 接下来就是使用torchtext读取了，这个和之前处理tsv文件并无太大差异。 Torchtext读取这里和之前处理tsv类似，不多赘述，只是将几个不同的点提出来说一下。 （1）Field的定义，这里source和target都是序列，因此两个字段的定义方式基本相同 （2）传入TabularDataset的fields和tsv的定义有所不同，这里定义成字典-元组格式 （3） TabularDataset的format要指定成json格式 （4）pad_token根据需要，一般来说source使用0作为padding， target使用-1进行padding 1234567891011121314151617181920212223242526272829303132333435363738def create_dataset(self): SOURCE = Field(sequential=True, tokenize=x_tokenize, use_vocab=False, batch_first=True, fix_length=self.fix_length, # 如需静态padding,则设置fix_length, 但要注意要大于文本最大长度 eos_token=None, init_token=None, include_lengths=True, pad_token=0) TARGET = Field(sequential=True, tokenize=x_tokenize, use_vocab=False, batch_first=True, fix_length=self.fix_length, # 如需静态padding,则设置fix_length, 但要注意要大于文本最大长度 eos_token=None, init_token=None, include_lengths=False, pad_token=-1) fields = &#123;'source': ('source', SOURCE), 'target': ('target', TARGET)&#125; train, valid = TabularDataset.splits( path=config.ROOT_DIR, train=self.train_path, validation=self.valid_path, format="json", skip_header=False, fields=fields) return train, validdef get_iterator(self, train, valid): train_iter = BucketIterator(train, batch_size=self.batch_size, device = torch.device("cpu"), # cpu by -1, gpu by 0 sort_key=lambda x: len(x.source), # field sorted by len sort_within_batch=True, repeat=False) val_iter = BucketIterator(valid, batch_size=self.batch_size, device=torch.device("cpu"), # cpu by -1, gpu by 0 sort_key=lambda x: len(x.source), # field sorted by len sort_within_batch=True, repeat=False) return train_iter, val_iter]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>torchtext</tag>
        <tag>json</tag>
        <tag>batch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BiLSTM模型中CRF层的代码实现（四）]]></title>
    <url>%2F2018%2F12%2F31%2FBiLSTM%E6%A8%A1%E5%9E%8B%E4%B8%ADCRF%E5%B1%82%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%EF%BC%88%E5%9B%9B%EF%BC%89%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[CRF ON THE TOP OF BILSTM（四） 代码实现概述前面我们重点介绍了CRF的原理，损失函数以及分数的计算。本节将结合前面的相关内容，介绍BILSTM+CRF代码实现（pytorch 0.4.0）及一些需要注意的细节。 模型总览BILSTM+CRF模型由BILSTM， CRF， 损失函数， 预测函数几部分组成。BILSTM的输出作为CRF的输入，损失函数定义在CRF中， 损失函数使用前向算法，预测函数使用Viterbi算法，下面逐一介绍。 1234567891011121314151617181920212223242526272829303132333435363738class BISLTM_CRF(nn.Module): def __init__(self, vocab_size, word_embedding_dim, word2id, hidden_size, bi_flag, num_layer, input_size, cell_type, dropout, num_tag, checkpoint_dir): super(BISLTM_CRF, self).__init__() self.embedding = nn.Embedding(vocab_size, word_embedding_dim) for p in self.embedding.parameters(): p.requires_grad = False self.embedding.weight.data.copy_(torch.from_numpy(get_embedding(vocab_size, word_embedding_dim, word2id))) self.rnn = RNN(hidden_size, bi_flag, num_layer, input_size, cell_type, dropout, num_tag) self.crf = CRF(num_tag=num_tag) self.checkpoint_dir = checkpoint_dir def forward(self, inputs, length): embeddings = self.embedding(inputs) rnn_output = self.rnn(embeddings, length) # (batch_size, time_steps, num_tag+2) return rnn_output def loss_fn(self, rnn_output, labels, length): loss = self.crf.negative_log_loss(inputs=rnn_output, length=length, tags=labels) return loss def predict(self, rnn_output, length): best_path = self.crf.get_batch_best_path(rnn_output, length) return best_path BILSTMBILSTM模型这里采用的是双向GRU。GRU的输出（维度为 batch_size time_steps hidden_size）经过线性层（hidden_size num_tag）变为(batch_size time_steps num_tag), 这是最终的输出结果。其中，hidden_size是GRU隐藏层个数2(因为是双向)， num_tag 为需要标注的标签个数。 CRF损失函数对于一个句子，损失函数为这个句子的所有可能的路径分数之和减去真实路径分数。值得注意的是，这里的“路径”指的句子中的有小部分，不包括padding部分。 对于一个batch，我们将所有句子的损失函数相加，然后除以总的句子长度，将损失分配到每个字上，作为训练的损失函数。 这里的重点在于计算真实路径分数和所有路径分数之和，下面来看。 1234567891011121314151617181920212223242526def negative_log_loss(self, inputs, length, tags): """ features:(batch_size, time_step, num_tag) target_function = P_real_path_score/P_all_possible_path_score = exp(S_real_path_score)/ sum(exp(certain_path_score)) 我们希望P_real_path_score的概率越高越好，即target_function的值越大越好 因此，loss_function取其相反数，越小越好 loss_function = -log(target_function) = -S_real_path_score + log(exp(S_1 + exp(S_2) + exp(S_3) + ...)) = -S_real_path_score + log(all_possible_path_score) """ if not self.use_cuda: inputs = inputs.cpu() length = length.cpu() tags = tags.cpu() loss = Variable(torch.tensor(0.), requires_grad=True) num_chars = torch.sum(length.data).float() # 所有字的个数 for ix, (features, tag) in enumerate(zip(inputs, tags)): features = features[:length[ix]] tag = tag[:length[ix]] real_score = self.real_path_score(features, tag) # 真实路径分数 total_score = self.all_possible_path_score(features) # 所有可能的路径分数 cost = total_score - real_score loss = loss + cost return loss/num_chars # 分配到每个字的损失 转移矩阵在介绍分数计算之前，先来看下CRF中的一个重要的训练参数，转移矩阵。转移矩阵的定义决定了后面前向算法和维特比算法的实现，因此十分重要。 我们的标签序列id映射定义为： 12345678910111213tag_to_ix = &#123; "B_PER": 0, # 人名 "I_PER": 1, "B_LOC": 2, # 地点 "I_LOC": 3, "B_ORG": 4, # 机构 "I_ORG": 5, "B_T": 6, # 时间 "I_T": 7, "O": 8, # 其他 "SOS": 9, # 起始符 "EOS":10 # 结束符&#125; 转移矩阵的大小为：(num_tag +2, num_tag +2)。+2是因为包含了起始符号和结束符号。 并且，P_jk 表示从tag_j到tag_k的分数，这样就有： （1）P_j* 表示所有从tag_j出发的边 （2） P_*k 表示所有到tag_k的边 此外，在均匀初始化时，设定起始符号和结束符号对应的转移分数，使得： （1） 从EOS-&gt;其他标签为不可能事件, 如果发生，则产生一个极大的损失 （2） 从其他标签-&gt;SOS为不可能事件，如果发生，则产生一个极大的损失 123456789101112131415def __init__(self, num_tag, use_cuda=False): if num_tag &lt;= 0: raise ValueError("Invalid value of num_tag: %d" % num_tag) super(CRF, self).__init__() self.num_tag = num_tag self.start_tag = num_tag self.end_tag = num_tag + 1 self.use_cuda = use_cuda # 转移矩阵transitions：P_jk 表示从tag_j到tag_k的分数 # P_j* 表示所有从tag_j出发的边 # P_*k 表示所有到tag_k的边 self.transitions = nn.Parameter(torch.Tensor(num_tag + 2, num_tag + 2)) nn.init.uniform_(self.transitions, -0.1, 0.1) self.transitions.data[self.end_tag, :] = -10000 # 表示从EOS-&gt;其他标签为不可能事件, 如果发生，则产生一个极大的损失 self.transitions.data[:, self.start_tag] = -10000 # 表示从其他标签-&gt;SOS为不可能事件, 同上 真实路径分数真实路径分数由发射分数和转移分数组成。其中，转移分数是CRF需要训练的参数，我们随机初始化；发射分数是BISTLM的输出矩阵（对于每个句子，其维度为 time_steps * num_tag）， 对于每个tag， 该矩阵都给出一个分数，我们知道了真实的标签序列， 拿这个标签去索引该矩阵，即可得到真实路径的发射分数。 123456789101112131415161718192021def real_path_score(self, features, tags): """ features: (time_steps, num_tag) real_path_score表示真实路径分数 它由Emission score和Transition score两部分相加组成 Emission score由LSTM输出结合真实的tag决定，表示我们希望由输出得到真实的标签 Transition score则是crf层需要进行训练的参数，它是随机初始化的，表示标签序列前后间的约束关系（转移概率） Transition矩阵存储的是标签序列相互间的约束关系 在训练的过程中，希望real_path_score最高，因为这是所有路径中最可能的路径 """ r = torch.LongTensor(range(features.size(0))) if self.use_cuda: pad_start_tags = torch.cat([torch.cuda.LongTensor([self.start_tag]), tags]) pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.end_tag])]) r = r.cuda() else: pad_start_tags = torch.cat([torch.LongTensor([self.start_tag]), tags]) pad_stop_tags = torch.cat([tags, torch.LongTensor([self.end_tag])]) # Transition score + Emission score score = torch.sum(self.transitions[pad_start_tags, pad_stop_tags]).cpu() + torch.sum(features[r, tags]) return score 所有路径分数之和所有路径分数之和采用前向算法计算，算法复杂度为O(N N T)， 其中N为标签个数，T为序列长度。 这里， 将start_tag的发射分数初始化为0，从start_tag开始，沿着序列一个字一个字地计算前向分数。 最后，加上end_tag的转移分数，即为所有START_TAG -&gt; 1st word -&gt; 2nd word -&gt;…-&gt;END_TAG的分数。 事实上，前向算法的精髓在于，用指数将每个字的分数用三个数存起来，而这三个数后面又是可以拆分的(指数的计算特性，乘法拆成加法。）ps：如果对前向算法不是很明白或者大致明白但不是特别清晰，建议画个图看下就一目了然了。 需要注意的是，根据前向算法，需要计算指数和的log， log_sum_exp， 由于前向方法是一个不断累积的过程， 会导致exp之和趋于无穷大，超过计算机的浮点数最大值限制，出现“上溢”。避免这种做法的手段是，找到整个矩阵的最大值，将矩阵减去该最大值进行指数求和，最终结果再加上该最大值即可。减去最大值后求指数和尽管也会出现无穷大的情况，但是这时是在分母上，该项可以计算为0，这样我们取log后仍可以得到一个合理的数值。 123456789101112131415161718192021222324252627282930313233343536373839404142def all_possible_path_score(self, features): """ 计算所有可能的路径分数的log和：前向算法 step1: 将forward列expand成9*9 step2: 将下个单词的emission行expand成9*9 step3: 将1和2和对应位置的转移矩阵相加 step4: 更新forward，合并行 step5: 取forward指数的对数计算total """ time_steps = features.size(0) # 初始化 forward = Variable(torch.zeros(self.num_tag)) # 初始化START_TAG的发射分数为0 if self.use_cuda: forward = forward.cuda() for i in range(0, time_steps): # START_TAG -&gt; 1st word -&gt; 2nd word -&gt;...-&gt;END_TAG emission_start = forward.expand(self.num_tag, self.num_tag).t() emission_end = features[i,:].expand(self.num_tag, self.num_tag) if i == 0: trans_score = self.transitions[self.start_tag, :self.start_tag].cpu() else: trans_score = self.transitions[:self.start_tag, :self.start_tag].cpu() sum = emission_start + emission_end + trans_score forward = log_sum(sum, dim=0) forward = forward + self.transitions[:self.start_tag, self.end_tag].cpu() # END_TAG total_score = log_sum(forward, dim=0) return total_scoredef log_sum(matrix, dim): """ 前向算法是不断累积之前的结果，这样就会有个缺点 指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf 为了避免这种情况，我们做了改动： 1. 用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算 SUM = log(exp(s1)+exp(s2)+...+exp(s100)) = log&#123;exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]&#125; = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)] where clip=max """ clip_value = torch.max(matrix) # 极大值 clip_value = int(clip_value.data.tolist()) log_sum_value = clip_value + torch.log(torch.sum(torch.exp(matrix-clip_value), dim=dim)) return log_sum_value 整个训练部分至此就介绍完了。下面介绍使用维特比算法进行预测。 预测预测算法迭代过程和前向算法类似。不同的是，forward不再是到该点的路径分数之和，而是到该点的所有路径中最大分数， 这样我们就有了到该点的最大路径。同时，在迭代过程中 ，我们将最大路径中到当前节点（具有最大分数）的前一个节点的索引存储下来，作为最佳路径点。 中间的迭代过程较容易理解，比较绕的是 （1） 如何处理起始符号和结束符号 （2） 最后一个节点怎么找 我们只需要把握一个点，上面的问题就能迎刃而解： 预测和回溯的时候，我们要从START_TAG -&gt; 1st word -&gt; 2nd word -&gt;…-&gt;END_TAG，一个都不能少！ 最后奉上一份PPThttps://baidupan.com， 里面用图形化的方式对CRF原理进行了详细的描述。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 def viterbi(self, features): time_steps = features.size(0) forward = Variable(torch.zeros(self.num_tag)) # START_TAG if self.use_cuda: forward = forward.cuda() # back_points 到该点的最大分数 last_points 前一个点的索引 back_points, index_points = [self.transitions[self.start_tag, :self.start_tag].cpu()], [torch.LongTensor([-1]).expand_as(forward)] for i in range(1, time_steps): # START_TAG -&gt; 1st word -&gt; 2nd word -&gt;...-&gt;END_TAG emission_start = forward.expand(self.num_tag, self.num_tag).t() emission_end = features[i,:].expand(self.num_tag, self.num_tag) trans_score = self.transitions[:self.start_tag, :self.start_tag].cpu() sum = emission_start + emission_end + trans_score forward, index = torch.max(sum.detach(), dim=0) back_points.append(forward) index_points.append(index) back_points.append(forward + self.transitions[:self.start_tag, self.end_tag].cpu()) # END_TAG return back_points, index_points def get_best_path(self, features): back_points, index_points = self.viterbi(features) # 找到线头 best_last_point = argmax(back_points[-1]) index_points = torch.stack(index_points) # 堆成矩阵 m = index_points.size(0) # 初始化矩阵 best_path = [best_last_point] # 循着线头找到其对应的最佳路径 for i in range(m-1, 0, -1): best_index_point = index_points[i][best_last_point] best_path.append(best_index_point) best_last_point = best_index_point best_path.reverse() return best_path def get_batch_best_path(self, inputs, length): if not self.use_cuda: inputs = inputs.cpu() length = length.cpu() max_len = inputs.size(1) batch_best_path = [] for ix, features in enumerate(inputs): features = features[:length[ix]] best_path = self.get_best_path(features) best_path = torch.Tensor(best_path).long() best_path = padding(best_path, max_len) batch_best_path.append(best_path) batch_best_path = torch.stack(batch_best_path, dim=1) return batch_best_path def argmax(matrix, dim=0): """(0.5, 0.4, 0.3) —&gt; 0""" _, index = torch.max(matrix, dim=dim) return indexdef padding(vec, max_len, pad_token=-1): new_vec = torch.zeros(max_len).long() new_vec[:vec.size(0)] = vec new_vec[vec.size(0):] = pad_token return new_vec 完整代码请戳 https://github.com/circlePi/NER/BILSTM_CRF]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>深度学习</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BiLSTM模型中CRF层的运行原理(三)]]></title>
    <url>%2F2018%2F12%2F31%2FCRF-Layer-on-the-Top-of-BiLSTM%2B--3%2F</url>
    <content type="text"><![CDATA[在前面的部分中，我们学习了BiLSTM-CRF模型的结构和CRF损失函数的细节。您可以通过各种开源框架（Keras，Chainer，TensorFlow等）实现您自己的BiLSTM-CRF模型。最重要的事情之一是在这些框架上自动计算模型的反向传播，因此您不需要自己实现反向传播来训练模型（即计算梯度和更新参数）。此外，一些框架已经实现了CRF层，因此只需添加一行代码就可以非常轻松地将CRF层与您自己的模型相结合。 在本节中，我们将探讨如何预测句子的标签序列。 1. 预测句子的标签序列第1步：BiLSTM-CRF模型的emission score和transition score假设句子$x$包含３个字符：$x=[w_0, w_1, w_2]$。此外，假设我们已经获得了BiLSTM模型的emission score和CRF层的transition score： $l_1$ $l_2$ $w_0$ $w_{01}$ $w_{02}$ $w_1$ $w_{11}$ $w_{12}$ $w_2$ $w_{21}$ $w_{22}$ $x_{ij}$表示$w_i$被标记为$l_j$的得分。 $l_1$ $l_2$ $l_1$ $t_{11}$ $t_{12}$ $l_2$ $t_{21}$ $t_{22}$ $t_{ij}$表示标签$i$转移到标签$j$的分数 第2步：预测如果您熟悉Viterbi算法，这部分对您来说很容易。如果你不太了解的话，请不要担心，我将逐步解释算法。对于一个句子，我们将按从左到右的方式进行预测，即: $w_0$$w_0$–&gt;$w_1$$w_0$–&gt;$w_1$–&gt;$w_2$ 你将看到两个变量：$obs$和$previous$。$previous$存储前面步骤的最终结果。$obs$表示来自当前单元的信息。 首先，我们来看看$w_0$，即: $obs=[x_{01}, x_{02}]$ $previous=None$ 对于第一个字符$w_0$。$w_0$的最优标签很简单。例如，如果$obs=[x_{01}=0.2, x_{02}=0.8]$，显然，$w_0$的最优标签是$l_2$，由于只有一个字符从而无标签到标签之间的transition，因此不用计算transition scores。 接着，对于$w_0$ –&gt; $w_1$： $obs=[x_{11, x_{12}}]$ $previous=[x_{01}, x_{02}]$ 1).扩展$previous$为： $$previous=\left(^{previous[0] \quad previous[0]}{previous[1] \quad previous[1]}\right)=\left(^{x{01} \quad x_{01}}_{x_{02} \quad x_{02}}\right)$$ 2).扩展$obs$为： $$obs=\left(^{obs[0] \quad obs[1]}{obs[0] \quad obs[1]}\right)=\left(^{x{11} \quad x_{12}}_{x_{11} \quad x_{12}}\right)$$ 3).将$previous$，$obs$和transition score进行求和: $$scores=\left(^{x_{01} \quad x_{01}}_{x_{02} \quad x_{02}}\right) + \left(^{x_{11} \quad x_{12}}_{x_{11} \quad x_{12}}\right) + \left(^{t_{11} \quad t_{12}}_{t_{21} \quad t_{22}}\right)$$ 即: $$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}_{x_{02}+x_{11}+t{21} \quad x_{02}+x_{12}+t_{22}}\right)$$ 当我们计算所有可能标签序列组合的总得分时，您可能想知道与前一部分没有有区别。接下来我们将看到其中的差异性。 更新$previous$值： $$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])]$$ 假设，我们得到的score为： $$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}_{x_{02}+x_{11}+t{21} \quad x_{02}+x_{12}+t_{22}}\right)=\left( ^{0.2 \quad 0.3}_{0.5 \quad 0.4}\right)$$ 则$previous$将更新为: $$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])] = [0.5, 0.4]$$ $previous$是什么意思？$previous$列表存储当前字符对每个标签的最大得分。 1.1 案例假设，在语料库中，我们总共有2个标签，$label1(l_1)$和$label2(l_2)$，这两个标签的索引分别为0和1。 $previous[0]$是以第0个标签$l_1$结束的序列的最大得分，类似的$previous[1]$是以$l_2$结束的序列的最大得分。在每次迭代过程中，我们仅仅保留每个标签对应的最优序列的信息($previous=[\max(scores[00], scores[10]),\max( scores[01], scores[11])]$)。分数较少的序列信息将被丢弃。 回到我们的主要任务： 同时，我们还有两个变量来存储历史信息（分数和索引），即$alpha_0$和$alpha_1$。每次迭代，我们都将最好的得分追加到$alpha_0$。为方便起见，每个标签的最高分都有下划线。 $$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}{\underline{x{02}+x_{11}+t{21}} \quad \underline{x_{02}+x_{12}+t_{22}}}\right)=\left( ^{0.2 \quad 0.3}_{\underline{0.5} \quad \underline{0.4}}\right)$$ $$alpha_0=[(scores[10],scores[11])]=[(0.5,0.4)]$$ 另外，相应的列的索引被保存在$alpha_1$： $$alpha_1=[(ColumnIndex(scores[10]),ColumnIndex(scores[11]))]=[(1,1)]$$ 其中，$l_1$的索引是0，$l_2$的索引是1，所以$(1, 1)=(l_2, l_2)$，这意味着对于当前的单元$w_i$和标签$l^(i)$：$(1, 1)=(l_2, l_2)$=(当序列是$\underline{l^{(i-1)}=l_2} -&gt; \underline{l^{(i)}=l_1}$时我们可以得到最大得分为0.5, 当序列是$\underline{l^{(i-1)}=l_2} -&gt; \underline{l^{(i)}=l_2}$时我们可以得到最大得分为0.4) $l^{(i-1)}$是前一个字符$w_{i-1}$对应的标签 最后，我们计算$w_0$ –&gt; $w_1$ –&gt; $w_2$: $obs=[x_{21}, x_{22}]$ $previous=[0.5, 0.4]$ 1).扩展$previous$为： $$previous=\left(^{previous[0] \quad previous[0]}{previous[1] \quad previous[1]}\right)=\left(^{0.5 \quad 0.5}{0.4 \quad 0.4}\right)$$ 2).扩展$obs$为: $$obs=\left(^{obs[0] \quad obs[1]}{obs[0] \quad obs[1]}\right)=\left(^{x{21} \quad x_{22}}_{x_{21} \quad x_{22}}\right)$$ 3).对$previous$，$obs$和transition score进行求和: $$scores=\left(^{0.5 \quad 0.5}{0.4 \quad 0.4}\right) +\left(^{x{21} \quad x_{22}}_{x_{21} \quad x_{22}}\right)+ \left(^{t_{11} \quad t_{12}}_{t_{21} \quad t_{22}}\right)$$ 即: $$scores=\left(^{0.5+x_{21}+t_{11} \quad 0.5+x_{22}+t_{12}}{0.4+x{21}+t{21} \quad 0.4+x_{22}+t_{22}}\right)$$ 更新$previous$为： $$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])]$$ 比如，我们得到的scores为： $$scores=\left( ^{0.6 \quad \underline{0.9}}_{\underline{0.8} \quad 0.7}\right)$$ 因此，$previous$将更新为： $$previous=[0.8, 0.9]$$ 实际上，$previousp[0]$和$previous[1]$之间的较大的一个是最好的预测结果的得分。与此同时，每个标签的最大得分和索引将被添加到$alpha_0$和$alpha_1$中： $alpha_0=[(0.5,0.4),\underline{(scores[10],scores[01])}]$ $ =[(0.5,0.4),\underline{(0.8,0.9)}]$ $alpha_1=[(1,1),\underline{(1,0)}]$ 第3步：找出得分最高的最佳序列在该步骤中，我们将根据$previousp[0]$和$previous[1]$找到最高的得分。我们将从右到左的方式反推最优序列，即最后一个单元反推到第一个单元。 $w_1$ –&gt; $w_2$：首先，检查$alpha_0$和$alpha_1$最后一个元素:(0.8, 0.9)和(1, 0)。0.9是最高分数，其对应的位置是1，因此对应的标签是$l_2$。继续从$alpha_1$中对应位置获得$w_1$对应的标签索引， 即(1, 0)[1]=0。索引0表示$w_1$对应的标签是$l_1$。因此我们可以得到$w_1 -&gt; w_2$的最佳序列是$l_1 -&gt; l_2$。 $w_0$ –&gt; $w_1$： 接着，我们继续向前移动并获得$alpha_1$的上一个元素：(1, 1)。从上面可知$w_1$的标签是$l_1$(标签对应的索引为0)，因此我们可以得到$w_0$对应的标签索引为(1,1)[0]=1。所以我们可以得到$w_0 -&gt; w_1$的最佳序列是$l_2 -&gt; l_1$。 最终可以得到$w_0 -&gt; w_1 -&gt; w_2$的最佳标签序列是$l_2 -&gt; l_1 -&gt; l_2$ 参考[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360. https://arxiv.org/abs/1603.01360]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>深度学习</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BiLSTM模型中CRF层的运行原理(二)]]></title>
    <url>%2F2018%2F12%2F31%2FCRF-Layer-on-the-Top-of-BiLSTM%2B--2%2F</url>
    <content type="text"><![CDATA[在上一节中，我们知道CRF层可以从训练数据集中自动学习到一些约束规则来保证预测标签的合法性。这些约束包括： 句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-” 标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-PersonI-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列. 标签序列“O I-label”是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说，有效的标签序列应该是“O B-label”。 这一节，我们将解释为什么CRF层会自动学习到这些约束规则。 1.CRF层在CRF层损失函数中，有两种形式的score。这些scores是CRF层的关键概念。 1.1 emission score第一个是emission score，主要来自BiLSTM层的输出，如下图所示，比如，单元$w_0$标记为‘B-Person’的score为1.5： 为了方便起见，我们用数字来表示各个实体标签，对应关系如下： Label Index B-Person 0 I-Person 1 B-Organization 2 I-Organization 3 O 4 另外，我们使用$x_{iy_j}$表示emission score，其中: i 表示词的索引 $y_j$表示标签的索引 例如，根据上图，我们可以得到$x_{i=1,y_j=2} = x_{w_1,B-Organization} = 0.1$，也就是说$w_1$标记为’B-Organization‘的score为0.1。 1.2 Transition score我们定义$t_{y_iy_j}$表示transition score，从上图中可知$t_{B-Person,I-Person} = 0.9$，也就是说“$B-Person \rightarrow I-Person$“的标签转移得分为0.9。因此，对于所有的标签序列组合，我们将得到一个transition score矩阵，包含了所有标签之间的transition score。 为了使transition score矩阵更具鲁棒性，我们额外增加两个标签——START 和END，START 代表句子的开始位置，而非第一个词，同理，END代表句子的结束位置. 表1 中包含了START和END标签的transition score矩阵。 如表1所示，我们可以发现transition score矩阵可以学习到好多约束规则，比如: 句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”（例如，从“START” 到 “I-Person or I-Organization”的score都很低 ）。 标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理的标签序列（例如，从“B-Organization”到’I-Person’的score只有0.0003，比其他都低。）。 标签序列“O I-label” 是不合理的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”（例如，$t_{O,I-Person}$的score就非常小）。 你可能会有一个疑问，该transition score矩阵如何计算得到？ 事实上，该矩阵是BiLSTM-CRF模型的一个参数，在训练模型之前，一般会初始化一个矩阵作为transition score矩阵，在训练过程中，该矩阵中的transition score都会不断地自动更新。也就是说CRF层可以自动学习，通过学习得到一个最优的transition score矩阵，而不用我们手工定义一个transition score矩阵。当模型训练不断地持续，该矩阵会越来越合理。 1.3 CRF损失函数CRF损失函数中包含了真实标签序列得分和所有可能标签序列的总得分，正常情况下，真实标签序列得分在所有可能标签序列得分中是最高的。 比如，假设数据集中的标签如下所示： Label Index B-Person 0 I-Person 1 B-Organization 2 I-Organization 3 O 4 START 5 END 6 那么，在第一节中我们假设的句子$x$，所有可能的标签序列组合为: (1) START B-Person B-Person B-Person B-Person B-Person END (2) START B-Person I-Person B-Person B-Person B-Person END … (10) START B-Person I-Person O B-Organization O END … (N) O O O O O O O 假设一共有N中可能的标签序列组合，且第$i$个标签序列的得分为$P_i$，那么所有可能标签序列组合的总得分为： $$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ 按照我们之前的假设，第10个是真实的标签序列，那么，我们想要的结果是第10个标签序列得分在所有可能的标签序列得分中是最高的。 因此，我们可以定义模型的损失函数，在整个模型训练过程中，BiLSTM-CRF模型的参数不断地进行更新，使得真实标签序列得分在所有可能标签序列组合得分中的占比是最高的。因此，模型的损失函数格式如下所示： $$ LossFunction = \frac{P_{RealPath}}{P_1+P_2+…+P_N} $$ 那么，问题就来了： 如何定义一个标签序列的得分？ 如何计算所有可能标签序列组合的总得分？ 在计算总得分中，一定需要计算每一个可能的标签序列的得分吗？ 接下来，我们来解答每一个问题。 1.4 真实标签序列得分前面我们定义了标签序列得分为$P_i$，以及所有可能标签序列的总得分为： $$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ 其中$e^{S_i}$表示第i个标签序列得分。 显然，在所有可能的标签序列组合必然存在一个序列是真实标签序列，而剩下的标签序列组合都是错误的，比如序列”START B-Person I-Person O B-Organization O END “是正确的，而序列‘START B-Person I-Person B-Person B-Person B-Person END’是错误的。 在整个模型训练过程中，CRF层的损失函数只需要两个得分： 一个是真实标签序列得分 一个是所有可能标签序列组合的总得分 而我们的学习目的是让真实的标签序列得分在总得分中的占比是最高的。 对于真实标签序列的得分$e^{S_i}$，我们直接计算$S_i$即可。 我们使用之前的案例，真实的标签序列为“START B-Person I-Person O B-Organization O END ”，即： 句子$x$由5个字符组成，$w_1,w_2,w_3,w_4,w_5$ 我们在句子前后增加两个字符，记为$w_0,w_6$ $S_i$主要由第一节中提到的Emission Score和Transition Score组成，即$S_i = Emission Score + Transition Score$ 1.4.1 Emission ScoreEmission Score计算公式如下所示： $Emission Score = $ $x_{0,START} + x_{1,B-Person} + x_{2,I-Person} + x_{3,O} +$ $x_{4,B-Organization} + x_{5,O} + x_{6,END}$ 其中： $x_{index,label}$表示第index个词被标记为label的得分 $x_{1,B-Person}, x_{2,I-Person} , x_{3,O}, x_{4,B-Organization},x_{5,O}$ 为BiLSTM层的输出 一般$x_{0,START}$和$x_{6,END}$为0 1.4.2 Transition ScoreTransition Score计算公式如下所示: $Transition Score =$$t_{START \rightarrow B-Person} + t_{B-Person \rightarrow I-Person} +$$t_{I-Person \rightarrow O} + t_{O \rightarrow B-Organization} + t_{B-Organization \rightarrow O} + t_{O \rightarrow END}$ 其中: $t_{label1 \rightarrow label2}$ 表示$label1$到$label2$的transition Score。 transition Score主要是在CRF层进行计算的，也就是说，transition Score完全是CRF层的参数。 因此，我们通过计算$s_i$，可以得到第i条标签序列的得分。 1.5 所有可能标签序列组合的总得分前面，我们计算了单条标签序列得分，接下来，我们需要计算所有可能标签序列的总得分。由之前内容可知，总得分的计算公式为; $$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ 很显然，总得分计算方式就是每一条标签序列得分的求和，那么我们能想到的最简单的方法就是先计算每一条的标签序列得分，然后将所有的标签序列得分进行相加得到总得分。虽然计算很简单，但是效率不高，需要很长的训练时间。 接下来,我们将通过公式推导来认识总得分计算过程。 1.6 CRF的损失函数由前面可知，CRF层的损失函数为: $$ Loss Function = \frac{P_{RealPath}}{P_1 + P_2 + … + P_N} $$ 我们对其对数化，即： $$ LogLossFunction = \log \frac{P_{RealPath}}{P_1 + P_2 + … + P_N} $$ 一般在模型训练过程中，我们希望损失函数最小化，因此，在损失函数添加一个负号，即: $Log Loss Function$$= - \log \frac{P_{RealPath}}{P_1 + P_2 + … + P_N}$$= - \log \frac{e^{S_{RealPath}}}{e^{S_1} + e^{S_2} + … + e^{S_N}}$$= - (\log(e^{S_{RealPath}}) - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$$= - (S_{RealPath} - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$$= - ( \sum_{i=1}^{N} x_{iy_i} + \sum_{i=1}^{N-1} t_{y_iy_{i+1}} - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$ 因此，对于总得分，我们需要一个高效的方法计算: $$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$$ 1.6.1 emission Score和transition Score为了简化公式，我们假设句子的长度为3，即: $x = (w_0,w_1,w_2)$ 假设数据集中只有两个标签，即： $LabelSet = (l_1,l_2)$ 则emission Score矩阵可从BiLSTM层的输出获得，即： $l_1$ $l_2$ $w_0$ $x_{01}$ $x_{02}$ $w_1$ $x_{11}$ $x_{12}$ $w_2$ $x_{21}$ $x_{22}$ 其中$x_{ij}$为单元$w_i$被标记为$l_j$的得分。 而且，我们可以从CRF层中得到transition Score矩阵，即: $l_1$ $l_2$ $l_1$ $t_{11}$ $t_{12}$ $l_2$ $t_{21}$ $t_{22}$ 其中$t_{ij}$为标签$i$到标签$j$的得分。 1.6.2 公式推导记住我们的目标是计算: $\log(e^{S_1} + e^{S_2} + … + e^{S_N})$ 很显然，我们可以使用动态规划思想进行计算（如果你不了解动态规划，没关系，本文将一步一步地进行解释，当然还是建议你学习下动态规划算法）。简而言之，首先，我们计算$w_0$的所有可能序列的总得分。接着，我们使用上一步的总得分计算$w_0 \rightarrow w_1$的总得分。最后，我们同样使用上一步的总得分计算$w_0 \rightarrow w_1 \rightarrow w_2$的总得分。最后的总得分就是我们想要的总得分。 很明显，我们每一次计算都需要利用到上一步计算得到的结果，因此，接下来，你将看到两个变量: obs: 定义当前单元的信息 previous: 存储上一步计算的最后结果 备注：以下内容如果看不懂的话，结合上面的emission Score矩阵和transition Score矩阵一起看就明白了 首先，我们计算$w_0$: $obs = [x_{01},x_{02}]$$previous = None$ 如果我们的句子只有一个词$w_0$，那么存储上一步结果的$previous$为$None$，另外，对于$w_0$而言，$obs = [x_{01},x_{02}]$，其中$x_{01}$和$x_{02}$分别为emission Score（ＢiLSTM层的输出）。 因此，$w_0$的所有可能标签序列总得分为: $TotalScore(w_0)=\log (e^{x_{01}} + e^{x_{02}})$ 接着，我们计算$w_0 \rightarrow w_1$: $obs = [x_{11},x_{12}]$$previous = [x_{01},x_{02}]$ 为了计算方便，我们将$previous$转变为: $$ previous =\begin{pmatrix}x_{01} &amp; x_{01} \x_{02} &amp; x_{02}\end{pmatrix}$$同样，将$obs$转变为: $$ obs =\begin{pmatrix}x_{11} &amp; x_{12} \x_{11} &amp; x_{12}\end{pmatrix}$$备注：通过矩阵方式计算更高效 接着，我们将$previous,abs$和transition Score进行相加,即:$$scores =\begin{pmatrix}x_{01}&amp;x_{01}\x_{02}&amp;x_{02}\end{pmatrix}+\begin{pmatrix}x_{11}&amp;x_{12}\x_{11}&amp;x_{12}\end{pmatrix}+\begin{pmatrix}t_{11}&amp;t_{12}\t_{21}&amp;t_{22}\end{pmatrix}$$ 接着，可得到: $$scores =\begin{pmatrix}x_{01}+x_{11}+t_{11}&amp;x_{01}+x_{12}+t_{12}\x_{02}+x_{11}+t_{21}&amp;x_{02}+x_{12}+t_{22}\end{pmatrix}$$从而我们可得到当前的$previous$为: $$previous=[\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}), \log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})]$$ 实际上，第二步已经算完了，可能还有人还无法理解如何得到$w_0$到$w_1$的所有可能序列组合（$label_1 \rightarrow label_1, label_1 \rightarrow label_2 , label_2 \rightarrow label_1, label_2 \rightarrow label_2$）的总得分，其实你主要按照以下计算方式即可; $TotalScore(w_0 → w_1)$ $=\log (e^{previous[0]} + e^{previous[1]})$ $=\log (e^{\log(e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})}+e^{\log(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})})$ $=\log(e^{x_{01}+x_{11}+t_{11}}+e^{x_{02}+x_{11}+t_{21}}+e^{x_{01}+x_{12}+t_{12}}+e^{x_{02}+x_{12}+t_{22}})$ 很明显，与$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$很相似。 在上述公式中，我们可以看到: $S_1 = x_{01}+x_{11}+t_{11}$ ($label_1$ → $label_1$) $S_2 = x_{02}+x_{11}+t_{21}$ ($label_2$ → $label_1$) $S_3 = x_{01}+x_{12}+t_{12}$ ($label_1$ → $label_2$) $S_4 = x_{02}+x_{12}+t_{22}$ ($label_2$ → $label_2$) 接着我们计算$w_0$ → $w_1$ → $w_2$: 如果你理解了上一步的计算过程的话，其实这一步的计算与上一步类似。即： $obs = [x_{21}, x_{22}]$ $previous=[\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}), \log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})]$ 类似于第二步，我们将$previous$转化为: $$previous =\begin{pmatrix}\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})&amp;\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})\\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})&amp;\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})\end{pmatrix}$$ 同样，将$obs$转化为: $$obs =\begin{pmatrix}x_{21}&amp;x_{22}\x_{21}&amp;x_{22}\end{pmatrix}$$ 将$previous，obs$和transition Score进行相加，即: $scores =$$\begin{pmatrix}\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})&amp;\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})\\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})&amp;\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})\end{pmatrix}$$+$$\begin{pmatrix}x_{21}&amp;x_{22}\x_{21}&amp;x_{22}\end{pmatrix}$$+$$\begin{pmatrix}t_{11}&amp;t_{12}\t_{21}&amp;t_{22}\end{pmatrix}$ 更新$previous$为: $previous = [\log(e^{\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}) + x_{22} + t_{12}}+e^{\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}}) + x_{22} + t_{22}})]$$=\log( (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{22} + t_{12}} + (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{22} + t_{22}})]$ 当计算到最后一步时，我们使用新的$previous$计算总得分: $TotalScore(w_0 → w_1 → w_2)$ $=\log (e^{previous[0]} + e^{previous[1]})$ $=\log (e^{\log((e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{21} + t_{11}}+(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{21} + t_{21}})}$ $+e^{\log((e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{22} + t_{12}}+(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{22} + t_{22}})})$ $=\log (e^{x_{01}+x_{11}+t_{11}+x_{21}+t_{11}}+e^{x_{02}+x_{11}+t_{21}+x_{21}+t_{11}}$$+e^{x_{01}+x_{12}+t_{12}+x_{21}+t_{21}}+e^{x_{02}+x_{12}+t_{22}+x_{21}+t_{21}}$$+e^{x_{01}+x_{11}+t_{11}+x_{22}+t_{12}}+e^{x_{02}+x_{11}+t_{21}+x_{22}+t_{12}}$$+e^{x_{01}+x_{12}+t_{12}+x_{22}+t_{22}}+e^{x_{02}+x_{12}+t_{22}+x_{22}+t_{22}})$ 到这里，我们就完成 了$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$的计算过程。 参考文献[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360.https://arxiv.org/abs/1603.01360]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>深度学习</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BiLSTM模型中CRF层的运行原理(一)]]></title>
    <url>%2F2018%2F12%2F31%2FCRF-Layer-on-the-Top-of-BiLSTM%2B--1%2F</url>
    <content type="text"><![CDATA[本文主要内容如下: 介绍: 在命名实体识别任务中，BiLSTM模型中CRF层的通用思想 实例: 通过实例来一步步展示CRF的工作原理 实现: CRF层的一步步实现过程 备注: 需要有的基础知识：你只需要知道什么是命名实体识别，如果你不懂神经网络，条件随机场（CRF）或者其它相关知识，不必担心，本文将向你展示CRF层是如何工作的。本文将尽可能的讲的通俗易懂。 1.介绍基于神经网络的方法，在命名实体识别任务中非常流行和普遍。在文献[1]中，作者提出了BiLSTM-CRF模型用于实体识别任务中，在模型中用到了字嵌入和词嵌入。本文将向你展示CRF层是如何工作的。 如果你不知道BiLSTM和CRF是什么，你只需要记住他们分别是命名实体识别模型中的两个层。 1.1开始之前我们假设我们的数据集中有两类实体——人名和地名，与之相对应在我们的训练数据集中，有五类标签： B-Person I-Person B-Organization I-Organization O 假设句子$x$由5个字符组成，即$x = (w_0,w_1,w_2,w_3,w_4)$，其中$[w_0,w_1]$为人名实体，$[w_3]$为组织实体，其他字符的标签为”O”。 1.2 BiLSTM-CRF 模型首先，对该模型进行简单的介绍，具体的模型结构，如下图所示：​ 从上图中，总的来说可以总结为以下两点： 句子$x$中的每一个单元都代表着由字嵌入或词嵌入构成的向量。其中，字嵌入一般是随机初始化的，而词嵌入一般是通过一个预训练好的词向量模型得到的。所有的嵌入在训练过程中都会调整到最优。 这些字或词嵌入作为BiLSTM-CRF模型的输入，输出的是句子$x$中每个单元的标签。 为了更容易了解CRF层的运行原理，我们需要知道BiLSTM的输出层。 如上图所示，BiLSTM层的输出为句子$x$中的每一个单元每一个标签的预测分值，例如，对于单元$w_0$，BiLSTM层输出的是1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) and 0.05 (O). 这些分值将作为CRF层的输入。 1.3 如果没有CRF层会怎样你也许已经发现了，即使没有CRF层，我们也可以训练一个BiLSTM命名实体识别模型，如图下图所示： 由于BiLSTM的输出为每个单元的每一个标签预测分值，我们可以挑选分值最高的一个作为该单元的标签。例如，对于单元$w_0$,“B-Person”标签分值最高——1.5，因此我们可以将“B-Person”作为$w_0$的预测标签。同理，我们可以得到$w_1$—“I-Person”，$w_2$— “O” ，$w_3$—“B-Organization”，$w_4$—“O”。 虽然我们可以得到句子$x$中每个单元的正确标签，但是我们不能保证标签每次都是预测正确的。例如，如下图所示: 很显然，标签序列“I-Organization I-Person” and “B-Organization I-Person”是错误的。 1.4 CRF层能从训练数据中学到约束规则CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合理的。在训练过程中，这些约束可以通过CRF层自动学习到。 这些约束可以是： 句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-” 标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列. 标签序列“O I-label” 是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。有了这些约束，标签序列预测中不合理序列出现的概率将会大大降低。 下一节，将通过CRF层的损失函数，解释CRF层如何从训练数据集中学习到这些约束。 参考文献[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360.https://arxiv.org/abs/1603.01360]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>深度学习</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析pytorch中的LSTM(GRU)]]></title>
    <url>%2F2018%2F11%2F30%2F%E6%B5%85%E6%9E%90pytorch%E4%B8%AD%E7%9A%84LSTM(GRU)%20%2F</url>
    <content type="text"><![CDATA[Introduction本文介绍pytorch RNN网络搭建，主要包括LSTM和GRU的使用。 最近从tensorflow入坑pytorch，发现两者的RNN模块前向传播有些不同。特别是RNN中的pack_padded_sequence和pad_packed_sequence。 Backgroundtensorflow的BILSTM以BILSTM为例，tensorflow中我们一般这样操作： 先定义个前向层 1self.fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size) 在定义一个后向层 1self.bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size) 然后前向传播是这个样子的 123456789101112131415161718192021def call(self, inputs, seq_length, training): # 嵌入层 embedded_words = self.embeddings(inputs) # RNN层 outputs, final_state = tf.nn.bidirectional_dynamic_rnn( self.fw_cell, self.bw_cell, inputs=embedded_words, # 句子原始长度（batch_size，1） sequence_length=seq_length, dtype=tf.float32, time_major=False) # 合并前后向的结果 # outputs是一个列表:[（batch_size, cell_size）,(...)] # len(output) = time_steps outputs = tf.concat(outputs, axis=2) # 由于采用的是dynamic_rnn，padding部分自动被截断了 # 这里取最后一维就好了 final_output = outputs[-1] logits = self.Dense(final_output) return logits 可以看到，tensorflow的dynamic LSTM有两个特点： 前向传播时，不看词向量维度的话，输入是个二维（batch_size, time_steps) padding部分不参与计算，会被自动截断 with this in our mind, let‘s see what’s the bilstm in pytorch。 Pytorch BILSTMLSTM定义一般这样操作： 先定义个LSTM 12345self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension, hidden_size=hidden_size, num_layers=num_layer, batch_first=True, bidirectional=bi_flag) 双向的怎么办呢, 改个参数就行了 1bidirectional=True 多层呢 1num_layers=True 可以看到，pytorch的LSTM把所有的功能整合到一起了，不需要定义前向后向（当然我们也可以搞两个单向的）。此外，我们除了要传递隐藏层的个数cell_size, 还需要将词向量的维度传递进来，这是为什么呢？ 这还不是关键。关键在于pytorch的动态padding机制。因为我们一般是将输入作为batch传进来的，对于变长的文本来说，padding是不可避免的。但是在我们使用LSTM进行计算时，是不希望padding部分参与计算的（他们不是真实的文本，假如纳入计算，会引入不必要的噪声和不必要的计算量）。tensorflow采用dynamic LSTM很好地解决了这一问题。pytorch当然也有他自己的一套，下面来看看。 pack_padded_sequence我们从embedding层拿到的输入维度为（batch_size, time_steps, word_embedding_dimension）, 并不将他直接喂给LSTM，而是要预加工一下，这时候第一个重要的函数pack_padded_sequence就登场了，可以将其看成一个截断函数，作用就是将padding部分截断。不仅如此，阶段后会将输入拉平，变成（batch_size*time_steps, word_embedding_dimension）, 仔细看一下，降了一个维度。来看实例： 123456789import torchfrom torch.nn.utils.rnn import pack_padded_sequence # 手动造个张量, 包含3个序列，长度分别为10,5,3，使用0进行paddingx = torch.FloatTensor([[[1],[2],[3],[4],[5],[6],[7],[8],[8],[9]],[[1], [2],[3],[4],[5],[0],[0],[0],[0],[0]],[[5],[4],[6],[0],[0],[0],[0],[0],[0],[0]]])print("x shape is：", x.shape)# x 的真实长度length = torch.LongTensor([10, 5,3])x_packed = pack_padded_sequence(x, length,batch_first=True)print(x_packed) 结果为： 12345678910111213141516171819x shape is：（3，10，1）PackedSequence(data=tensor([[1.], [1.], [5.], [2.], [2.], [4.], [3.], [3.], [6.], [4.], [4.], [5.], [5.], [6.], [7.], [8.], [8.], [9.]]), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1, 1, 1, 1])) 这里有几点要说一下： 输入的张量是已经按照长度排序的！这是必须的，否则pack_padded_sequence函数会报错！一般而言排序有两种做法，一是使用torch.sort函数，第二是在构造batch时使用torchtext进行batch内排序，见我上篇博文 经过pack，输入发生了三个变化。一是由三维变成了2维(18,1)，二是padding部分的0没有了，三是对序列进行了拼接，这其实是维度降低的结果 pack完之后的结果是个tuple，tuple[0]是数据，是按列进行拼接的，tuple[1]是batch_size, 跟我们之前的batch_size是不一样的，这是为了我们后面可以还原 好了，现在pddding问题解决了，我们将它输入进LSTM，前向传播是这样的： 123456789101112def forward(self, inputs, length): """前向传播""" embeddings = self.embedding(inputs, length) # (batch_size, time_steps, embedding_dim) # 去除padding元素 # embeddings_packed: (batch_size*time_steps, embedding_dim) embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=True) output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0)) # padded_output: (batch_size, time_steps, hidden_size * bi_num) # h_n|c_n: (num_layer*bi_num, batch_size, hidden_size) padded_output, length = pad_packed_sequence(output, batch_first=True) # 取最后一个有效输出作为最终输出（0为无效输出） last_output = padded_output[0] 这里做下参数说明： batch_first 类型：bool， True，输入的维度为（batch_size，time_steps, word_embedding_dimension）；False，输入维度为（time_steps, batch_size, word_embedding_dimension） h_0: 初始化隐藏态 c_0: 初始化细胞态 output：输出，为一个tuple，后面会用例子说明 h_n: 最后个隐藏态 c_n: 最后个细胞态 还是以上面我们造的那个张量为例，看下输出： 1234567import torch.nn as nnbirnn = nn.LSTM(input_size=1, hidden_size=8, bidirectional=True)output, (h_n, c_n) = birnn(x_packed)print('output:\n', output)print('h_n shape:\n',h_n.shape)print('c_n shape:\n',c_n.shape) 结果为： 12345678910111213141516171819202122232425262728293031323334353637383940414243output: PackedSequence(data=tensor([[-0.0087, 0.0716, 0.0069, -0.0040, -0.1375, 0.0404, 0.0757, 0.0291, 0.0619, 0.0213, 0.1517, 0.0241, 0.2986, -0.2594, -0.1432, -0.1742], [-0.0087, 0.0716, 0.0069, -0.0040, -0.1375, 0.0404, 0.0757, 0.0291, 0.0486, 0.0195, 0.1454, 0.0138, 0.2596, -0.2467, -0.1491, -0.1596], [-0.1019, 0.1606, -0.0482, -0.0519, -0.1932, 0.2632, -0.0423, 0.0774, 0.0434, 0.0142, 0.0915, -0.0455, 0.2970, -0.6665, -0.0578, -0.0521], [-0.0895, 0.1504, -0.0069, -0.0137, -0.2248, 0.1365, 0.1002, 0.0918, 0.0730, 0.0247, 0.1526, 0.0160, 0.3559, -0.4301, -0.1375, -0.1429], [-0.0895, 0.1504, -0.0069, -0.0137, -0.2248, 0.1365, 0.1002, 0.0918, 0.0517, 0.0197, 0.1431, -0.0005, 0.2929, -0.4016, -0.1401, -0.1218], [-0.1855, 0.2300, -0.0625, -0.0508, -0.2488, 0.2741, -0.0638, 0.1441, 0.0239, 0.0137, 0.0786, -0.0524, 0.2311, -0.5685, -0.0673, -0.0485], [-0.1621, 0.2254, -0.0318, -0.0240, -0.2693, 0.2207, 0.0843, 0.1487, 0.0789, 0.0259, 0.1354, 0.0050, 0.3851, -0.5676, -0.1110, -0.1117], [-0.1621, 0.2254, -0.0318, -0.0240, -0.2693, 0.2207, 0.0843, 0.1487, 0.0439, 0.0172, 0.1198, -0.0221, 0.2856, -0.5105, -0.1089, -0.0852], [-0.1346, 0.3093, -0.0775, -0.0678, -0.2616, 0.2946, -0.1167, 0.1699, -0.0280, 0.0067, 0.0333, -0.0924, 0.1308, -0.5332, -0.0237, -0.0201], [-0.1886, 0.2883, -0.0563, -0.0352, -0.2872, 0.2700, 0.0431, 0.1887, 0.0803, 0.0254, 0.1112, -0.0077, 0.3929, -0.6681, -0.0804, -0.0828], [-0.1886, 0.2883, -0.0563, -0.0352, -0.2872, 0.2700, 0.0431, 0.1887, 0.0194, 0.0129, 0.0852, -0.0521, 0.2364, -0.5524, -0.0720, -0.0538], [-0.1767, 0.3363, -0.0719, -0.0475, -0.2858, 0.2911, -0.0109, 0.2141, 0.0779, 0.0234, 0.0866, -0.0215, 0.3831, -0.7393, -0.0539, -0.0580], [-0.1767, 0.3363, -0.0719, -0.0475, -0.2858, 0.2911, -0.0109, 0.2141, -0.0274, 0.0076, 0.0448, -0.0817, 0.1408, -0.4683, -0.0382, -0.0285], [-0.1470, 0.3710, -0.0771, -0.0605, -0.2693, 0.2962, -0.0684, 0.2291, 0.0724, 0.0204, 0.0651, -0.0356, 0.3568, -0.7885, -0.0341, -0.0385], [-0.1142, 0.3956, -0.0744, -0.0727, -0.2434, 0.2935, -0.1226, 0.2366, 0.0636, 0.0164, 0.0477, -0.0497, 0.3136, -0.8199, -0.0208, -0.0245], [-0.0854, 0.4126, -0.0669, -0.0829, -0.2140, 0.2871, -0.1699, 0.2389, 0.0496, 0.0118, 0.0343, -0.0641, 0.2542, -0.8307, -0.0124, -0.0151], [-0.0847, 0.4176, -0.0620, -0.0877, -0.2053, 0.2863, -0.2063, 0.2468, 0.0281, 0.0090, 0.0254, -0.0784, 0.1827, -0.7871, -0.0100, -0.0116], [-0.0620, 0.4284, -0.0550, -0.0931, -0.1827, 0.2785, -0.2409, 0.2438, -0.0278, 0.0044, 0.0120, -0.1073, 0.0924, -0.6543, -0.0047, -0.0063]], grad_fn=&lt;CatBackward&gt;), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1, 1, 1, 1]))h_n shape: torch.Size([2, 3, 8])c_n shape: torch.Size([2, 3, 8]) output由一个tuple组成，第一个元素就是输出，这里的维度为torch.Size([18, 16])，第二个元素和我们之前pack时的batch_size参数一样。 h_n和c_n分别为两个三维张量，维度如上，因为是双向，第一维为2. 到这里是不是就结束了呢，当然不是，pack完了之后经过LSTM输出了结果，看上去很费劲，因为序列连一起了，batch都没了。所以我们还原，这时候就用到pack_padded_sequence的好基友pad_packed_sequence了。 pad_packed_sequencepad_packed_sequence可以看成是解压缩操作。从上面可以看到，h_n, c_n已经是正常维度的张量了，没有pack，当然也用不着pad。我们只需对output做pad。 12345padded_output, length = pad_packed_sequence(output, batch_first=True)print('padded_output\n', padded_output)print('padded_output shape\n', padded_output.shape)print('length\n',length) 结果为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697padded_output tensor([[[-0.0087, 0.0716, 0.0069, -0.0040, -0.1375, 0.0404, 0.0757, 0.0291, 0.0619, 0.0213, 0.1517, 0.0241, 0.2986, -0.2594, -0.1432, -0.1742], [-0.0895, 0.1504, -0.0069, -0.0137, -0.2248, 0.1365, 0.1002, 0.0918, 0.0730, 0.0247, 0.1526, 0.0160, 0.3559, -0.4301, -0.1375, -0.1429], [-0.1621, 0.2254, -0.0318, -0.0240, -0.2693, 0.2207, 0.0843, 0.1487, 0.0789, 0.0259, 0.1354, 0.0050, 0.3851, -0.5676, -0.1110, -0.1117], [-0.1886, 0.2883, -0.0563, -0.0352, -0.2872, 0.2700, 0.0431, 0.1887, 0.0803, 0.0254, 0.1112, -0.0077, 0.3929, -0.6681, -0.0804, -0.0828], [-0.1767, 0.3363, -0.0719, -0.0475, -0.2858, 0.2911, -0.0109, 0.2141, 0.0779, 0.0234, 0.0866, -0.0215, 0.3831, -0.7393, -0.0539, -0.0580], [-0.1470, 0.3710, -0.0771, -0.0605, -0.2693, 0.2962, -0.0684, 0.2291, 0.0724, 0.0204, 0.0651, -0.0356, 0.3568, -0.7885, -0.0341, -0.0385], [-0.1142, 0.3956, -0.0744, -0.0727, -0.2434, 0.2935, -0.1226, 0.2366, 0.0636, 0.0164, 0.0477, -0.0497, 0.3136, -0.8199, -0.0208, -0.0245], [-0.0854, 0.4126, -0.0669, -0.0829, -0.2140, 0.2871, -0.1699, 0.2389, 0.0496, 0.0118, 0.0343, -0.0641, 0.2542, -0.8307, -0.0124, -0.0151], [-0.0847, 0.4176, -0.0620, -0.0877, -0.2053, 0.2863, -0.2063, 0.2468, 0.0281, 0.0090, 0.0254, -0.0784, 0.1827, -0.7871, -0.0100, -0.0116], [-0.0620, 0.4284, -0.0550, -0.0931, -0.1827, 0.2785, -0.2409, 0.2438, -0.0278, 0.0044, 0.0120, -0.1073, 0.0924, -0.6543, -0.0047, -0.0063]], [[-0.0087, 0.0716, 0.0069, -0.0040, -0.1375, 0.0404, 0.0757, 0.0291, 0.0486, 0.0195, 0.1454, 0.0138, 0.2596, -0.2467, -0.1491, -0.1596], [-0.0895, 0.1504, -0.0069, -0.0137, -0.2248, 0.1365, 0.1002, 0.0918, 0.0517, 0.0197, 0.1431, -0.0005, 0.2929, -0.4016, -0.1401, -0.1218], [-0.1621, 0.2254, -0.0318, -0.0240, -0.2693, 0.2207, 0.0843, 0.1487, 0.0439, 0.0172, 0.1198, -0.0221, 0.2856, -0.5105, -0.1089, -0.0852], [-0.1886, 0.2883, -0.0563, -0.0352, -0.2872, 0.2700, 0.0431, 0.1887, 0.0194, 0.0129, 0.0852, -0.0521, 0.2364, -0.5524, -0.0720, -0.0538], [-0.1767, 0.3363, -0.0719, -0.0475, -0.2858, 0.2911, -0.0109, 0.2141, -0.0274, 0.0076, 0.0448, -0.0817, 0.1408, -0.4683, -0.0382, -0.0285], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], [[-0.1019, 0.1606, -0.0482, -0.0519, -0.1932, 0.2632, -0.0423, 0.0774, 0.0434, 0.0142, 0.0915, -0.0455, 0.2970, -0.6665, -0.0578, -0.0521], [-0.1855, 0.2300, -0.0625, -0.0508, -0.2488, 0.2741, -0.0638, 0.1441, 0.0239, 0.0137, 0.0786, -0.0524, 0.2311, -0.5685, -0.0673, -0.0485], [-0.1346, 0.3093, -0.0775, -0.0678, -0.2616, 0.2946, -0.1167, 0.1699, -0.0280, 0.0067, 0.0333, -0.0924, 0.1308, -0.5332, -0.0237, -0.0201], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=&lt;TransposeBackward0&gt;)padded_output shape torch.Size([3, 10, 16])length tensor([10, 5, 3]) 是不是又回来了？16是cell_size*2, 因为是双向。 可以看到，没参与计算后来被补上来的部分都成为了0！ 完整代码tensorflow123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182mport osimport timeimport numpy as npimport tensorflow as tffrom util.embedding_util import get_embeddingfrom util.plot_util import loss_acc_plotfrom util.lr_util import lr_updateimport config.lstm_config as configclass BILSTM(tf.keras.Model): def __init__(self, cell_size, checkpoint_dir, num_classes, model_type, vocab_size, word2id, embedding_dim, keep_prob): super().__init__() self.checkpoint_dir = checkpoint_dir self.history = &#123;&#125; self.keep_prob = keep_prob # embedding layer weights = get_embedding(model_type=model_type, word2id=word2id, embedding_dim=embedding_dim) if model_type == 'static': self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=False) elif model_type == 'non-static': self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=True) elif model_type == 'rand': self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=True) elif model_type == 'multichannel': pass else: raise ValueError('unknown model type') # BILSTM layer self.fw_cell = tf.nn.rnn_cell.DropoutWrapper( tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=0.7) self.bw_cell = tf.nn.rnn_cell.DropoutWrapper( tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=0.7) self.Dense = tf.layers.Dense(units=num_classes, activation=None) def call(self, inputs, seq_length, training): embedded_words = self.embeddings(inputs) outputs, final_state = tf.nn.bidirectional_dynamic_rnn( self.fw_cell, self.bw_cell, inputs=embedded_words, sequence_length=seq_length, dtype=tf.float32, time_major=False) outputs = tf.concat(outputs, axis=2) final_output = outputs[-1] logits = self.Dense(final_output) return logits def loss_fn(self, inputs, target, seq_length, training): preds = self.call(inputs, seq_length, training) # L2正则化 loss_L2 = tf.add_n([tf.nn.l2_loss(v) for v in self.trainable_variables if 'bias' not in v.name]) * 0.001 loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds) loss = loss + loss_L2 return loss def grads_fn(self, inputs, target, seq_length, training): with tf.GradientTape() as tape: loss = self.loss_fn(inputs, target, seq_length, training) return tape.gradient(loss, self.variables) def save_model(self, model): """ Function to save trained model. """ checkpoint = tf.train.Checkpoint(model=model) checkpoint_prefix = os.path.join(self.checkpoint_dir, 'ckpt') checkpoint.save(file_prefix=checkpoint_prefix) def restore_model(self): # Run the model once to initialize variables dummy_input = tf.constant(tf.zeros((1, 1))) dummy_length = tf.constant(1, shape=(1,)) self(dummy_input, dummy_length, False) # Restore the variables of the model saver = tf.contrib.Saver(self.variables) saver.restore(tf.train.latest_checkpoint (self.checkpoint_directory)) def get_accuracy(self, inputs, target, seq_length, training): y = self.call(inputs, seq_length, training) y_pred = tf.argmax(y, axis=1) correct = tf.where(tf.equal(y_pred, target)).numpy().shape[0] total = target.numpy().shape[0] return correct/total def fit(self, training_data, eval_data, pbar, num_epochs=100, early_stopping_rounds=5, verbose=1, train_from_scratch=True): """train the model""" if train_from_scratch is False: self.restore_model() # Initialize best loss. This variable will store the lowest loss on the # eval dataset. best_loss = 2018 # Initialize classes to update the mean loss of train and eval train_loss = [] eval_loss = [] train_accuracy = [] eval_accuracy = [] # Initialize dictionary to store the loss history self.history['train_loss'] = [] self.history['eval_loss'] = [] self.history['train_accuracy'] = [] self.history['eval_accuracy'] = [] count = early_stopping_rounds # Begin training for i in range(num_epochs): # 在每个epoch训练之初初始化optimizer，决定是否使用学习率衰减 learning_rate = lr_update(i+1, mode=config.lr_mode) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Training with gradient descent start = time.time() for index, (sequence, label, seq_length) in enumerate(training_data): # cpu需要类型转换，不然会报错：Could not find valid device sequence = tf.cast(sequence, dtype=tf.float32) label = tf.cast(label, dtype=tf.int64) grads = self.grads_fn(sequence, label, seq_length, training=True) optimizer.apply_gradients(zip(grads, self.variables)) pbar.show(index, use_time=time.time()-start) # Compute the loss on the training data after one epoch for sequence, label, seq_length in training_data: sequence = tf.cast(sequence, dtype=tf.float32) label = tf.cast(label, dtype=tf.int64) train_los = self.loss_fn(sequence, label, seq_length, training=False) train_acc = self.get_accuracy(sequence, label, seq_length, training=False) train_loss.append(train_los) train_accuracy.append(train_acc) self.history['train_loss'].append(np.mean(train_loss)) self.history['train_accuracy'].append(np.mean(train_accuracy)) # Compute the loss on the eval data after one epoch for sequence, label, seq_length in eval_data: sequence = tf.cast(sequence, dtype=tf.float32) label = tf.cast(label, dtype=tf.int64) eval_los = self.loss_fn(sequence, label, seq_length, training=False) eval_acc = self.get_accuracy(sequence, label, seq_length, training=False) eval_loss.append(eval_los) eval_accuracy.append(eval_acc) self.history['eval_loss'].append(np.mean(eval_loss)) self.history['eval_accuracy'].append(np.mean(eval_accuracy)) # Print train and eval losses if (i == 0) | ((i + 1) % verbose == 0): print('Epoch %d - train_loss: %4f - eval_loss: %4f - train_acc:%4f - eval_acc:%4f' % (i + 1, self.history['train_loss'][-1], self.history['eval_loss'][-1], self.history['train_accuracy'][-1], self.history['eval_accuracy'][-1])) # Check for early stopping if self.history['eval_loss'][-1] &lt; best_loss: best_loss = self.history['eval_loss'][-1] count = early_stopping_rounds else: count -= 1 if count == 0: break # 画出loss_acc曲线 loss_acc_plot(history=self.history) pytorch123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import osimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequencefrom sklearn.metrics import f1_scoreimport numpy as npimport config.config as configfrom util.embedding_util import get_embeddingtorch.manual_seed(2018)torch.cuda.manual_seed(2018)torch.cuda.manual_seed_all(2018)np.random.seed(2018)os.environ["CUDA_VISIBLE_DEVICE"] = "1"class RNN(nn.Module): def __init__(self, vocab_size, word_embedding_dimension, hidden_size, bi_flag, num_layer, labels, cell_type, dropout, checkpoint_dir): super(RNN, self).__init__() self.labels = labels self.num_label = len(labels) self.num_layer = num_layer self.hidden_size = hidden_size self.dropout = dropout self.checkpoint_dir = checkpoint_dir if torch.cuda.is_available(): self.device = torch.device("cuda") self.embedding = nn.Embedding(vocab_size, word_embedding_dimension) for p in self.embedding.parameters(): p.requires_grad = False self.embedding.weight.data.copy_(torch.from_numpy(get_embedding(vocab_size, word_embedding_dimension))) if cell_type == "LSTM": self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension, hidden_size=hidden_size, num_layers=num_layer, batch_first=True, dropout=dropout, bidirectional=bi_flag) elif cell_type == "GRU": self.rnn_cell = nn.GRU(input_size=word_embedding_dimension, hidden_size=hidden_size, num_layers=num_layer, batch_first=True, dropout=dropout, bidirectional=bi_flag) else: raise TypeError("RNN: Unknown rnn cell type") # 是否双向 self.bi_num = 2 if bi_flag else 1 self.linear = nn.Linear(hidden_size*self.bi_num, self.num_label) def forward(self, inputs, length): batch_size = inputs.shape[0] # 初始化态h和C,默认为zeros h_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float() c_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float() embeddings = self.embedding(inputs, length) # (batch_size, time_steps, embedding_dim) # 去除padding元素 # embeddings_packed: (batch_size*time_steps, embedding_dim) embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=True) output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0)) # padded_output: (batch_size, time_steps, hidden_size * bi_num) # h_n|c_n: (num_layer*bi_num, batch_size, hidden_size) padded_output, _ = pad_packed_sequence(output, batch_first=True) # 取最后一个有效输出作为最终输出（0为无效输出） last_output = padded_output[torch.LongTensor(range(batch_size)), length] last_output = F.dropout(last_output, p=self.dropout, training=self.training) output = self.linear(last_output) return output def load(self): self.load_state_dict(torch.load(self.checkpoint_dir)) def save(self): torch.save(self.state_dict(), self.checkpoint_dir) def evaluate(self, y_pred, y_true): _, y_pred = torch.max(y_pred.data, 1) if config.use_cuda: y_true = y_true.cpu().numpy() y_pred = y_pred.cpu().numpy() else: y_true = y_true.numpy() y_pred = y_pred.numpy() f1 = f1_score(y_true, y_pred, labels=self.labels, average="macro") correct = np.sum((y_true==y_pred).astype(int)) acc = correct/y_pred.shape[0] return (acc, f1) 各位晚安~]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch，lstm，nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[torchtext读取文本数据集]]></title>
    <url>%2F2018%2F11%2F28%2Ftorchtext%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[Introduction本文主要介绍如何使用Torchtext读取文本数据集。 Torchtext是非官方的、一种为pytorch提供文本数据处理能力的库， 类似于图像处理库Torchvision。 Install 下载地址：https://github.com/text 安装：pip install text-master.zip 测试安装是否成功： import torchtext How To Use概览 先上一张图。使用tortext的目的是将文本转换成Batch，方便后面训练模型时使用。过程如下: 使用Field对象进行文本预处理， 生成example 使用Dataset类生成数据集dataset 使用Iterator生成迭代器 从图中还可以看到，torchtext可以生成词典vocab和词向量embedding，但个人比较喜欢将这两步放在数据预处理和模型里面进行，所以这两个功能不在本文之列。 常用的类12from torchtext.data import Field, Example, TabularDatasetfrom torchtext.data import BucketIterator Field：用来定义字段以及文本预处理方法 Example: 用来表示一个样本，通常为“数据+标签” TabularDataset: 用来从文件中读取数据，生成Dataset， Dataset是Example实例的集合 BucketIterator：迭代器，用来生成batch， 类似的有Iterator，Buckeiterator的功能较强大点，支持排序，动态padding等 数据准备见我上篇博文&lt;文本预处理&gt;。使用生成的train.tsv和valid.tsv。 使用步骤创建Field对象123456789101112131415def x_tokenize(x): # 如果加载进来的是已经转成id的文本 # 此处必须将字符串转换成整型 # 否则必须将use_vocab设为True return [int(c) for c in x.split()]def y_tokenize(y): return int(y)TEXT = Field(sequential=True, tokenize=x_tokenize, use_vocab=False, batch_first=True, fix_length=self.fix_length, eos_token=None, init_token=None, include_lengths=True, pad_token=0)LABEL = Field(sequential=False, tokenize=y_tokenize, use_vocab=False, batch_first=True) 参数说明 sequential 类型boolean, 作用：是否为序列，一般文本都为True，标签为False tokenize 类型: function， 作用: 文本处理，默认为str.split(), 这里对x和y分别自定义了处理函数。 use_vocab： 类型: boolean， 作用：是否建立词典 batch_first：类型: boolean， 作用：为True则返回Batch维度为(batch_size， 文本长度), False 则相反 fix_length：类型: int, 作用：固定文本的长度，长则截断，短则padding，可认为是静态padding；为None则按每个Batch内的最大长度进行动态padding。 eos_token：类型：str, 作用: 句子结束字符 init_token：类型：str, 作用: 句子开始字符 include_lengths：类型: boolean， 作用：是否返回句子的原始长度，一般为True，方便RNN使用。 pad_token：padding的字符，默认为”“, 这里因为原始数据已经转成了int类型，所以使用0。注意这里的pad_token要和你的词典vocab里的“”的Id保持一致，否则会影响后面词向量的读取。 读取文件生成数据集12345678910fields = [ ("label", LABEL), ("text", TEXT)]train, valid = TabularDataset.splits( path=config.ROOT_DIR, train=self.train_path, validation=self.valid_path, format='tsv', skip_header=False, fields=fields)return train, valid 生成迭代器123456train_iter, val_iter = BucketIterator.splits((train, valid), batch_sizes=(self.batch_size, self.batch_size), device = torch.device("cpu"), sort_key=lambda x: len(x.text), # field sorted by len sort_within_batch=True, repeat=False) 这里要注意的是sort_with_batch要设置为True，并指定排序的key为文本长度，方便后面pytorch RNN进行pack和pad。 我们来看下train_iter和val_iter里放了什么东西。 12345678bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=1, fix_length=None)train, valid = bi.create_dataset()train_iter, valid_iter = bi.get_iterator(train, valid)batch = next(iter(train_iter))print(train_iter)print('batch:\n', batch)print('batch_text:\n', batch.text)print('batch_label:\n', batch.label) 结果为： 123456789101112131415161718192021&lt;torchtext.data.iterator.BucketIterator object at 0x7f04a9d845f8&gt;batch:[torchtext.data.batch.Batch of size 1] [.label]:[torch.LongTensor of size 1] [.text]:('[torch.LongTensor of size 1x125]', '[torch.LongTensor of size 1]')batch_text: (tensor([[11149, 7772, 13752, 13743, 13773, 13793, 13791, 13591, 12478, 13759, 13783, 13492, 13793, 13745, 13754, 13612, 7452, 12185, 13789, 13784, 13765, 12451, 12112, 13620, 12240, 13073, 13790, 13738, 13637, 13759, 13776, 13793, 13739, 13783, 13787, 13793, 12702, 13790, 13698, 13774, 13792, 13768, 13715, 13641, 13761, 13713, 13682, 13712, 13786, 13749, 13097, 13734, 13702, 13735, 13257, 13642, 13700, 13793, 13684, 13755, 13488, 13789, 13750, 13484, 13494, 13793, 13624, 13670, 13786, 13655, 13768, 13687, 13774, 13792, 13791, 13591, 13546, 13777, 13658, 13740, 13577, 13790, 13684, 13755, 13793, 13572, 12891, 13793, 13368, 13713, 13682, 13712, 13786, 13786, 13642, 13700, 13793, 13429, 13520, 13613, 13792, 13368, 13790, 13750, 13699, 13764, 13590, 13675, 13742, 13691, 13688, 13742, 13782, 13538, 13742, 13783, 13787, 13774, 13645, 13742, 13791, 13740, 13744, 13750, 13792]]), tensor([125]))batch_label: tensor([11]) 可以看到batch有两个属性，分别为label和text, text是一个元组，第一个元素为文本，第二个元素为文本原始长度（这里因为我们在定义TEXT时使用了include_lengths=True，否则这里只返回文本）， label则是标签。 这里为了方便展示只使用了一个batch，返回的batch维度为（batch_size * length）, 数据格式为LongTensor。如果想看动态padding的效果，可多取几个batch，会发现他们是按照长度进行排序，并且是以0进行padding的。 对Batch包装一下，方便调用通过以上步骤，我们能够得到一个batch。但是很快就发现有个不太方便的地方。我们只能通过batch的属性，即自定义的字段名称，如text和label，来访问数据。这样的话在训练时我们只能这样操作： 123456for e in range(num_epoch): for batch in train_iter: inputs = batch.text[0] label = batch.label length = batch.text[1] pass 万一这个字段改了，还要去改训练的代码，很麻烦，关键是显得很LOW，姿势不对。 怎么办呢？ 我们对获得的iter进行包装一下，就可以避免这个问题了。 1234567891011121314151617181920class BatchWrapper(object): """对batch做个包装，方便调用，可选择性使用""" def __init__(self, dl, x_var, y_vars): self.dl, self.x_var, self.y_vars = dl, x_var, y_vars def __iter__(self): for batch in self.dl: x = getattr(batch, self.x_var) if self.y_vars is not None: temp = [getattr(batch, feat).unsqueeze(1) for feat in self.y_vars] label = torch.cat(temp, dim=1).long() else: raise ValueError('BatchWrapper: invalid label') text = x[0] length = x[1] yield (text, label, length) def __len__(self): return len(self.dl) 我们这样使用： 12train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars)val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars) 这样你就会发现batch不再有text和label属性了，而是一个三元组（text， label， length），调用时 就可以采用如下优雅一点的姿势： 123for e in range(num_epoch): for inputs, label, length in train_iter: pass 完整代码data_loader.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596"""将id格式的输入转换成dataset，并做动态padding"""import torchfrom torchtext.data import Field, TabularDatasetfrom torchtext.data import BucketIteratorimport configdef x_tokenize(x): # 如果加载进来的是已经转成id的文本 # 此处必须将字符串转换成整型 return [int(c) for c in x.split()]def y_tokenize(y): return int(y)class BatchIterator(object): def __init__(self, train_path, valid_path, batch_size, fix_length=None, x_var="text", y_var=["label"], format='tsv'): self.train_path = train_path self.valid_path = valid_path self.batch_size = batch_size self.fix_length = fix_length self.format = format self.x_var = x_var self.y_vars = y_var def create_dataset(self): TEXT = Field(sequential=True, tokenize=x_tokenize, use_vocab=False, batch_first=True, fix_length=self.fix_length, # 如需静态padding,则设置fix_length, 但要注意要大于文本最大长度 eos_token=None, init_token=None, include_lengths=True, pad_token=0) LABEL = Field(sequential=False, tokenize=y_tokenize, use_vocab=False, batch_first=True) fields = [ ("label", LABEL), ("text", TEXT)] train, valid = TabularDataset.splits( path=config.ROOT_DIR, train=self.train_path, validation=self.valid_path, format='tsv', skip_header=False, fields=fields) return train, valid def get_iterator(self, train, valid): train_iter, val_iter = BucketIterator.splits((train, valid), batch_sizes=(self.batch_size, self.batch_size), device = torch.device("cpu"), sort_key=lambda x: len(x.text), # field sorted by len sort_within_batch=True, repeat=False) train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars) val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars) ### batch = iter(train_iter) ### batch： ((text, length), y) return train_iter, val_iterclass BatchWrapper(object): """对batch做个包装，方便调用，可选择性使用""" def __init__(self, dl, x_var, y_vars): self.dl, self.x_var, self.y_vars = dl, x_var, y_vars def __iter__(self): for batch in self.dl: x = getattr(batch, self.x_var) if self.y_vars is not None: temp = [getattr(batch, feat).unsqueeze(1) for feat in self.y_vars] y = torch.cat(temp, dim=1).long() else: raise ValueError('BatchWrapper: invalid label') text = x[0] length = x[1] yield (text, y, length) def __len__(self): return len(self.dl) if __name__ == '__main__': bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=1, fix_length=None) train, valid = bi.create_dataset() train_iter, valid_iter = bi.get_iterator(train, valid) batch = next(iter(train_iter)) print(train_iter) print('batch:\n', batch) print('batch_text:\n', batch.text) print('batch_label:\n', batch.label) config.py 12TRAIN_FILE = 'outputs/intermediate/train.tsv'VALID_FILE = 'outputs/intermediate/valid.tsv']]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>torchtext，pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本预处理]]></title>
    <url>%2F2018%2F11%2F28%2F%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Introduction在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本文介绍如何利用有限内存进行大规模数据处理，主要包括： 建立词典 将单词转换为id 训练集验证集切分 How To Do IT原始数据集123411@成都 高新技术 产业 开发区 人民 检察院 指控 ， 2015年 3 月 29日 23时 许 ， 被告人 刘某 某 饮 酒后 驾驶川 A ＊ ＊ ＊ 84 北京 现代牌 小型 轿车 ， 从 成都市 桐梓林 附近 出发 上 人民 南 路 出 城 ， 当 车 行驶 至 成都 高新区 天府 大道 与 府城 大道 交叉 路口处 时 ， 公诉 机关 认为 ， 被告人 刘 某某 在 道路 上 醉 酒 驾驶 机动车 ， 危害 公共 安全 ， 其 行为 应当 以 ×× 追究 其 刑事 责任 。11@黑龙江省 尚志市 人民 检察院 指控 ： ×× 2014年 9 月 22日 20时 许 ， 被告人 矫 2 某 在 尚志市 苇河镇 阿里郎歌厅 对面 停放 的 货车 的 副 驾驶 座位 上 ， 将 被害人 李某 甲 的 蓝色 女式 拎 包 盗 走 ， 包 内 有 人民币 57000 元 ， 红色 钱包 一个 ， 农业 银行卡 一 张 ， 身份证 一 张 、 驾驶证 一 本 、 账本 一 册 。 案 发 前 ， 被告人 矫 2 某 将 盗走 的 财物 返还 被害人 。 在 ×× 到 五 年 幅度 内 量刑 ， 并 处 罚金 ； 对 所 犯 的 ×× 在 ×× 到 六 个 月 幅度 内 量刑 ， 并 处 罚金 。 针对 上述 指控 ， 公诉 机关 提供 了 相应 的 证据 。... 这里以法研杯比赛的文本数据集为例。格式为 标签@文本 其中，文本已经过分词处理，使用空格分隔。 建立词典123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def sent_label_split(line): """ 句子处理成单词 :param line: 原始行 :return: 单词， 标签 """ line = line.strip('\n').split('@') label = line[0] sent = line[1].split(' ') return sent, labeldef word_to_id(word, word2id): """ 单词--&gt;ID :param word: 单词 :param word2id: word2id @type: dict :return: """ return word2id[word] if word in word2id else word2id['unk']def bulid_vocab(vocab_size, min_freq=3, stop_word_list=None, is_debug=False): """ 建立词典 :param vocab_size: 词典大小 :param min_freq: 最小词频限制 :param stop_list: 停用词 @type：file_path :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试 :return: word2id """ size = 0 count = Counter() with open(os.path.join(config.ROOT_DIR, config.RAW_DATA), 'r') as fr: logger.info('Building vocab') for line in tqdm(fr, desc='Build vocab'): words, label = sent_label_split(line) count.update(words) size += 1 if is_debug: limit_train_size = 10000 if size &gt; limit_train_size: break if stop_word_list: stop_list = &#123;&#125; with open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), 'r') as fr: for i, line in enumerate(fr): word = line.strip('\n') if stop_list.get(word) is None: stop_list[word] = i count = &#123;k: v for k, v in count.items() if k not in stop_list&#125; count = sorted(count.items(), key=operator.itemgetter(1)) # 词典 vocab = [w[0] for w in count if w[1] &gt;= min_freq] if vocab_size &lt; len(vocab): vocab = vocab[:vocab_size] vocab = config.flag_words + vocab logger.info('vocab_size is %d'%len(vocab)) # 词典到编号的映射 word2id = &#123;k: v for k, v in zip(vocab, range(0, len(vocab)))&#125; assert word2id['&lt;pad&gt;'] == 0, "ValueError: '&lt;pad&gt;' id is not 0" print(word2id) with open(config.WORD2ID_FILE, 'wb') as fw: pickle.dump(word2id, fw) return word2id 文本映射到Id123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def text2id(word2id, maxlen=None, valid_size=0.3, random_state=2018, shuffle=True, is_debug=False): """ 训练集文本转ID :param valid_size: 验证集大小 """ print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE)) if len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; 0: logger.info('Text to id file existed') return logger.info('Text to id') sentences, labels, lengths = [], [], [] size = 0 with open(os.path.join(config.ROOT_DIR, config.RAW_DATA), 'r') as fr: for line in tqdm(fr, desc='text_to_id'): words, label = sent_label_split(line) sent = [word_to_id(word=word, word2id=word2id) for word in words] if maxlen: sent = sent[:maxlen] length = len(sent) sentences.append(sent) labels.append(label) lengths.append(length) size += 1 if is_debug: limit_train_size = 10000 if size &gt; limit_train_size: break train, valid = train_val_split(sentences, labels, valid_size=valid_size, random_state=random_state, shuffle=shuffle) del sentences, labels, lengths with open(config.TRAIN_FILE, 'w') as fw: for sent, label in train: sent = [str(s) for s in sent] line = "\t".join[str(label), " ".join(sent)] fw.write(line + '\n') logger.info('Writing train to file done') with open(config.VALID_FILE, 'w') as fw: for sent, label in train: sent = [str(s) for s in sent] line = "\t".join[str(label), " ".join(sent)] fw.write(line + '\n') logger.info('Writing valid to file done') 训练集验证集分割12345678910111213141516171819def train_val_split(X, y, valid_size=0.3, random_state=2018, shuffle=True): """ 训练集验证集分割 :param X: sentences :param y: labels :param random_state: 随机种子 """ logger.info('train val split') data = [(data_x, data_y) for data_x, data_y in zip(X, y)] N = len(data) test_size = int(N * valid_size) if shuffle: random.seed(random_state) random.shuffle(data) valid = data[:test_size] train = data[test_size:] return train, valid 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165import osimport randomimport pickleimport operatorfrom glob import globfrom tqdm import tqdmfrom collections import Counterimport configfrom Logginger import init_loggerlogger = init_logger("torch", logging_path=config.LOG_PATH)def sent_label_split(line): """ 句子处理成单词 :param line: 原始行 :return: 单词， 标签 """ line = line.strip('\n').split('@') label = line[0] sent = line[1].split(' ') return sent, labeldef word_to_id(word, word2id): """ 单词--&gt;ID :param word: 单词 :param word2id: word2id @type: dict :return: """ return word2id[word] if word in word2id else word2id['unk']def bulid_vocab(vocab_size, min_freq=3, stop_word_list=None, is_debug=False): """ 建立词典 :param vocab_size: 词典大小 :param min_freq: 最小词频限制 :param stop_list: 停用词 @type：file_path :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试 :return: word2id """ size = 0 count = Counter() with open(os.path.join(config.ROOT_DIR, config.RAW_DATA), 'r') as fr: logger.info('Building vocab') for line in tqdm(fr, desc='Build vocab'): words, label = sent_label_split(line) count.update(words) size += 1 if is_debug: limit_train_size = 10000 if size &gt; limit_train_size: break if stop_word_list: stop_list = &#123;&#125; with open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), 'r') as fr: for i, line in enumerate(fr): word = line.strip('\n') if stop_list.get(word) is None: stop_list[word] = i count = &#123;k: v for k, v in count.items() if k not in stop_list&#125; count = sorted(count.items(), key=operator.itemgetter(1)) # 词典 vocab = [w[0] for w in count if w[1] &gt;= min_freq] if vocab_size &lt; len(vocab): vocab = vocab[:vocab_size] vocab = config.flag_words + vocab logger.info('vocab_size is %d'%len(vocab)) # 词典到编号的映射 word2id = &#123;k: v for k, v in zip(vocab, range(0, len(vocab)))&#125; assert word2id['&lt;pad&gt;'] == 0, "ValueError: '&lt;pad&gt;' id is not 0" print(word2id) with open(config.WORD2ID_FILE, 'wb') as fw: pickle.dump(word2id, fw) return word2iddef train_val_split(X, y, valid_size=0.3, random_state=2018, shuffle=True): """ 训练集验证集分割 :param X: sentences :param y: labels :param random_state: 随机种子 """ logger.info('train val split') data = [(data_x, data_y) for data_x, data_y in zip(X, y)] N = len(data) test_size = int(N * valid_size) if shuffle: random.seed(random_state) random.shuffle(data) valid = data[:test_size] train = data[test_size:] return train, validdef text2id(word2id, maxlen=None, valid_size=0.3, random_state=2018, shuffle=True, is_debug=False): """ 训练集文本转ID :param valid_size: 验证集大小 """ print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE)) if len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; 0: logger.info('Text to id file existed') return logger.info('Text to id') sentences, labels, lengths = [], [], [] size = 0 with open(os.path.join(config.ROOT_DIR, config.RAW_DATA), 'r') as fr: for line in tqdm(fr, desc='text_to_id'): words, label = sent_label_split(line) sent = [word_to_id(word=word, word2id=word2id) for word in words] if maxlen: sent = sent[:maxlen] length = len(sent) sentences.append(sent) labels.append(label) lengths.append(length) size += 1 if is_debug: limit_train_size = 10000 if size &gt; limit_train_size: break train, valid = train_val_split(sentences, labels, valid_size=valid_size, random_state=random_state, shuffle=shuffle) del sentences, labels, lengths with open(config.TRAIN_FILE, 'w') as fw: for sent, label in train: sent = [str(s) for s in sent] line = "\t".join[str(label), " ".join(sent)] fw.write(line + '\n') logger.info('Writing train to file done') with open(config.VALID_FILE, 'w') as fw: for sent, label in train: sent = [str(s) for s in sent] line = "\t".join[str(label), " ".join(sent)] fw.write(line + '\n') logger.info('Writing valid to file done')# 功能整合，提供给外部调用的函数接口def data_helper(vocab_size, min_freq=3, stop_list=None, valid_size=0.3, random_state=2018, shuffle=True, is_debug=False): # 判断文件是否已存在 if len(glob(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE))) &gt; 0: logger.info('Word to id file existed') with open(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE), 'rb') as fr: word2id = pickle.load(fr) else: word2id = bulid_vocab(vocab_size=vocab_size, min_freq=min_freq, stop_word_list=stop_list, is_debug=is_debug) text2id(word2id, valid_size=valid_size, random_state=random_state, shuffle=shuffle, is_debug=is_debug) config.py 12345678# ---------PATH------------ROOT_DIR = '/home/daizelin/pytorch/'RAW_DATA = 'data/data_for_test.csv'TRAIN_FILE = 'outputs/intermediate/train.tsv'VALID_FILE = 'outputs/intermediate/valid.tsv'LOG_PATH = 'outputs/logs'is_debug = Falseflag_words = ['&lt;pad&gt;', '&lt;unk&gt;'] Logginger.py 12345678910111213141516171819202122232425262728293031323334353637383940414243import loggingfrom logging import Loggerfrom logging.handlers import TimedRotatingFileHandler'''使用方式from you_logging_filename.py import init_loggerlogger = init_logger("dataset",logging_path='')def you_function(): logger.info() logger.error()''''''日志模块1. 同时将日志打印到屏幕跟文件中2. 默认值保留近7天日志文件'''def init_logger(logger_name, logging_path): if logger_name not in Logger.manager.loggerDict: logger = logging.getLogger(logger_name) logger.setLevel(logging.DEBUG) handler = TimedRotatingFileHandler(filename=logging_path+"/all.log",when='D',backupCount = 7) datefmt = '%Y-%m-%d %H:%M:%S' format_str = '[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s %(message)s' formatter = logging.Formatter(format_str,datefmt) handler.setFormatter(formatter) handler.setLevel(logging.INFO) logger.addHandler(handler) console= logging.StreamHandler() console.setLevel(logging.INFO) console.setFormatter(formatter) logger.addHandler(console) handler = TimedRotatingFileHandler(filename=logging_path+"/error.log",when='D',backupCount=7) datefmt = '%Y-%m-%d %H:%M:%S' format_str = '[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s %(message)s' formatter = logging.Formatter(format_str,datefmt) handler.setFormatter(formatter) handler.setLevel(logging.ERROR) logger.addHandler(handler) logger = logging.getLogger(logger_name) return logger]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>文本预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[周期性学习率(Cyclical Learning Rate)技术]]></title>
    <url>%2F2018%2F10%2F28%2F%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[本文介绍神经网络训练中的周期性学习率技术。 Introduction学习率(learning_rate, LR)是神经网络训练过程中最重要的超参数之一，它对于快速、高效地训练神经网络至关重要。简单来说，LR决定了我们当前的权重参数朝着降低损失的方向上改变多少。 1new_weight = exsiting_weight - learning_rate * gradient Fig.: A simple neural network where the w’s and b’s are to be learnt (Img Credit: Matt Mazur) 这看上去很简单。但是正如许多研究显示的那样，单单通过提升这一步就会对我们的训练产生深远的影响，并且尚有很大的优化空间。 本文介绍了一种叫做周期性学习率（CLR）的技术，它是一种非常新的、简单的想法，用来设置和控制训练过程中LR的大小。该技术在jeremyphoward今年的fast.ai course课程中提及过。 Motivation神经网络用来完成某项任务需要对大量参数进行训练。参数训练意味着寻找合适的一些参数，使得在每个batch训练完成后损失（loss）达到最小，而参数更新的方式则与LR密切相关。 通常来说，有两种广泛使用的方法用来设置训练过程中的LR。 One LR for all parameters一个典型的例子是SGD， 在训练开始时设置一个LR常量，并且设定一个LR衰减策略（如step，exponential等）。这个单一的LR用来更新所有的参数。在每个epochs中，LR按预先设定随时间逐渐衰减，当我们临近最小损失时， 通过衰减可以减缓更新，以防止我们越过最小值。 Fig. Effect of various learning rates on convergence (Img Credit: cs231n) 该方法存在如下挑战(refer)： 难以选择初始的LR达到想要的效果（如上图所示）； LR衰减策略同样难以设定，他们很难自适应动态变化的数据； 所有的参数使用相同的LR进行更新，而这些参数可能学习速率不完全相同； 很容易陷入马鞍点不能自拔 Adaptive LR for each parameter一些改进的优化器如AdaGrad, AdaDelta, RMSprop and Adam 很大程度上缓解了上述困难，方法是对每个参数采用不同的自适应学习率。比如AdaDelta，它的更新机制甚至不需要我们主动设置默认的学习率。 Fig: Animation comparing optimization algorithms (Img Credit: Alec Radford) Cycling Learning RateCLR是Leslie Smith于2015年提出的。这是一种调节LR的方法，在该方法中，设定一个LR上限和下限，LR的值在上限和下限的区间里周期性地变化。看上去，LCR似乎是自适应LR技术和SGD的竞争者，事实上，CLR技术是可以和上述提到的改进的优化器一起使用来进行参数更新的。 而在计算上，CLR比上述提到的改进的优化器更容易实现，正如文献[1]所述： Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. In addition, CLR policies are computationally simpler than adaptive learning rates. CLR is likely most similar to the SGDR method that appeared recently. Why it works直觉上看，随着训练进度的增加我们应该保持学习率一直减小以便于在某一时刻达到收敛。 然而，事实恰与直觉相反，使用一个在给定区间里周期性变化的LR可能更有用处。原因是周期性高的学习率能够使模型跳出在训练过程中遇到的局部最低点和马鞍点。事实上，Dauphin等[3]指出相比于局部最低点，马鞍点更加阻碍收敛。如果马鞍点正好发生在一个巧妙的平衡点，小的学习率通常不能产生足够大的梯度改变使其跳过该点（即使跳过，也需要花费很长时间）。这正是周期性高学习率的作用所在，它能够更快地跳过马鞍点。 Fig.: A saddle point in the error surface (Img Credit: safaribooksonline) 另外一个好处是，最优的LR肯定落在最小值和最大值之间。换言之，我们确实在迭代过程中使用了最好的LR。 Epoch，iterations, cycles and stepsize首先介绍几个术语，理解这些术语可以更好地理解下面描述的算法和公式。 我们现在考虑一个包含50000个样本的训练集。 一个epoch是至将整个训练集训练一轮。如果我们将batch_size, 我们在一个epoch里会得到500个batch或者叫iteration。iteration的数目随着epoch的增加不断积累，在第二个epoch，对应着501到1000次iteration，后面的以此类推。 一个cycle定义为学习率从低到高，然后从高到低走一轮所用的iteration数。而stepsize指的是cycle迭代步数的一半。注意，cycle不一定必须和epoch相同，但实践上通常将cycle和epoch对应相同的iteration。 Fig: Triangular LR policy. (Img Credit: https://arxiv.org/pdf/1506.01186.pdf) 在上图中，两条红线分别表示学习率最小值（base lr）和学习率最大值（max lr）。蓝色的线是学习率随着iteration改变的方式。蓝线上下一次表示一个cycle，stepsize则是其一半。 Calculating the LR综上所述，接下来我们需要参数作为该算法的输入： stepsize base_lr max_lr 下面是LR更新的一段代码。 123456789101112131415161718192021222324252627282930import numpy as npimport matplotlib.pyplot as pltdef get_triangular_lr(iteration, stepsize, base_lr, max_lr): """ Given the inputs, calculates the lr that should be applicable for this iteration """ cycle = np.floor(1 + iteration/(2 * stepsize)) x = np.abs(iteration/stepsize - 2 * cycle + 1) lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1-x)) return lrif __name__ == '__main__': # Demo of how the LR varies with iterations num_iterations = 10000 stepsize = 1000 base_lr = 0.0001 max_lr = 0.001 lr_trend = list() for iteration in range(num_iterations): lr = get_triangular_lr(iteration, stepsize, base_lr, max_lr) # Update your optimizer to use this learning rate in this iteration lr_trend.append(lr) plt.plot(lr_trend) plt.show() 结果如下图所示。 Fig: Graph showing the variation of lr with iteration. We are using the triangular profile. Deriving the optimal base lr and max lr对于给定的数据集，怎么确定合理的base lr 和max lr呢？ 答案是先跑几个epoch，并且让学习率线性增加，观察准确率的变化，从中选出合适的base 和max lr。 我们让学习率按照上面的斜率进行增长，跑了几轮，结果如下图所示。 Fig: Plot of accuracy vs learning rate (Img Credit: https://arxiv.org/pdf/1506.01186.pdf) 可以看出，开始的时候，准确率随着学习率的增加而增加，然后进入平缓起期，然后又开始减小，出现震荡。注意图中准确率开始增长的那一点和达到平衡的那一点（图中红色箭头所示）。这两个点可以作为比较好的base lr 和 max lr。当然，你也可以选择平衡点旁边的准确率峰值点作为max lr， 把base lr 设为其1/3 或者1/4。 好了，三个参数中已经有两个确定了，那么怎么确定stepsize呢？ 已经有论文做过实验，他们将stepsize设成一个epoch包含的iteration数量的2-10倍。拿我们之前举的例子来说，我们一个epoch包含500个iteration，那么stepsize就设成1000-5000。该论文实验表明，stepsize设成2倍或者10倍，两者结果并没有太大的不同。 Variants上面我们实现的算法中，学习率是按照三角的规律周期性变化。除了这种以外，还有其他几种不同的函数形式。 traiangular2：这里max lr 按cycle进行对半衰减。 Fig: Graph showing the variation of lr with iteration for the triangular2 approach (Img Credit: Brad Kenstler) exp_range：这里max lr按iteration进行指数衰减。 Fig: Graph showing the variation of lr with iteration for the exp-range approach (Img Credit: Brad Kenstler) 这些与固定学习率的指数衰减（exponential decay）相比，有论文表明效果都得到了明显的提升。 Results如下图所示，在某神经网络上，CLR提供了一个快速的收敛，因此它的确值得一试。 Fig. CLR tested on CIFAR 10 (Img Credit: https://arxiv.org/pdf/1506.01186.pdf) 在上图的试验中，CLR花了25K次迭代达到了81%的准确率，传统的LR更新方法大约需要70K才能达到同样的水平。 Fig. CLR used with Nesterov and Adam. Much faster convergence with Nesterov (Nesterov is an improvement over SGD) (Img Credit: https://arxiv.org/pdf/1506.01186.pdf) 在另一项试验中，如上图所示，CLR + Nesterov优化器比著名的Adam收敛的还要快。 ConclusionCLR带来了一种新的方案来控制学习率的更新，它可以与SGD以及一些更加高级的优化器上一起使用。CLR应该成为每一个深度学习实践者工具箱里的一项技术。 References Cyclical Learning Rates for Training Neural Networks, Smith An overview of gradient descent optimization algorithms, Rudder Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization. SGDR: Stochastic Gradient Descent with Warm Restarts, Loshchilov, Hutter https://github.com/bckenstler/CLR]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>超参数</tag>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像数据-TFrecord在动态图中的使用]]></title>
    <url>%2F2018%2F10%2F27%2FTFrecord%E5%9C%A8%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文介绍图片数据使用TFrecord和tf.data.dataset进行存储和读取。 Tensorflow 提供了四种数据读取方式： Preloaded data: 用一个tf.constant常量将数据集加载进来，主要用于很小的数据集； Feeding: 使用python代码供给数据，将所有数据加载进内存，然后一个batch一个batch地输入到计算图中， 适用于小数据集； QueueRunner: 基于队列的输入通道，读取TFrecord静态图使用； tf.data API: 能够从不同的输入或文件格式中读取、预处理数据，并且对数据应用一些变换（例如，batching、shuffling、mapping function over the dataset），tf.data API 是旧的 feeding、QueueRunner的升级。值得注意的是， Eager模式必须使用该API来构建输入通道， 一般结合TFrecord使用。该API相比于Queue更容易使用。 What‘s TFrecordTFrecord是Tensorflow提供的一种二进制存储格式，可将数据和标签统一存储。从上述读取方式中可以看出，TFrecord在QueueRunner和tf.data API读取中均扮演了重要的角色。 Why TFrecord与其他方案相比， 使用TFrecord读取的优点在于： 可处理大规模数据量，而不会造成其他方案所带来的内存不够用的问题； 在Feeding方案中，batch读取的IO操作势必会阻塞训练，前一个batch加载完成后，神经网络必须等待下一个batch加载完成后才能继续训练，效率较低。 How To UseTFrecord的使用主要有两块：一是图片数据转TFrecord格式存储，二是解析存储好的TFrecord文件。下面逐一介绍。 图片转TFrecord本文使用的数据集是Kaggle猫狗数据集。 该数据集包含train和test两个文件夹， 分别为训练集和测试集，下面以train集为例操作。 123ls |wc -w25000 训练集包含25000张图片，猫狗各一半。 1234$ ls cat.124.jpg cat.3750.jpg cat.6250.jpg cat.8751.jpg dog.11250.jpg dog.2500.jpg dog.5000.jpg dog.7501.jpg... 图片文件以jpg格式存储，以cat， dog作为文件名开头。 1234567891011121314151617181920212223242526272829303132333435363738394041import osfrom tqdm import tqdmimport tensorflow as tfdef img_tfrecord_encode(classes, tfrecord_filename, data_path, is_training=True): """ 功能：读取图片转换成tfrecord格式的文件 @params: classes: 标签类别 @type：classes: dict @params: tfrecord_filename: tfrecord文件保存文件 @type：tfrecord_filename: str @params: data_path: 原始训练集存储路径 @is_training: 是否为训练集，用来区分训练集和测试集 """ # 初始化一个writer writer = tf.python_io.TFRecordWriter(tfrecord_filename) for img_name in tqdm(os.listdir(path)): name = img_name.split('.')[0] # 使用tf.gfile.FastFile读取图片要比PIL.Image读取处理得到的 # 最终TFrecod文件小得多，在本案例中，IMAGE方式读取得到的TFrecord大小约为3.7G # 而tf.gfile.FastFile得到的约为548M with tf.gfile.FastGFile(os.path.join(path, img_name), 'rb') as gf: img = gf.read() if is_training: # 构造特征 feature = &#123; 'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[classes[name]])), 'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])), 'file_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()])) &#125; else: feature = &#123; 'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[-1])), 'img_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])), 'file_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()])) &#125; # example 对象将label和image特征进行封装 example = tf.train.Example(features=tf.train.Features(feature=feature)) writer.write(example.SerializeToString()) # 序列化为字符串 writer.close() print('tfrecord writen done!') 调用上述函数，可得到猫狗训练集的TFrecord格式文件 12345if __name__ == '__main__': classes = &#123;'cat': 0, 'dog': 1&#125; tfrecord_filename = 'cat_and_dog.tfrecord' data_path = 'train/' img_tfrecord_encode(classes, tfrecord_filename, data_path, is_training=True) 上述程序运行大约需要2min。 使用tf.data读取TFrecord在动态图（eager）模式下，QueueRunner不可用，必须使用tf.data进行TFrecord的读取。 12345678910111213141516171819202122232425262728293031323334353637383940def img_tfrecord_parse(tfrecord_filename, epochs, batch_size, shape, padded_shapes=None, shuffle=True, buffer_size=1000): """ @param: tfrecord_filename:tfrecord文件列表 @type:list @param: epoch:训练轮数（repeating次数） @type:int @param：batch_size:批数据大小 @type:int @param: shape:图片维度 @type:tuple @param: padded_shapes:不定长padding @type:tuple @param: shuffle:是否打乱 @type:boolean """ # 解析单个example，特征与encode一一对应。 def parse_example(serialized_example): features = tf.parse_single_example(serialized_example, features=&#123; 'label': tf.FixedLenFeature([], tf.int64), 'img_raw': tf.FixedLenFeature([], tf.string), 'file_name': tf.FixedLenFeature([], tf.string) &#125;) # 解码 image = tf.image.decode_jpeg(features['img_raw']) # 设置shape image = tf.image.resize_images(image, shape, method=1) label = tf.cast(features['label'], tf.int64) file_name = tf.cast(features['file_name'], tf.string) return image, label, file_name # 解析TFrecord dataset = tf.data.TFRecordDataset(tfrecord_filename).map(parse_example) if shuffle: if padded_shapes: dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).padded_batch(batch_size, padded_shapes) else: dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).batch(batch_size) else: if padded_shapes: dataset = dataset.repeat(epochs).padded_batch(batch_size, padded_shapes) else: dataset = dataset.repeat(epochs).batch(batch_size) return dataset 调用上述函数，解析TFrecord得到dataset。 123456789101112131415if __name__ == '__main__'(): tfrecord_filename = 'cat_and_dog.tfrecord' epochs = 100 batch_size = 64 shape = (227, 227) dataset = img_tfrecord_parse(tfrecord_filename=tfrecord_filename, epochs=epochs, batch_size=batch_size, shape=shape) # 查看dataset iterator = dataset.make_one_hot_iterator() image, label, file_name = iterator.get_next() print(image[0]) print(label[0]) print(file_name[0])]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
</search>
