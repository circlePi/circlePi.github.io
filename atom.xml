<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>circlepi&#39;s blog</title>
  
  <subtitle>Even with an intractable probelm, one can find a way to do the right thing.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://state-of-art.top/"/>
  <updated>2018-11-30T13:59:48.467Z</updated>
  <id>http://state-of-art.top/</id>
  
  <author>
    <name>circlepi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>浅析pytorch中的LSTM(GRU)</title>
    <link href="http://state-of-art.top/2018/11/30/%E6%B5%85%E6%9E%90pytorch%E4%B8%AD%E7%9A%84LSTM(GRU)%20/"/>
    <id>http://state-of-art.top/2018/11/30/浅析pytorch中的LSTM(GRU) /</id>
    <published>2018-11-30T15:21:08.000Z</published>
    <updated>2018-11-30T13:59:48.467Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文介绍pytorch RNN网络搭建，主要包括LSTM和GRU的使用。</p><p>最近从tensorflow入坑pytorch，发现两者的RNN模块前向传播有些不同。特别是RNN中的pack_padded_sequence和pad_packed_sequence。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="tensorflow的BILSTM"><a href="#tensorflow的BILSTM" class="headerlink" title="tensorflow的BILSTM"></a>tensorflow的BILSTM</h3><p>以BILSTM为例，tensorflow中我们一般这样操作：</p><ul><li><p>先定义个前向层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size)</span><br></pre></td></tr></table></figure></li><li><p>在定义一个后向层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size)</span><br></pre></td></tr></table></figure></li></ul><p>然后前向传播是这个样子的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, seq_length, training)</span>:</span></span><br><span class="line">       <span class="comment"># 嵌入层</span></span><br><span class="line">       embedded_words = self.embeddings(inputs)</span><br><span class="line">       <span class="comment"># RNN层</span></span><br><span class="line">       outputs, final_state = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">           self.fw_cell,</span><br><span class="line">           self.bw_cell,</span><br><span class="line">           inputs=embedded_words, </span><br><span class="line">           <span class="comment"># 句子原始长度（batch_size，1）</span></span><br><span class="line">           sequence_length=seq_length, </span><br><span class="line">           dtype=tf.float32,</span><br><span class="line">           time_major=<span class="keyword">False</span>)</span><br><span class="line">       <span class="comment"># 合并前后向的结果</span></span><br><span class="line">       <span class="comment"># outputs是一个列表:[（batch_size, cell_size）,(...)]</span></span><br><span class="line">       <span class="comment"># len(output) = time_steps</span></span><br><span class="line">       outputs = tf.concat(outputs, axis=<span class="number">2</span>)</span><br><span class="line">       <span class="comment"># 由于采用的是dynamic_rnn，padding部分自动被截断了</span></span><br><span class="line">       <span class="comment"># 这里取最后一维就好了</span></span><br><span class="line">       final_output = outputs[<span class="number">-1</span>]</span><br><span class="line">       logits = self.Dense(final_output)</span><br><span class="line">       <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>可以看到，tensorflow的<em>dynamic</em> LSTM有两个特点：</p><ul><li>前向传播时，不看词向量维度的话，输入是个二维（batch_size, time_steps)</li><li>padding部分不参与计算，会被自动截断</li></ul><p><em>with this in our mind</em>, <em>let‘s see what’s the bilstm in pytorch。</em></p><h2 id="Pytorch-BILSTM"><a href="#Pytorch-BILSTM" class="headerlink" title="Pytorch BILSTM"></a>Pytorch BILSTM</h2><h3 id="LSTM定义"><a href="#LSTM定义" class="headerlink" title="LSTM定义"></a>LSTM定义</h3><p>一般这样操作：</p><ul><li><p>先定义个LSTM</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension,</span><br><span class="line">                                    hidden_size=hidden_size,</span><br><span class="line">                                    num_layers=num_layer,</span><br><span class="line">                                    batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                    bidirectional=bi_flag)</span><br></pre></td></tr></table></figure><p>双向的怎么办呢, 改个参数就行了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bidirectional=<span class="keyword">True</span></span><br></pre></td></tr></table></figure></li><li><p>多层呢</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_layers=<span class="keyword">True</span></span><br></pre></td></tr></table></figure></li></ul><p>可以看到，pytorch的LSTM把所有的功能整合到一起了，不需要定义前向后向（当然我们也可以搞两个单向的）。此外，我们除了要传递隐藏层的个数cell_size, 还需要将词向量的维度传递进来，这是为什么呢？</p><p>这还不是关键。关键在于pytorch的动态padding机制。因为我们一般是将输入作为batch传进来的，对于变长的文本来说，padding是不可避免的。但是在我们使用LSTM进行计算时，是不希望padding部分参与计算的（他们不是真实的文本，假如纳入计算，会引入不必要的噪声和不必要的计算量）。tensorflow采用dynamic LSTM很好地解决了这一问题。pytorch当然也有他自己的一套，下面来看看。</p><h3 id="pack-padded-sequence"><a href="#pack-padded-sequence" class="headerlink" title="pack_padded_sequence"></a>pack_padded_sequence</h3><p>我们从embedding层拿到的输入维度为（batch_size, time_steps, word_embedding_dimension）, 并不将他直接喂给LSTM，而是要预加工一下，这时候第一个重要的函数pack_padded_sequence就登场了，可以将其看成一个截断函数，作用就是将padding部分截断。不仅如此，阶段后会将输入拉平，变成（batch_size*time_steps, word_embedding_dimension）, 仔细看一下，降了一个维度。来看实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence </span><br><span class="line"><span class="comment"># 手动造个张量, 包含3个序列，长度分别为10,5,3，使用0进行padding</span></span><br><span class="line">x = torch.FloatTensor([[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>],[<span class="number">7</span>],[<span class="number">8</span>],[<span class="number">8</span>],[<span class="number">9</span>]],[[<span class="number">1</span>], [<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]],[[<span class="number">5</span>],[<span class="number">4</span>],[<span class="number">6</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]]])</span><br><span class="line">print(<span class="string">"x shape is："</span>, x.shape)</span><br><span class="line"><span class="comment"># x 的真实长度</span></span><br><span class="line">length = torch.LongTensor([<span class="number">10</span>, <span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">x_packed = pack_padded_sequence(x, length,batch_first=<span class="keyword">True</span>)</span><br><span class="line">print(x_packed)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x shape <span class="keyword">is</span>：（<span class="number">3</span>，<span class="number">10</span>，<span class="number">1</span>）</span><br><span class="line">PackedSequence(data=tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>],</span><br><span class="line">        [<span class="number">8.</span>],</span><br><span class="line">        [<span class="number">8.</span>],</span><br><span class="line">        [<span class="number">9.</span>]]), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这里有几点要说一下：</p><ul><li>输入的张量是已经按照长度排序的！这是必须的，否则pack_padded_sequence函数会报错！一般而言排序有两种做法，一是使用torch.sort函数，第二是在构造batch时使用torchtext进行batch内排序，见我上篇博文</li><li>经过pack，输入发生了三个变化。一是由三维变成了2维(18,1)，二是padding部分的0没有了，三是对序列进行了拼接，这其实是维度降低的结果</li><li>pack完之后的结果是个tuple，tuple[0]是数据，是按列进行拼接的，tuple[1]是batch_size, 跟我们之前的batch_size是不一样的，这是为了我们后面可以还原</li></ul><p>好了，现在pddding问题解决了，我们将它输入进LSTM，前向传播是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">    <span class="string">"""前向传播"""</span></span><br><span class="line">    embeddings = self.embedding(inputs, length)  <span class="comment"># (batch_size, time_steps, embedding_dim)</span></span><br><span class="line">    <span class="comment"># 去除padding元素</span></span><br><span class="line">    <span class="comment"># embeddings_packed: (batch_size*time_steps, embedding_dim)</span></span><br><span class="line">    embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">    output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0))</span><br><span class="line">    <span class="comment"># padded_output: (batch_size, time_steps, hidden_size * bi_num)</span></span><br><span class="line">    <span class="comment"># h_n|c_n: (num_layer*bi_num, batch_size, hidden_size)</span></span><br><span class="line">    padded_output, length = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># 取最后一个有效输出作为最终输出（0为无效输出）</span></span><br><span class="line">    last_output = padded_output[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>这里做下参数说明：</p><ul><li>batch_first    类型：bool， True，输入的维度为（batch_size，time_steps, word_embedding_dimension）；False，输入维度为（time_steps, batch_size, word_embedding_dimension）</li><li>h_0: 初始化隐藏态</li><li>c_0: 初始化细胞态</li><li>output：输出，为一个tuple，后面会用例子说明</li><li>h_n: 最后个隐藏态</li><li>c_n: 最后个细胞态</li></ul><p>还是以上面我们造的那个张量为例，看下输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">birnn = nn.LSTM(input_size=<span class="number">1</span>, hidden_size=<span class="number">8</span>, bidirectional=<span class="keyword">True</span>)</span><br><span class="line">output, (h_n, c_n) = birnn(x_packed)</span><br><span class="line">print(<span class="string">'output:\n'</span>, output)</span><br><span class="line">print(<span class="string">'h_n shape:\n'</span>,h_n.shape)</span><br><span class="line">print(<span class="string">'c_n shape:\n'</span>,c_n.shape)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line"> PackedSequence(data=tensor([[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,  <span class="number">0.0291</span>,</span><br><span class="line">          <span class="number">0.0619</span>,  <span class="number">0.0213</span>,  <span class="number">0.1517</span>,  <span class="number">0.0241</span>,  <span class="number">0.2986</span>, <span class="number">-0.2594</span>, <span class="number">-0.1432</span>, <span class="number">-0.1742</span>],</span><br><span class="line">        [<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,  <span class="number">0.0291</span>,</span><br><span class="line">          <span class="number">0.0486</span>,  <span class="number">0.0195</span>,  <span class="number">0.1454</span>,  <span class="number">0.0138</span>,  <span class="number">0.2596</span>, <span class="number">-0.2467</span>, <span class="number">-0.1491</span>, <span class="number">-0.1596</span>],</span><br><span class="line">        [<span class="number">-0.1019</span>,  <span class="number">0.1606</span>, <span class="number">-0.0482</span>, <span class="number">-0.0519</span>, <span class="number">-0.1932</span>,  <span class="number">0.2632</span>, <span class="number">-0.0423</span>,  <span class="number">0.0774</span>,</span><br><span class="line">          <span class="number">0.0434</span>,  <span class="number">0.0142</span>,  <span class="number">0.0915</span>, <span class="number">-0.0455</span>,  <span class="number">0.2970</span>, <span class="number">-0.6665</span>, <span class="number">-0.0578</span>, <span class="number">-0.0521</span>],</span><br><span class="line">        [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,  <span class="number">0.0918</span>,</span><br><span class="line">          <span class="number">0.0730</span>,  <span class="number">0.0247</span>,  <span class="number">0.1526</span>,  <span class="number">0.0160</span>,  <span class="number">0.3559</span>, <span class="number">-0.4301</span>, <span class="number">-0.1375</span>, <span class="number">-0.1429</span>],</span><br><span class="line">        [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,  <span class="number">0.0918</span>,</span><br><span class="line">          <span class="number">0.0517</span>,  <span class="number">0.0197</span>,  <span class="number">0.1431</span>, <span class="number">-0.0005</span>,  <span class="number">0.2929</span>, <span class="number">-0.4016</span>, <span class="number">-0.1401</span>, <span class="number">-0.1218</span>],</span><br><span class="line">        [<span class="number">-0.1855</span>,  <span class="number">0.2300</span>, <span class="number">-0.0625</span>, <span class="number">-0.0508</span>, <span class="number">-0.2488</span>,  <span class="number">0.2741</span>, <span class="number">-0.0638</span>,  <span class="number">0.1441</span>,</span><br><span class="line">          <span class="number">0.0239</span>,  <span class="number">0.0137</span>,  <span class="number">0.0786</span>, <span class="number">-0.0524</span>,  <span class="number">0.2311</span>, <span class="number">-0.5685</span>, <span class="number">-0.0673</span>, <span class="number">-0.0485</span>],</span><br><span class="line">        [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,  <span class="number">0.1487</span>,</span><br><span class="line">          <span class="number">0.0789</span>,  <span class="number">0.0259</span>,  <span class="number">0.1354</span>,  <span class="number">0.0050</span>,  <span class="number">0.3851</span>, <span class="number">-0.5676</span>, <span class="number">-0.1110</span>, <span class="number">-0.1117</span>],</span><br><span class="line">        [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,  <span class="number">0.1487</span>,</span><br><span class="line">          <span class="number">0.0439</span>,  <span class="number">0.0172</span>,  <span class="number">0.1198</span>, <span class="number">-0.0221</span>,  <span class="number">0.2856</span>, <span class="number">-0.5105</span>, <span class="number">-0.1089</span>, <span class="number">-0.0852</span>],</span><br><span class="line">        [<span class="number">-0.1346</span>,  <span class="number">0.3093</span>, <span class="number">-0.0775</span>, <span class="number">-0.0678</span>, <span class="number">-0.2616</span>,  <span class="number">0.2946</span>, <span class="number">-0.1167</span>,  <span class="number">0.1699</span>,</span><br><span class="line">         <span class="number">-0.0280</span>,  <span class="number">0.0067</span>,  <span class="number">0.0333</span>, <span class="number">-0.0924</span>,  <span class="number">0.1308</span>, <span class="number">-0.5332</span>, <span class="number">-0.0237</span>, <span class="number">-0.0201</span>],</span><br><span class="line">        [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,  <span class="number">0.1887</span>,</span><br><span class="line">          <span class="number">0.0803</span>,  <span class="number">0.0254</span>,  <span class="number">0.1112</span>, <span class="number">-0.0077</span>,  <span class="number">0.3929</span>, <span class="number">-0.6681</span>, <span class="number">-0.0804</span>, <span class="number">-0.0828</span>],</span><br><span class="line">        [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,  <span class="number">0.1887</span>,</span><br><span class="line">          <span class="number">0.0194</span>,  <span class="number">0.0129</span>,  <span class="number">0.0852</span>, <span class="number">-0.0521</span>,  <span class="number">0.2364</span>, <span class="number">-0.5524</span>, <span class="number">-0.0720</span>, <span class="number">-0.0538</span>],</span><br><span class="line">        [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,  <span class="number">0.2141</span>,</span><br><span class="line">          <span class="number">0.0779</span>,  <span class="number">0.0234</span>,  <span class="number">0.0866</span>, <span class="number">-0.0215</span>,  <span class="number">0.3831</span>, <span class="number">-0.7393</span>, <span class="number">-0.0539</span>, <span class="number">-0.0580</span>],</span><br><span class="line">        [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,  <span class="number">0.2141</span>,</span><br><span class="line">         <span class="number">-0.0274</span>,  <span class="number">0.0076</span>,  <span class="number">0.0448</span>, <span class="number">-0.0817</span>,  <span class="number">0.1408</span>, <span class="number">-0.4683</span>, <span class="number">-0.0382</span>, <span class="number">-0.0285</span>],</span><br><span class="line">        [<span class="number">-0.1470</span>,  <span class="number">0.3710</span>, <span class="number">-0.0771</span>, <span class="number">-0.0605</span>, <span class="number">-0.2693</span>,  <span class="number">0.2962</span>, <span class="number">-0.0684</span>,  <span class="number">0.2291</span>,</span><br><span class="line">          <span class="number">0.0724</span>,  <span class="number">0.0204</span>,  <span class="number">0.0651</span>, <span class="number">-0.0356</span>,  <span class="number">0.3568</span>, <span class="number">-0.7885</span>, <span class="number">-0.0341</span>, <span class="number">-0.0385</span>],</span><br><span class="line">        [<span class="number">-0.1142</span>,  <span class="number">0.3956</span>, <span class="number">-0.0744</span>, <span class="number">-0.0727</span>, <span class="number">-0.2434</span>,  <span class="number">0.2935</span>, <span class="number">-0.1226</span>,  <span class="number">0.2366</span>,</span><br><span class="line">          <span class="number">0.0636</span>,  <span class="number">0.0164</span>,  <span class="number">0.0477</span>, <span class="number">-0.0497</span>,  <span class="number">0.3136</span>, <span class="number">-0.8199</span>, <span class="number">-0.0208</span>, <span class="number">-0.0245</span>],</span><br><span class="line">        [<span class="number">-0.0854</span>,  <span class="number">0.4126</span>, <span class="number">-0.0669</span>, <span class="number">-0.0829</span>, <span class="number">-0.2140</span>,  <span class="number">0.2871</span>, <span class="number">-0.1699</span>,  <span class="number">0.2389</span>,</span><br><span class="line">          <span class="number">0.0496</span>,  <span class="number">0.0118</span>,  <span class="number">0.0343</span>, <span class="number">-0.0641</span>,  <span class="number">0.2542</span>, <span class="number">-0.8307</span>, <span class="number">-0.0124</span>, <span class="number">-0.0151</span>],</span><br><span class="line">        [<span class="number">-0.0847</span>,  <span class="number">0.4176</span>, <span class="number">-0.0620</span>, <span class="number">-0.0877</span>, <span class="number">-0.2053</span>,  <span class="number">0.2863</span>, <span class="number">-0.2063</span>,  <span class="number">0.2468</span>,</span><br><span class="line">          <span class="number">0.0281</span>,  <span class="number">0.0090</span>,  <span class="number">0.0254</span>, <span class="number">-0.0784</span>,  <span class="number">0.1827</span>, <span class="number">-0.7871</span>, <span class="number">-0.0100</span>, <span class="number">-0.0116</span>],</span><br><span class="line">        [<span class="number">-0.0620</span>,  <span class="number">0.4284</span>, <span class="number">-0.0550</span>, <span class="number">-0.0931</span>, <span class="number">-0.1827</span>,  <span class="number">0.2785</span>, <span class="number">-0.2409</span>,  <span class="number">0.2438</span>,</span><br><span class="line">         <span class="number">-0.0278</span>,  <span class="number">0.0044</span>,  <span class="number">0.0120</span>, <span class="number">-0.1073</span>,  <span class="number">0.0924</span>, <span class="number">-0.6543</span>, <span class="number">-0.0047</span>, <span class="number">-0.0063</span>]],</span><br><span class="line">       grad_fn=&lt;CatBackward&gt;), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">h_n shape:</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>])</span><br><span class="line">c_n shape:</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><p>output由一个tuple组成，第一个元素就是输出，这里的维度为torch.Size([18, 16])，第二个元素和我们之前pack时的batch_size参数一样。</p><p>h_n和c_n分别为两个三维张量，维度如上，因为是双向，第一维为2.</p><p>到这里是不是就结束了呢，当然不是，pack完了之后经过LSTM输出了结果，看上去很费劲，因为序列连一起了，batch都没了。所以我们还原，这时候就用到pack_padded_sequence的好基友pad_packed_sequence了。</p><h3 id="pad-packed-sequence"><a href="#pad-packed-sequence" class="headerlink" title="pad_packed_sequence"></a>pad_packed_sequence</h3><p>pad_packed_sequence可以看成是解压缩操作。从上面可以看到，h_n, c_n已经是正常维度的张量了，没有pack，当然也用不着pad。我们只需对output做pad。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">padded_output, length = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'padded_output\n'</span>, padded_output)</span><br><span class="line">print(<span class="string">'padded_output shape\n'</span>, padded_output.shape)</span><br><span class="line">print(<span class="string">'length\n'</span>,length)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">padded_output</span><br><span class="line"> tensor([[[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,</span><br><span class="line">           <span class="number">0.0291</span>,  <span class="number">0.0619</span>,  <span class="number">0.0213</span>,  <span class="number">0.1517</span>,  <span class="number">0.0241</span>,  <span class="number">0.2986</span>, <span class="number">-0.2594</span>,</span><br><span class="line">          <span class="number">-0.1432</span>, <span class="number">-0.1742</span>],</span><br><span class="line">         [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,</span><br><span class="line">           <span class="number">0.0918</span>,  <span class="number">0.0730</span>,  <span class="number">0.0247</span>,  <span class="number">0.1526</span>,  <span class="number">0.0160</span>,  <span class="number">0.3559</span>, <span class="number">-0.4301</span>,</span><br><span class="line">          <span class="number">-0.1375</span>, <span class="number">-0.1429</span>],</span><br><span class="line">         [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,</span><br><span class="line">           <span class="number">0.1487</span>,  <span class="number">0.0789</span>,  <span class="number">0.0259</span>,  <span class="number">0.1354</span>,  <span class="number">0.0050</span>,  <span class="number">0.3851</span>, <span class="number">-0.5676</span>,</span><br><span class="line">          <span class="number">-0.1110</span>, <span class="number">-0.1117</span>],</span><br><span class="line">         [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,</span><br><span class="line">           <span class="number">0.1887</span>,  <span class="number">0.0803</span>,  <span class="number">0.0254</span>,  <span class="number">0.1112</span>, <span class="number">-0.0077</span>,  <span class="number">0.3929</span>, <span class="number">-0.6681</span>,</span><br><span class="line">          <span class="number">-0.0804</span>, <span class="number">-0.0828</span>],</span><br><span class="line">         [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,</span><br><span class="line">           <span class="number">0.2141</span>,  <span class="number">0.0779</span>,  <span class="number">0.0234</span>,  <span class="number">0.0866</span>, <span class="number">-0.0215</span>,  <span class="number">0.3831</span>, <span class="number">-0.7393</span>,</span><br><span class="line">          <span class="number">-0.0539</span>, <span class="number">-0.0580</span>],</span><br><span class="line">         [<span class="number">-0.1470</span>,  <span class="number">0.3710</span>, <span class="number">-0.0771</span>, <span class="number">-0.0605</span>, <span class="number">-0.2693</span>,  <span class="number">0.2962</span>, <span class="number">-0.0684</span>,</span><br><span class="line">           <span class="number">0.2291</span>,  <span class="number">0.0724</span>,  <span class="number">0.0204</span>,  <span class="number">0.0651</span>, <span class="number">-0.0356</span>,  <span class="number">0.3568</span>, <span class="number">-0.7885</span>,</span><br><span class="line">          <span class="number">-0.0341</span>, <span class="number">-0.0385</span>],</span><br><span class="line">         [<span class="number">-0.1142</span>,  <span class="number">0.3956</span>, <span class="number">-0.0744</span>, <span class="number">-0.0727</span>, <span class="number">-0.2434</span>,  <span class="number">0.2935</span>, <span class="number">-0.1226</span>,</span><br><span class="line">           <span class="number">0.2366</span>,  <span class="number">0.0636</span>,  <span class="number">0.0164</span>,  <span class="number">0.0477</span>, <span class="number">-0.0497</span>,  <span class="number">0.3136</span>, <span class="number">-0.8199</span>,</span><br><span class="line">          <span class="number">-0.0208</span>, <span class="number">-0.0245</span>],</span><br><span class="line">         [<span class="number">-0.0854</span>,  <span class="number">0.4126</span>, <span class="number">-0.0669</span>, <span class="number">-0.0829</span>, <span class="number">-0.2140</span>,  <span class="number">0.2871</span>, <span class="number">-0.1699</span>,</span><br><span class="line">           <span class="number">0.2389</span>,  <span class="number">0.0496</span>,  <span class="number">0.0118</span>,  <span class="number">0.0343</span>, <span class="number">-0.0641</span>,  <span class="number">0.2542</span>, <span class="number">-0.8307</span>,</span><br><span class="line">          <span class="number">-0.0124</span>, <span class="number">-0.0151</span>],</span><br><span class="line">         [<span class="number">-0.0847</span>,  <span class="number">0.4176</span>, <span class="number">-0.0620</span>, <span class="number">-0.0877</span>, <span class="number">-0.2053</span>,  <span class="number">0.2863</span>, <span class="number">-0.2063</span>,</span><br><span class="line">           <span class="number">0.2468</span>,  <span class="number">0.0281</span>,  <span class="number">0.0090</span>,  <span class="number">0.0254</span>, <span class="number">-0.0784</span>,  <span class="number">0.1827</span>, <span class="number">-0.7871</span>,</span><br><span class="line">          <span class="number">-0.0100</span>, <span class="number">-0.0116</span>],</span><br><span class="line">         [<span class="number">-0.0620</span>,  <span class="number">0.4284</span>, <span class="number">-0.0550</span>, <span class="number">-0.0931</span>, <span class="number">-0.1827</span>,  <span class="number">0.2785</span>, <span class="number">-0.2409</span>,</span><br><span class="line">           <span class="number">0.2438</span>, <span class="number">-0.0278</span>,  <span class="number">0.0044</span>,  <span class="number">0.0120</span>, <span class="number">-0.1073</span>,  <span class="number">0.0924</span>, <span class="number">-0.6543</span>,</span><br><span class="line">          <span class="number">-0.0047</span>, <span class="number">-0.0063</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,</span><br><span class="line">           <span class="number">0.0291</span>,  <span class="number">0.0486</span>,  <span class="number">0.0195</span>,  <span class="number">0.1454</span>,  <span class="number">0.0138</span>,  <span class="number">0.2596</span>, <span class="number">-0.2467</span>,</span><br><span class="line">          <span class="number">-0.1491</span>, <span class="number">-0.1596</span>],</span><br><span class="line">         [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,</span><br><span class="line">           <span class="number">0.0918</span>,  <span class="number">0.0517</span>,  <span class="number">0.0197</span>,  <span class="number">0.1431</span>, <span class="number">-0.0005</span>,  <span class="number">0.2929</span>, <span class="number">-0.4016</span>,</span><br><span class="line">          <span class="number">-0.1401</span>, <span class="number">-0.1218</span>],</span><br><span class="line">         [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,</span><br><span class="line">           <span class="number">0.1487</span>,  <span class="number">0.0439</span>,  <span class="number">0.0172</span>,  <span class="number">0.1198</span>, <span class="number">-0.0221</span>,  <span class="number">0.2856</span>, <span class="number">-0.5105</span>,</span><br><span class="line">          <span class="number">-0.1089</span>, <span class="number">-0.0852</span>],</span><br><span class="line">         [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,</span><br><span class="line">           <span class="number">0.1887</span>,  <span class="number">0.0194</span>,  <span class="number">0.0129</span>,  <span class="number">0.0852</span>, <span class="number">-0.0521</span>,  <span class="number">0.2364</span>, <span class="number">-0.5524</span>,</span><br><span class="line">          <span class="number">-0.0720</span>, <span class="number">-0.0538</span>],</span><br><span class="line">         [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,</span><br><span class="line">           <span class="number">0.2141</span>, <span class="number">-0.0274</span>,  <span class="number">0.0076</span>,  <span class="number">0.0448</span>, <span class="number">-0.0817</span>,  <span class="number">0.1408</span>, <span class="number">-0.4683</span>,</span><br><span class="line">          <span class="number">-0.0382</span>, <span class="number">-0.0285</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.1019</span>,  <span class="number">0.1606</span>, <span class="number">-0.0482</span>, <span class="number">-0.0519</span>, <span class="number">-0.1932</span>,  <span class="number">0.2632</span>, <span class="number">-0.0423</span>,</span><br><span class="line">           <span class="number">0.0774</span>,  <span class="number">0.0434</span>,  <span class="number">0.0142</span>,  <span class="number">0.0915</span>, <span class="number">-0.0455</span>,  <span class="number">0.2970</span>, <span class="number">-0.6665</span>,</span><br><span class="line">          <span class="number">-0.0578</span>, <span class="number">-0.0521</span>],</span><br><span class="line">         [<span class="number">-0.1855</span>,  <span class="number">0.2300</span>, <span class="number">-0.0625</span>, <span class="number">-0.0508</span>, <span class="number">-0.2488</span>,  <span class="number">0.2741</span>, <span class="number">-0.0638</span>,</span><br><span class="line">           <span class="number">0.1441</span>,  <span class="number">0.0239</span>,  <span class="number">0.0137</span>,  <span class="number">0.0786</span>, <span class="number">-0.0524</span>,  <span class="number">0.2311</span>, <span class="number">-0.5685</span>,</span><br><span class="line">          <span class="number">-0.0673</span>, <span class="number">-0.0485</span>],</span><br><span class="line">         [<span class="number">-0.1346</span>,  <span class="number">0.3093</span>, <span class="number">-0.0775</span>, <span class="number">-0.0678</span>, <span class="number">-0.2616</span>,  <span class="number">0.2946</span>, <span class="number">-0.1167</span>,</span><br><span class="line">           <span class="number">0.1699</span>, <span class="number">-0.0280</span>,  <span class="number">0.0067</span>,  <span class="number">0.0333</span>, <span class="number">-0.0924</span>,  <span class="number">0.1308</span>, <span class="number">-0.5332</span>,</span><br><span class="line">          <span class="number">-0.0237</span>, <span class="number">-0.0201</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br><span class="line">padded_output shape</span><br><span class="line"> torch.Size([<span class="number">3</span>, <span class="number">10</span>, <span class="number">16</span>])</span><br><span class="line">length</span><br><span class="line"> tensor([<span class="number">10</span>,  <span class="number">5</span>,  <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>是不是又回来了？16是cell_size*2, 因为是双向。</p><p>可以看到，没参与计算后来被补上来的部分都成为了0！</p><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line">mport os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> util.embedding_util <span class="keyword">import</span> get_embedding</span><br><span class="line"><span class="keyword">from</span> util.plot_util <span class="keyword">import</span> loss_acc_plot</span><br><span class="line"><span class="keyword">from</span> util.lr_util <span class="keyword">import</span> lr_update</span><br><span class="line"><span class="keyword">import</span> config.lstm_config <span class="keyword">as</span> config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BILSTM</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cell_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 checkpoint_dir,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                 model_type,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word2id,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedding_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 keep_prob)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line">        self.history = &#123;&#125;</span><br><span class="line">        self.keep_prob = keep_prob</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding layer</span></span><br><span class="line">        weights = get_embedding(model_type=model_type,</span><br><span class="line">                                word2id=word2id,</span><br><span class="line">                                embedding_dim=embedding_dim)</span><br><span class="line">        <span class="keyword">if</span> model_type == <span class="string">'static'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'non-static'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'rand'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'multichannel'</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'unknown model type'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># BILSTM layer</span></span><br><span class="line">        self.fw_cell = tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">            tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=<span class="number">0.7</span>)</span><br><span class="line">        self.bw_cell = tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">            tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">        self.Dense = tf.layers.Dense(units=num_classes, activation=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, seq_length, training)</span>:</span></span><br><span class="line">        embedded_words = self.embeddings(inputs)</span><br><span class="line">        outputs, final_state = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">            self.fw_cell,</span><br><span class="line">            self.bw_cell,</span><br><span class="line">            inputs=embedded_words,</span><br><span class="line">            sequence_length=seq_length,</span><br><span class="line">            dtype=tf.float32,</span><br><span class="line">            time_major=<span class="keyword">False</span>)</span><br><span class="line">        outputs = tf.concat(outputs, axis=<span class="number">2</span>)</span><br><span class="line">        final_output = outputs[<span class="number">-1</span>]</span><br><span class="line">        logits = self.Dense(final_output)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        preds = self.call(inputs, seq_length, training)</span><br><span class="line">        <span class="comment"># L2正则化</span></span><br><span class="line">        loss_L2 = tf.add_n([tf.nn.l2_loss(v)</span><br><span class="line">                            <span class="keyword">for</span> v <span class="keyword">in</span> self.trainable_variables</span><br><span class="line">                            <span class="keyword">if</span> <span class="string">'bias'</span> <span class="keyword">not</span> <span class="keyword">in</span> v.name]) * <span class="number">0.001</span></span><br><span class="line">        loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)</span><br><span class="line">        loss = loss + loss_L2</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grads_fn</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            loss = self.loss_fn(inputs, target, seq_length, training)</span><br><span class="line">        <span class="keyword">return</span> tape.gradient(loss, self.variables)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self, model)</span>:</span></span><br><span class="line">        <span class="string">""" Function to save trained model.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        checkpoint = tf.train.Checkpoint(model=model)</span><br><span class="line">        checkpoint_prefix = os.path.join(self.checkpoint_dir, <span class="string">'ckpt'</span>)</span><br><span class="line">        checkpoint.save(file_prefix=checkpoint_prefix)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Run the model once to initialize variables</span></span><br><span class="line">        dummy_input = tf.constant(tf.zeros((<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">        dummy_length = tf.constant(<span class="number">1</span>, shape=(<span class="number">1</span>,))</span><br><span class="line">        self(dummy_input, dummy_length, <span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># Restore the variables of the model</span></span><br><span class="line">        saver = tf.contrib.Saver(self.variables)</span><br><span class="line">        saver.restore(tf.train.latest_checkpoint</span><br><span class="line">                      (self.checkpoint_directory))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        y = self.call(inputs, seq_length, training)</span><br><span class="line">        y_pred = tf.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        correct = tf.where(tf.equal(y_pred, target)).numpy().shape[<span class="number">0</span>]</span><br><span class="line">        total = target.numpy().shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> correct/total</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, training_data, eval_data, pbar, num_epochs=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            early_stopping_rounds=<span class="number">5</span>, verbose=<span class="number">1</span>, train_from_scratch=True)</span>:</span></span><br><span class="line">        <span class="string">"""train the model"""</span></span><br><span class="line">        <span class="keyword">if</span> train_from_scratch <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            self.restore_model()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize best loss. This variable will store the lowest loss on the</span></span><br><span class="line">        <span class="comment"># eval dataset.</span></span><br><span class="line">        best_loss = <span class="number">2018</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize classes to update the mean loss of train and eval</span></span><br><span class="line">        train_loss = []</span><br><span class="line">        eval_loss = []</span><br><span class="line">        train_accuracy = []</span><br><span class="line">        eval_accuracy = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize dictionary to store the loss history</span></span><br><span class="line">        self.history[<span class="string">'train_loss'</span>] = []</span><br><span class="line">        self.history[<span class="string">'eval_loss'</span>] = []</span><br><span class="line">        self.history[<span class="string">'train_accuracy'</span>] = []</span><br><span class="line">        self.history[<span class="string">'eval_accuracy'</span>] = []</span><br><span class="line"></span><br><span class="line">        count = early_stopping_rounds</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Begin training</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># 在每个epoch训练之初初始化optimizer，决定是否使用学习率衰减</span></span><br><span class="line">            learning_rate = lr_update(i+<span class="number">1</span>, mode=config.lr_mode)</span><br><span class="line">            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Training with gradient descent</span></span><br><span class="line">            start = time.time()</span><br><span class="line">            <span class="keyword">for</span> index, (sequence, label, seq_length) <span class="keyword">in</span> enumerate(training_data):</span><br><span class="line">                <span class="comment"># cpu需要类型转换，不然会报错：Could not find valid device</span></span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                grads = self.grads_fn(sequence, label, seq_length, training=<span class="keyword">True</span>)</span><br><span class="line">                optimizer.apply_gradients(zip(grads, self.variables))</span><br><span class="line">                pbar.show(index, use_time=time.time()-start)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the loss on the training data after one epoch</span></span><br><span class="line">            <span class="keyword">for</span> sequence, label, seq_length <span class="keyword">in</span> training_data:</span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                train_los = self.loss_fn(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                train_acc = self.get_accuracy(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                train_loss.append(train_los)</span><br><span class="line">                train_accuracy.append(train_acc)</span><br><span class="line">            self.history[<span class="string">'train_loss'</span>].append(np.mean(train_loss))</span><br><span class="line">            self.history[<span class="string">'train_accuracy'</span>].append(np.mean(train_accuracy))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the loss on the eval data after one epoch</span></span><br><span class="line">            <span class="keyword">for</span> sequence, label, seq_length <span class="keyword">in</span> eval_data:</span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                eval_los = self.loss_fn(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                eval_acc = self.get_accuracy(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                eval_loss.append(eval_los)</span><br><span class="line">                eval_accuracy.append(eval_acc)</span><br><span class="line">            self.history[<span class="string">'eval_loss'</span>].append(np.mean(eval_loss))</span><br><span class="line">            self.history[<span class="string">'eval_accuracy'</span>].append(np.mean(eval_accuracy))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print train and eval losses</span></span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span>) | ((i + <span class="number">1</span>) % verbose == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">'Epoch %d - train_loss: %4f - eval_loss: %4f - train_acc:%4f - eval_acc:%4f'</span></span><br><span class="line">                      % (i + <span class="number">1</span>,</span><br><span class="line">                         self.history[<span class="string">'train_loss'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'train_accuracy'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'eval_accuracy'</span>][<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check for early stopping</span></span><br><span class="line">            <span class="keyword">if</span> self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>] &lt; best_loss:</span><br><span class="line">                best_loss = self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>]</span><br><span class="line">                count = early_stopping_rounds</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 画出loss_acc曲线</span></span><br><span class="line">        loss_acc_plot(history=self.history)</span><br></pre></td></tr></table></figure><h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_packed_sequence, pack_padded_sequence</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config.config <span class="keyword">as</span> config</span><br><span class="line"><span class="keyword">from</span> util.embedding_util <span class="keyword">import</span> get_embedding</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">2018</span>)</span><br><span class="line">torch.cuda.manual_seed(<span class="number">2018</span>)</span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">2018</span>)</span><br><span class="line">np.random.seed(<span class="number">2018</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICE"</span>] = <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word_embedding_dimension,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, bi_flag,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layer,</span></span></span><br><span class="line"><span class="function"><span class="params">                 labels,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cell_type,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout,</span></span></span><br><span class="line"><span class="function"><span class="params">                 checkpoint_dir)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.num_label = len(labels)</span><br><span class="line">        self.num_layer = num_layer</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            self.device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, word_embedding_dimension)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            p.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.embedding.weight.data.copy_(torch.from_numpy(get_embedding(vocab_size, word_embedding_dimension)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cell_type == <span class="string">"LSTM"</span>:</span><br><span class="line">            self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension,</span><br><span class="line">                                    hidden_size=hidden_size,</span><br><span class="line">                                    num_layers=num_layer,</span><br><span class="line">                                    batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                    dropout=dropout,</span><br><span class="line">                                    bidirectional=bi_flag)</span><br><span class="line">        <span class="keyword">elif</span> cell_type == <span class="string">"GRU"</span>:</span><br><span class="line">            self.rnn_cell = nn.GRU(input_size=word_embedding_dimension,</span><br><span class="line">                                   hidden_size=hidden_size,</span><br><span class="line">                                   num_layers=num_layer,</span><br><span class="line">                                   batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                   dropout=dropout,</span><br><span class="line">                                   bidirectional=bi_flag)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"RNN: Unknown rnn cell type"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否双向</span></span><br><span class="line">        self.bi_num = <span class="number">2</span> <span class="keyword">if</span> bi_flag <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(hidden_size*self.bi_num, self.num_label)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">        batch_size = inputs.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 初始化态h和C,默认为zeros</span></span><br><span class="line">        h_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float()</span><br><span class="line">        c_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float()</span><br><span class="line"></span><br><span class="line">        embeddings = self.embedding(inputs, length)  <span class="comment"># (batch_size, time_steps, embedding_dim)</span></span><br><span class="line">        <span class="comment"># 去除padding元素</span></span><br><span class="line">        <span class="comment"># embeddings_packed: (batch_size*time_steps, embedding_dim)</span></span><br><span class="line">        embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0))</span><br><span class="line">        <span class="comment"># padded_output: (batch_size, time_steps, hidden_size * bi_num)</span></span><br><span class="line">        <span class="comment"># h_n|c_n: (num_layer*bi_num, batch_size, hidden_size)</span></span><br><span class="line">        padded_output, _ = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 取最后一个有效输出作为最终输出（0为无效输出）</span></span><br><span class="line">        last_output = padded_output[torch.LongTensor(range(batch_size)), length]</span><br><span class="line">        last_output = F.dropout(last_output, p=self.dropout, training=self.training)</span><br><span class="line">        output = self.linear(last_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.load_state_dict(torch.load(self.checkpoint_dir))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self)</span>:</span></span><br><span class="line">        torch.save(self.state_dict(), self.checkpoint_dir)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, y_pred, y_true)</span>:</span></span><br><span class="line">        _, y_pred = torch.max(y_pred.data, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> config.use_cuda:</span><br><span class="line">            y_true = y_true.cpu().numpy()</span><br><span class="line">            y_pred = y_pred.cpu().numpy()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_true = y_true.numpy()</span><br><span class="line">            y_pred = y_pred.numpy()</span><br><span class="line">        f1 = f1_score(y_true, y_pred, labels=self.labels, average=<span class="string">"macro"</span>)</span><br><span class="line">        correct = np.sum((y_true==y_pred).astype(int))</span><br><span class="line">        acc = correct/y_pred.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> (acc, f1)</span><br></pre></td></tr></table></figure><p>各位晚安~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;本文介绍pytorch RNN网络搭建，主要包括LSTM和G
      
    
    </summary>
    
      <category term="pytorch" scheme="http://state-of-art.top/categories/pytorch/"/>
    
    
      <category term="pytorch，lstm，nlp" scheme="http://state-of-art.top/tags/pytorch%EF%BC%8Clstm%EF%BC%8Cnlp/"/>
    
  </entry>
  
  <entry>
    <title>文本预处理</title>
    <link href="http://state-of-art.top/2018/11/28/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://state-of-art.top/2018/11/28/文本预处理/</id>
    <published>2018-11-28T15:21:08.000Z</published>
    <updated>2018-11-28T14:14:22.908Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本文介绍如何利用有限内存进行大规模数据处理，主要包括：</p><ul><li>建立词典</li><li>将单词转换为id</li><li>训练集验证集切分</li></ul><h2 id="How-To-Do-IT"><a href="#How-To-Do-IT" class="headerlink" title="How To Do IT"></a>How To Do IT</h2><h3 id="原始数据集"><a href="#原始数据集" class="headerlink" title="原始数据集"></a>原始数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">11</span>@成都 高新技术 产业 开发区 人民 检察院 指控 ， <span class="number">2015</span>年 <span class="number">3</span> 月 <span class="number">29</span>日 <span class="number">23</span>时 许 ， 被告人 刘某 某 饮 酒后 驾驶川 A ＊ ＊ ＊ <span class="number">84</span> 北京 现代牌 小型 轿车 ， 从 成都市 桐梓林 附近 出发 上 人民 南 路 出 城 ， 当 车 行驶 至 成都 高新区 天府 大道 与 府城 大道 交叉 路口处 时 ，  公诉 机关 认为 ， 被告人 刘 某某 在 道路 上 醉 酒 驾驶 机动车 ， 危害 公共 安全 ， 其 行为 应当 以 ×× 追究 其 刑事 责任 。</span><br><span class="line"></span><br><span class="line"><span class="number">11</span>@黑龙江省 尚志市 人民 检察院 指控 ： ×× <span class="number">2014</span>年 <span class="number">9</span> 月 <span class="number">22</span>日 <span class="number">20</span>时 许 ， 被告人 矫 <span class="number">2</span> 某 在 尚志市 苇河镇 阿里郎歌厅 对面 停放 的 货车 的 副 驾驶 座位 上 ， 将 被害人 李某 甲 的 蓝色 女式 拎 包 盗 走 ， 包 内 有 人民币 <span class="number">57000</span> 元 ， 红色 钱包 一个 ， 农业 银行卡 一 张 ， 身份证 一 张 、 驾驶证 一 本 、 账本 一 册 。 案 发 前 ， 被告人 矫 <span class="number">2</span> 某 将 盗走 的 财物 返还 被害人 。  在 ×× 到 五 年 幅度 内 量刑 ， 并 处 罚金 ； 对 所 犯 的 ×× 在 ×× 到 六 个 月 幅度 内 量刑 ， 并 处 罚金 。 针对 上述 指控 ， 公诉 机关 提供 了 相应 的 证据 。</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这里以<a href="http://cail.cipsc.org.cn/" target="_blank" rel="noopener">法研杯</a>比赛的文本数据集为例。格式为 标签@文本</p><p>其中，文本已经过分词处理，使用空格分隔。</p><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent_label_split</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    句子处理成单词</span></span><br><span class="line"><span class="string">    :param line: 原始行</span></span><br><span class="line"><span class="string">    :return: 单词， 标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    line = line.strip(<span class="string">'\n'</span>).split(<span class="string">'@'</span>)</span><br><span class="line">    label = line[<span class="number">0</span>]</span><br><span class="line">    sent = line[<span class="number">1</span>].split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">return</span> sent, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_to_id</span><span class="params">(word, word2id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单词--&gt;ID</span></span><br><span class="line"><span class="string">    :param word: 单词</span></span><br><span class="line"><span class="string">    :param word2id: word2id @type: dict</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> word2id[word] <span class="keyword">if</span> word <span class="keyword">in</span> word2id <span class="keyword">else</span> word2id[<span class="string">'unk'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_vocab</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_word_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    建立词典</span></span><br><span class="line"><span class="string">    :param vocab_size: 词典大小</span></span><br><span class="line"><span class="string">    :param min_freq: 最小词频限制</span></span><br><span class="line"><span class="string">    :param stop_list: 停用词 @type：file_path</span></span><br><span class="line"><span class="string">    :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试</span></span><br><span class="line"><span class="string">    :return: word2id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    count = Counter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        logger.info(<span class="string">'Building vocab'</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'Build vocab'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            count.update(words)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> stop_word_list:</span><br><span class="line">        stop_list = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fr):</span><br><span class="line">                    word = line.strip(<span class="string">'\n'</span>)</span><br><span class="line">                    <span class="keyword">if</span> stop_list.get(word) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                        stop_list[word] = i</span><br><span class="line">        count = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> count.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> stop_list&#125;</span><br><span class="line">    count = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 词典</span></span><br><span class="line">    vocab = [w[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> count <span class="keyword">if</span> w[<span class="number">1</span>] &gt;= min_freq]</span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; len(vocab):</span><br><span class="line">        vocab = vocab[:vocab_size]</span><br><span class="line">    vocab = config.flag_words + vocab</span><br><span class="line">    logger.info(<span class="string">'vocab_size is %d'</span>%len(vocab))</span><br><span class="line">    <span class="comment"># 词典到编号的映射</span></span><br><span class="line">    word2id = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> zip(vocab, range(<span class="number">0</span>, len(vocab)))&#125;</span><br><span class="line">    <span class="keyword">assert</span> word2id[<span class="string">'&lt;pad&gt;'</span>] == <span class="number">0</span>, <span class="string">"ValueError: '&lt;pad&gt;' id is not 0"</span></span><br><span class="line">    print(word2id)</span><br><span class="line">    <span class="keyword">with</span> open(config.WORD2ID_FILE, <span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        pickle.dump(word2id, fw)</span><br><span class="line">    <span class="keyword">return</span> word2id</span><br></pre></td></tr></table></figure><h3 id="文本映射到Id"><a href="#文本映射到Id" class="headerlink" title="文本映射到Id"></a>文本映射到Id</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text2id</span><span class="params">(word2id, maxlen=None, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集文本转ID</span></span><br><span class="line"><span class="string">    :param valid_size: 验证集大小</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))</span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Text to id file existed'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    logger.info(<span class="string">'Text to id'</span>)</span><br><span class="line">    sentences, labels, lengths = [], [], []</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'text_to_id'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            sent = [word_to_id(word=word, word2id=word2id) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">            <span class="keyword">if</span> maxlen:</span><br><span class="line">                sent = sent[:maxlen]</span><br><span class="line">            length = len(sent)</span><br><span class="line">            sentences.append(sent)</span><br><span class="line">            labels.append(label)</span><br><span class="line">            lengths.append(length)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    train, valid = train_val_split(sentences, labels,</span><br><span class="line">                                   valid_size=valid_size,</span><br><span class="line">                                   random_state=random_state,</span><br><span class="line">                                   shuffle=shuffle)</span><br><span class="line">    <span class="keyword">del</span> sentences, labels, lengths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing train to file done'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.VALID_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing valid to file done'</span>)</span><br></pre></td></tr></table></figure><h3 id="训练集验证集分割"><a href="#训练集验证集分割" class="headerlink" title="训练集验证集分割"></a>训练集验证集分割</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_split</span><span class="params">(X, y, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集验证集分割</span></span><br><span class="line"><span class="string">    :param X: sentences</span></span><br><span class="line"><span class="string">    :param y: labels</span></span><br><span class="line"><span class="string">    :param random_state: 随机种子</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger.info(<span class="string">'train val split'</span>)</span><br><span class="line">    data = [(data_x, data_y) <span class="keyword">for</span> data_x, data_y <span class="keyword">in</span> zip(X, y)]</span><br><span class="line">    N = len(data)</span><br><span class="line">    test_size = int(N * valid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.seed(random_state)</span><br><span class="line">        random.shuffle(data)</span><br><span class="line"></span><br><span class="line">    valid = data[:test_size]</span><br><span class="line">    train = data[test_size:]</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">from</span> Logginger <span class="keyword">import</span> init_logger</span><br><span class="line"></span><br><span class="line">logger = init_logger(<span class="string">"torch"</span>, logging_path=config.LOG_PATH)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent_label_split</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    句子处理成单词</span></span><br><span class="line"><span class="string">    :param line: 原始行</span></span><br><span class="line"><span class="string">    :return: 单词， 标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    line = line.strip(<span class="string">'\n'</span>).split(<span class="string">'@'</span>)</span><br><span class="line">    label = line[<span class="number">0</span>]</span><br><span class="line">    sent = line[<span class="number">1</span>].split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">return</span> sent, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_to_id</span><span class="params">(word, word2id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单词--&gt;ID</span></span><br><span class="line"><span class="string">    :param word: 单词</span></span><br><span class="line"><span class="string">    :param word2id: word2id @type: dict</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> word2id[word] <span class="keyword">if</span> word <span class="keyword">in</span> word2id <span class="keyword">else</span> word2id[<span class="string">'unk'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_vocab</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_word_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    建立词典</span></span><br><span class="line"><span class="string">    :param vocab_size: 词典大小</span></span><br><span class="line"><span class="string">    :param min_freq: 最小词频限制</span></span><br><span class="line"><span class="string">    :param stop_list: 停用词 @type：file_path</span></span><br><span class="line"><span class="string">    :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试</span></span><br><span class="line"><span class="string">    :return: word2id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    count = Counter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        logger.info(<span class="string">'Building vocab'</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'Build vocab'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            count.update(words)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> stop_word_list:</span><br><span class="line">        stop_list = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fr):</span><br><span class="line">                    word = line.strip(<span class="string">'\n'</span>)</span><br><span class="line">                    <span class="keyword">if</span> stop_list.get(word) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                        stop_list[word] = i</span><br><span class="line">        count = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> count.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> stop_list&#125;</span><br><span class="line">    count = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 词典</span></span><br><span class="line">    vocab = [w[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> count <span class="keyword">if</span> w[<span class="number">1</span>] &gt;= min_freq]</span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; len(vocab):</span><br><span class="line">        vocab = vocab[:vocab_size]</span><br><span class="line">    vocab = config.flag_words + vocab</span><br><span class="line">    logger.info(<span class="string">'vocab_size is %d'</span>%len(vocab))</span><br><span class="line">    <span class="comment"># 词典到编号的映射</span></span><br><span class="line">    word2id = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> zip(vocab, range(<span class="number">0</span>, len(vocab)))&#125;</span><br><span class="line">    <span class="keyword">assert</span> word2id[<span class="string">'&lt;pad&gt;'</span>] == <span class="number">0</span>, <span class="string">"ValueError: '&lt;pad&gt;' id is not 0"</span></span><br><span class="line">    print(word2id)</span><br><span class="line">    <span class="keyword">with</span> open(config.WORD2ID_FILE, <span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        pickle.dump(word2id, fw)</span><br><span class="line">    <span class="keyword">return</span> word2id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_split</span><span class="params">(X, y, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集验证集分割</span></span><br><span class="line"><span class="string">    :param X: sentences</span></span><br><span class="line"><span class="string">    :param y: labels</span></span><br><span class="line"><span class="string">    :param random_state: 随机种子</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger.info(<span class="string">'train val split'</span>)</span><br><span class="line">    data = [(data_x, data_y) <span class="keyword">for</span> data_x, data_y <span class="keyword">in</span> zip(X, y)]</span><br><span class="line">    N = len(data)</span><br><span class="line">    test_size = int(N * valid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.seed(random_state)</span><br><span class="line">        random.shuffle(data)</span><br><span class="line"></span><br><span class="line">    valid = data[:test_size]</span><br><span class="line">    train = data[test_size:]</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text2id</span><span class="params">(word2id, maxlen=None, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集文本转ID</span></span><br><span class="line"><span class="string">    :param valid_size: 验证集大小</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))</span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Text to id file existed'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    logger.info(<span class="string">'Text to id'</span>)</span><br><span class="line">    sentences, labels, lengths = [], [], []</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'text_to_id'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            sent = [word_to_id(word=word, word2id=word2id) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">            <span class="keyword">if</span> maxlen:</span><br><span class="line">                sent = sent[:maxlen]</span><br><span class="line">            length = len(sent)</span><br><span class="line">            sentences.append(sent)</span><br><span class="line">            labels.append(label)</span><br><span class="line">            lengths.append(length)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    train, valid = train_val_split(sentences, labels,</span><br><span class="line">                                   valid_size=valid_size,</span><br><span class="line">                                   random_state=random_state,</span><br><span class="line">                                   shuffle=shuffle)</span><br><span class="line">    <span class="keyword">del</span> sentences, labels, lengths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing train to file done'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.VALID_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing valid to file done'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能整合，提供给外部调用的函数接口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_helper</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="comment"># 判断文件是否已存在</span></span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Word to id file existed'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE), <span class="string">'rb'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">            word2id = pickle.load(fr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        word2id = bulid_vocab(vocab_size=vocab_size, min_freq=min_freq, stop_word_list=stop_list,</span><br><span class="line">                is_debug=is_debug)</span><br><span class="line">    text2id(word2id, valid_size=valid_size, random_state=random_state, shuffle=shuffle, is_debug=is_debug)</span><br></pre></td></tr></table></figure><p>config.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------PATH------------</span></span><br><span class="line">ROOT_DIR = <span class="string">'/home/daizelin/pytorch/'</span></span><br><span class="line">RAW_DATA = <span class="string">'data/data_for_test.csv'</span></span><br><span class="line">TRAIN_FILE = <span class="string">'outputs/intermediate/train.tsv'</span></span><br><span class="line">VALID_FILE = <span class="string">'outputs/intermediate/valid.tsv'</span></span><br><span class="line">LOG_PATH = <span class="string">'outputs/logs'</span></span><br><span class="line">is_debug = <span class="keyword">False</span></span><br><span class="line">flag_words = [<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;unk&gt;'</span>]</span><br></pre></td></tr></table></figure><p>Logginger.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> logging <span class="keyword">import</span> Logger</span><br><span class="line"><span class="keyword">from</span> logging.handlers <span class="keyword">import</span> TimedRotatingFileHandler</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">使用方式</span></span><br><span class="line"><span class="string">from you_logging_filename.py import init_logger</span></span><br><span class="line"><span class="string">logger = init_logger("dataset",logging_path='')</span></span><br><span class="line"><span class="string">def you_function():</span></span><br><span class="line"><span class="string">logger.info()</span></span><br><span class="line"><span class="string">logger.error()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">日志模块</span></span><br><span class="line"><span class="string">1. 同时将日志打印到屏幕跟文件中</span></span><br><span class="line"><span class="string">2. 默认值保留近7天日志文件</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_logger</span><span class="params">(logger_name, logging_path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> logger_name <span class="keyword">not</span> <span class="keyword">in</span> Logger.manager.loggerDict:</span><br><span class="line">        logger  = logging.getLogger(logger_name)</span><br><span class="line">        logger.setLevel(logging.DEBUG)</span><br><span class="line">        handler = TimedRotatingFileHandler(filename=logging_path+<span class="string">"/all.log"</span>,when=<span class="string">'D'</span>,backupCount = <span class="number">7</span>)</span><br><span class="line">        datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">        format_str = <span class="string">'[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'</span></span><br><span class="line">        formatter = logging.Formatter(format_str,datefmt)</span><br><span class="line">        handler.setFormatter(formatter)</span><br><span class="line">        handler.setLevel(logging.INFO)</span><br><span class="line">        logger.addHandler(handler)</span><br><span class="line">        console= logging.StreamHandler()</span><br><span class="line">        console.setLevel(logging.INFO)</span><br><span class="line">        console.setFormatter(formatter)</span><br><span class="line">        logger.addHandler(console)</span><br><span class="line"></span><br><span class="line">        handler = TimedRotatingFileHandler(filename=logging_path+<span class="string">"/error.log"</span>,when=<span class="string">'D'</span>,backupCount=<span class="number">7</span>)</span><br><span class="line">        datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">        format_str = <span class="string">'[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'</span></span><br><span class="line">        formatter = logging.Formatter(format_str,datefmt)</span><br><span class="line">        handler.setFormatter(formatter)</span><br><span class="line">        handler.setLevel(logging.ERROR)</span><br><span class="line">        logger.addHandler(handler)</span><br><span class="line">    logger = logging.getLogger(logger_name)</span><br><span class="line">    <span class="keyword">return</span> logger</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本
      
    
    </summary>
    
      <category term="NLP" scheme="http://state-of-art.top/categories/NLP/"/>
    
    
      <category term="文本预处理" scheme="http://state-of-art.top/tags/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>torchtext读取文本数据集</title>
    <link href="http://state-of-art.top/2018/11/28/torchtext%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://state-of-art.top/2018/11/28/torchtext读取文本数据集/</id>
    <published>2018-11-28T15:21:08.000Z</published>
    <updated>2018-11-28T14:15:32.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文主要介绍如何使用Torchtext读取文本数据集。</p><p>Torchtext是非官方的、一种为pytorch提供文本数据处理能力的库， 类似于图像处理库Torchvision。</p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><ol><li>下载地址：<a href="https://github.com/text" target="_blank" rel="noopener">https://github.com/text</a></li><li>安装：pip install text-master.zip</li><li>测试安装是否成功： import torchtext</li></ol><h2 id="How-To-Use"><a href="#How-To-Use" class="headerlink" title="How To Use"></a>How To Use</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/20181128/1.png" alt="image text"></p><p>先上一张图。使用tortext的目的是将文本转换成Batch，方便后面训练模型时使用。过程如下:</p><ul><li>使用Field对象进行文本预处理， 生成example</li><li>使用Dataset类生成数据集dataset</li><li>使用Iterator生成迭代器</li></ul><p>从图中还可以看到，torchtext可以生成词典vocab和词向量embedding，但个人比较喜欢将这两步放在数据预处理和模型里面进行，所以这两个功能不在本文之列。</p><h3 id="常用的类"><a href="#常用的类" class="headerlink" title="常用的类"></a>常用的类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, Example, TabularDataset</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> BucketIterator</span><br></pre></td></tr></table></figure><p>Field：用来定义字段以及文本预处理方法</p><p>Example: 用来表示一个样本，通常为“数据+标签”</p><p>TabularDataset: 用来从文件中读取数据，生成Dataset， Dataset是Example实例的集合</p><p>BucketIterator：迭代器，用来生成batch， 类似的有Iterator，Buckeiterator的功能较强大点，支持排序，动态padding等</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>见我上篇博文&lt;文本预处理&gt;。使用生成的train.tsv和valid.tsv。</p><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><h4 id="创建Field对象"><a href="#创建Field对象" class="headerlink" title="创建Field对象"></a>创建Field对象</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">x_tokenize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 如果加载进来的是已经转成id的文本</span></span><br><span class="line">    <span class="comment"># 此处必须将字符串转换成整型</span></span><br><span class="line">    <span class="comment"># 否则必须将use_vocab设为True</span></span><br><span class="line">    <span class="keyword">return</span> [int(c) <span class="keyword">for</span> c <span class="keyword">in</span> x.split()]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_tokenize</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(y)</span><br><span class="line"></span><br><span class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                     use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                     fix_length=self.fix_length, </span><br><span class="line">                     eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                     include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line">LABEL = Field(sequential=<span class="keyword">False</span>, tokenize=y_tokenize, use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><ul><li>sequential     类型boolean,  作用：是否为序列，一般文本都为True，标签为False</li><li>tokenize    类型: function， 作用: 文本处理，默认为str.split(), 这里对x和y分别自定义了处理函数。</li><li>use_vocab： 类型: boolean， 作用：是否建立词典</li><li>batch_first：类型: boolean， 作用：为True则返回Batch维度为(batch_size， 文本长度), False 则相反</li><li>fix_length：类型: int, 作用：固定文本的长度，长则截断，短则padding，可认为是静态padding；为None则按每个Batch内的最大长度进行动态padding。</li><li>eos_token：类型：str, 作用: 句子结束字符</li><li>init_token：类型：str, 作用: 句子开始字符</li><li>include_lengths：类型: boolean， 作用：是否返回句子的原始长度，一般为True，方便RNN使用。</li><li>pad_token：padding的字符，默认为”<pad>“, 这里因为原始数据已经转成了int类型，所以使用0。注意这里的pad_token要和你的词典vocab里的“<pad>”的Id保持一致，否则会影响后面词向量的读取。</pad></pad></li></ul><h4 id="读取文件生成数据集"><a href="#读取文件生成数据集" class="headerlink" title="读取文件生成数据集"></a>读取文件生成数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fields = [</span><br><span class="line">    (<span class="string">"label"</span>, LABEL), (<span class="string">"text"</span>, TEXT)]</span><br><span class="line"></span><br><span class="line">train, valid = TabularDataset.splits(</span><br><span class="line">    path=config.ROOT_DIR,</span><br><span class="line">    train=self.train_path, validation=self.valid_path,</span><br><span class="line">    format=<span class="string">'tsv'</span>,</span><br><span class="line">    skip_header=<span class="keyword">False</span>,</span><br><span class="line">    fields=fields)</span><br><span class="line"><span class="keyword">return</span> train, valid</span><br></pre></td></tr></table></figure><h4 id="生成迭代器"><a href="#生成迭代器" class="headerlink" title="生成迭代器"></a>生成迭代器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_iter, val_iter = BucketIterator.splits((train, valid),</span><br><span class="line">                                             batch_sizes=(self.batch_size, self.batch_size),</span><br><span class="line">                                             device = torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">                                             sort_key=<span class="keyword">lambda</span> x: len(x.text), <span class="comment"># field sorted by len</span></span><br><span class="line">                                             sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                             repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>这里要注意的是sort_with_batch要设置为True，并指定排序的key为文本长度，方便后面pytorch RNN进行pack和pad。</p><p>我们来看下train_iter和val_iter里放了什么东西。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=<span class="number">1</span>, fix_length=<span class="keyword">None</span>)</span><br><span class="line">train, valid  = bi.create_dataset()</span><br><span class="line">train_iter, valid_iter = bi.get_iterator(train, valid)</span><br><span class="line">batch = next(iter(train_iter))</span><br><span class="line">print(train_iter)</span><br><span class="line">print(<span class="string">'batch:\n'</span>, batch)</span><br><span class="line">print(<span class="string">'batch_text:\n'</span>, batch.text)</span><br><span class="line">print(<span class="string">'batch_label:\n'</span>, batch.label)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;torchtext.data.iterator.BucketIterator object at <span class="number">0x7f04a9d845f8</span>&gt;</span><br><span class="line">batch:</span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">1</span>]</span><br><span class="line">[.label]:[torch.LongTensor of size <span class="number">1</span>]</span><br><span class="line">[.text]:(<span class="string">'[torch.LongTensor of size 1x125]'</span>, <span class="string">'[torch.LongTensor of size 1]'</span>)</span><br><span class="line">batch_text:</span><br><span class="line"> (tensor([[<span class="number">11149</span>,  <span class="number">7772</span>, <span class="number">13752</span>, <span class="number">13743</span>, <span class="number">13773</span>, <span class="number">13793</span>, <span class="number">13791</span>, <span class="number">13591</span>, <span class="number">12478</span>, <span class="number">13759</span>,</span><br><span class="line">         <span class="number">13783</span>, <span class="number">13492</span>, <span class="number">13793</span>, <span class="number">13745</span>, <span class="number">13754</span>, <span class="number">13612</span>,  <span class="number">7452</span>, <span class="number">12185</span>, <span class="number">13789</span>, <span class="number">13784</span>,</span><br><span class="line">         <span class="number">13765</span>, <span class="number">12451</span>, <span class="number">12112</span>, <span class="number">13620</span>, <span class="number">12240</span>, <span class="number">13073</span>, <span class="number">13790</span>, <span class="number">13738</span>, <span class="number">13637</span>, <span class="number">13759</span>,</span><br><span class="line">         <span class="number">13776</span>, <span class="number">13793</span>, <span class="number">13739</span>, <span class="number">13783</span>, <span class="number">13787</span>, <span class="number">13793</span>, <span class="number">12702</span>, <span class="number">13790</span>, <span class="number">13698</span>, <span class="number">13774</span>,</span><br><span class="line">         <span class="number">13792</span>, <span class="number">13768</span>, <span class="number">13715</span>, <span class="number">13641</span>, <span class="number">13761</span>, <span class="number">13713</span>, <span class="number">13682</span>, <span class="number">13712</span>, <span class="number">13786</span>, <span class="number">13749</span>,</span><br><span class="line">         <span class="number">13097</span>, <span class="number">13734</span>, <span class="number">13702</span>, <span class="number">13735</span>, <span class="number">13257</span>, <span class="number">13642</span>, <span class="number">13700</span>, <span class="number">13793</span>, <span class="number">13684</span>, <span class="number">13755</span>,</span><br><span class="line">         <span class="number">13488</span>, <span class="number">13789</span>, <span class="number">13750</span>, <span class="number">13484</span>, <span class="number">13494</span>, <span class="number">13793</span>, <span class="number">13624</span>, <span class="number">13670</span>, <span class="number">13786</span>, <span class="number">13655</span>,</span><br><span class="line">         <span class="number">13768</span>, <span class="number">13687</span>, <span class="number">13774</span>, <span class="number">13792</span>, <span class="number">13791</span>, <span class="number">13591</span>, <span class="number">13546</span>, <span class="number">13777</span>, <span class="number">13658</span>, <span class="number">13740</span>,</span><br><span class="line">         <span class="number">13577</span>, <span class="number">13790</span>, <span class="number">13684</span>, <span class="number">13755</span>, <span class="number">13793</span>, <span class="number">13572</span>, <span class="number">12891</span>, <span class="number">13793</span>, <span class="number">13368</span>, <span class="number">13713</span>,</span><br><span class="line">         <span class="number">13682</span>, <span class="number">13712</span>, <span class="number">13786</span>, <span class="number">13786</span>, <span class="number">13642</span>, <span class="number">13700</span>, <span class="number">13793</span>, <span class="number">13429</span>, <span class="number">13520</span>, <span class="number">13613</span>,</span><br><span class="line">         <span class="number">13792</span>, <span class="number">13368</span>, <span class="number">13790</span>, <span class="number">13750</span>, <span class="number">13699</span>, <span class="number">13764</span>, <span class="number">13590</span>, <span class="number">13675</span>, <span class="number">13742</span>, <span class="number">13691</span>,</span><br><span class="line">         <span class="number">13688</span>, <span class="number">13742</span>, <span class="number">13782</span>, <span class="number">13538</span>, <span class="number">13742</span>, <span class="number">13783</span>, <span class="number">13787</span>, <span class="number">13774</span>, <span class="number">13645</span>, <span class="number">13742</span>,</span><br><span class="line">         <span class="number">13791</span>, <span class="number">13740</span>, <span class="number">13744</span>, <span class="number">13750</span>, <span class="number">13792</span>]]), tensor([<span class="number">125</span>]))</span><br><span class="line">batch_label:</span><br><span class="line"> tensor([<span class="number">11</span>])</span><br></pre></td></tr></table></figure><p>可以看到batch有两个属性，分别为label和text, text是一个元组，第一个元素为文本，第二个元素为文本原始长度（这里因为我们在定义TEXT时使用了include_lengths=True，否则这里只返回文本）， label则是标签。</p><p>这里为了方便展示只使用了一个batch，返回的batch维度为（batch_size * length）, 数据格式为LongTensor。如果想看动态padding的效果，可多取几个batch，会发现他们是按照长度进行排序，并且是以0进行padding的。</p><h4 id="对Batch包装一下，方便调用"><a href="#对Batch包装一下，方便调用" class="headerlink" title="对Batch包装一下，方便调用"></a>对Batch包装一下，方便调用</h4><p>通过以上步骤，我们能够得到一个batch。但是很快就发现有个不太方便的地方。我们只能通过batch的属性，即自定义的字段名称，如text和label，来访问数据。这样的话在训练时我们只能这样操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_iter:</span><br><span class="line">        inputs = batch.text[<span class="number">0</span>]</span><br><span class="line">        label = batch.label</span><br><span class="line">        length = batch.text[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>万一这个字段改了，还要去改训练的代码，很麻烦，关键是显得很LOW，姿势不对。</p><p>怎么办呢？</p><p>我们对获得的iter进行包装一下，就可以避免这个问题了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""对batch做个包装，方便调用，可选择性使用"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                temp = [getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars]</span><br><span class="line">                label = torch.cat(temp, dim=<span class="number">1</span>).long()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'BatchWrapper: invalid label'</span>)</span><br><span class="line">            text = x[<span class="number">0</span>]</span><br><span class="line">            length = x[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> (text, label, length)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br></pre></td></tr></table></figure><p>我们这样使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br></pre></td></tr></table></figure><p>这样你就会发现batch不再有text和label属性了，而是一个三元组（text， label， length），调用时</p><p>就可以采用如下优雅一点的姿势：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> inputs, label, length <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>data_loader.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""将id格式的输入转换成dataset，并做动态padding"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, TabularDataset</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> BucketIterator</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">x_tokenize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 如果加载进来的是已经转成id的文本</span></span><br><span class="line">    <span class="comment"># 此处必须将字符串转换成整型</span></span><br><span class="line">    <span class="keyword">return</span> [int(c) <span class="keyword">for</span> c <span class="keyword">in</span> x.split()]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_tokenize</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(y)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchIterator</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_path, valid_path,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size, fix_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 x_var=<span class="string">"text"</span>, y_var=[<span class="string">"label"</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 format=<span class="string">'tsv'</span>)</span>:</span></span><br><span class="line">        self.train_path = train_path</span><br><span class="line">        self.valid_path = valid_path</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.fix_length = fix_length</span><br><span class="line">        self.format = format</span><br><span class="line">        self.x_var = x_var</span><br><span class="line">        self.y_vars = y_var</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line">        TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                     use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                     fix_length=self.fix_length,   <span class="comment">#  如需静态padding,则设置fix_length, 但要注意要大于文本最大长度</span></span><br><span class="line">                     eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                     include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line">        LABEL = Field(sequential=<span class="keyword">False</span>, tokenize=y_tokenize, use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        fields = [</span><br><span class="line">            (<span class="string">"label"</span>, LABEL), (<span class="string">"text"</span>, TEXT)]</span><br><span class="line"></span><br><span class="line">        train, valid = TabularDataset.splits(</span><br><span class="line">            path=config.ROOT_DIR,</span><br><span class="line">            train=self.train_path, validation=self.valid_path,</span><br><span class="line">            format=<span class="string">'tsv'</span>,</span><br><span class="line">            skip_header=<span class="keyword">False</span>,</span><br><span class="line">            fields=fields)</span><br><span class="line">        <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_iterator</span><span class="params">(self, train, valid)</span>:</span></span><br><span class="line">        train_iter, val_iter = BucketIterator.splits((train, valid),</span><br><span class="line">                                                     batch_sizes=(self.batch_size, self.batch_size),</span><br><span class="line">                                                     device = torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">                                                     sort_key=<span class="keyword">lambda</span> x: len(x.text), <span class="comment"># field sorted by len</span></span><br><span class="line">                                                     sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                                     repeat=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">        train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">        val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">        <span class="comment">### batch = iter(train_iter)</span></span><br><span class="line">        <span class="comment">### batch： ((text, length), y)</span></span><br><span class="line">        <span class="keyword">return</span> train_iter, val_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""对batch做个包装，方便调用，可选择性使用"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                temp = [getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars]</span><br><span class="line">                y = torch.cat(temp, dim=<span class="number">1</span>).long()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'BatchWrapper: invalid label'</span>)</span><br><span class="line">            text = x[<span class="number">0</span>]</span><br><span class="line">            length = x[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> (text, y, length)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=<span class="number">1</span>, fix_length=<span class="keyword">None</span>)</span><br><span class="line">    train, valid  = bi.create_dataset()</span><br><span class="line">    train_iter, valid_iter = bi.get_iterator(train, valid)</span><br><span class="line">    batch = next(iter(train_iter))</span><br><span class="line">    print(train_iter)</span><br><span class="line">    print(<span class="string">'batch:\n'</span>, batch)</span><br><span class="line">    print(<span class="string">'batch_text:\n'</span>, batch.text)</span><br><span class="line">    print(<span class="string">'batch_label:\n'</span>, batch.label)</span><br></pre></td></tr></table></figure><p>config.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_FILE = <span class="string">'outputs/intermediate/train.tsv'</span></span><br><span class="line">VALID_FILE = <span class="string">'outputs/intermediate/valid.tsv'</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;本文主要介绍如何使用Torchtext读取文本数据集。&lt;/p
      
    
    </summary>
    
      <category term="NLP" scheme="http://state-of-art.top/categories/NLP/"/>
    
    
      <category term="torchtext，pytorch" scheme="http://state-of-art.top/tags/torchtext%EF%BC%8Cpytorch/"/>
    
  </entry>
  
  <entry>
    <title>周期性学习率(Cyclical Learning Rate)技术</title>
    <link href="http://state-of-art.top/2018/10/28/%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%8A%80%E6%9C%AF/"/>
    <id>http://state-of-art.top/2018/10/28/周期性学习率技术/</id>
    <published>2018-10-28T15:21:08.000Z</published>
    <updated>2018-10-30T14:25:09.279Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍神经网络训练中的周期性学习率技术。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>学习率(learning_rate, LR)是神经网络训练过程中最重要的超参数之一，它对于快速、高效地训练神经网络至关重要。简单来说，LR决定了我们当前的权重参数朝着降低损失的方向上改变多少。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_weight = exsiting_weight - learning_rate * gradient</span><br></pre></td></tr></table></figure><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-neural_network-7.png" alt="image text"></p><p><em><center>Fig.: A simple neural network where the w’s and b’s are to be learnt (Img Credit: Matt Mazur)</center></em></p><p>这看上去很简单。但是正如许多研究显示的那样，单单通过提升这一步就会对我们的训练产生深远的影响，并且尚有很大的优化空间。</p><p>本文介绍了一种叫做周期性学习率（CLR）的技术，它是一种非常新的、简单的想法，用来设置和控制训练过程中LR的大小。该技术在<a href="https://twitter.com/jeremyphoward" target="_blank" rel="noopener">jeremyphoward</a>今年的<a href="http://www.fast.ai/" target="_blank" rel="noopener">fast.ai course</a>课程中提及过。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>神经网络用来完成某项任务需要对大量参数进行训练。参数训练意味着寻找合适的一些参数，使得在每个batch训练完成后损失（loss）达到最小，而参数更新的方式则与LR密切相关。</p><p>通常来说，有两种广泛使用的方法用来设置训练过程中的LR。</p><h3 id="One-LR-for-all-parameters"><a href="#One-LR-for-all-parameters" class="headerlink" title="One LR for all parameters"></a>One LR for all parameters</h3><p>一个典型的例子是<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">SGD</a>， 在训练开始时设置一个LR常量，并且设定一个LR衰减策略（如step，exponential等）。这个单一的LR用来更新所有的参数。在每个epochs中，LR按预先设定随时间逐渐衰减，当我们临近最小损失时， 通过衰减可以减缓更新，以防止我们越过最小值。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-learningrates.jpeg" alt="image text"></p><p><em><center>Fig. Effect of various learning rates on convergence (Img Credit: cs231n)</center></em></p><p>该方法存在如下挑战(<a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">refer</a>)：</p><ol><li>难以选择初始的LR达到想要的效果（如上图所示）；</li><li>LR衰减策略同样难以设定，他们很难自适应动态变化的数据；</li><li>所有的参数使用相同的LR进行更新，而这些参数可能学习速率不完全相同；</li><li>很容易陷入马鞍点不能自拔</li></ol><h3 id="Adaptive-LR-for-each-parameter"><a href="#Adaptive-LR-for-each-parameter" class="headerlink" title="Adaptive LR for each parameter"></a>Adaptive LR for each parameter</h3><p>一些改进的优化器如<em>AdaGrad</em>, <em>AdaDelta</em>, <em>RMSprop</em> and <em>Adam</em> 很大程度上缓解了上述困难，方法是对每个参数采用不同的自适应学习率。比如AdaDelta，它的更新机制甚至不需要我们主动设置默认的学习率。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/%E8%87%AA%E9%80%82%E5%BA%94SGD%E6%96%B9%E6%B3%95.gif" alt="image text"></p><p><em><center>Fig: Animation comparing optimization algorithms (Img Credit: Alec Radford)</center></em></p><h2 id="Cycling-Learning-Rate"><a href="#Cycling-Learning-Rate" class="headerlink" title="Cycling Learning Rate"></a>Cycling Learning Rate</h2><p>CLR是Leslie Smith于2015年提出的。这是一种调节LR的方法，在该方法中，设定一个LR上限和下限，LR的值在上限和下限的区间里周期性地变化。看上去，LCR似乎是自适应LR技术和SGD的竞争者，事实上，CLR技术是可以和上述提到的改进的优化器一起使用来进行参数更新的。</p><p>而在计算上，CLR比上述提到的改进的优化器更容易实现，正如文献[1]所述：</p><p><em>Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. In addition, CLR policies are computationally simpler than adaptive learning rates. CLR is likely most similar to the SGDR method that appeared recently.</em></p><h3 id="Why-it-works"><a href="#Why-it-works" class="headerlink" title="Why it works"></a>Why it works</h3><p>直觉上看，随着训练进度的增加我们应该保持学习率一直减小以便于在某一时刻达到收敛。</p><p>然而，事实恰与直觉相反，使用一个在给定区间里周期性变化的LR可能更有用处。原因是周期性高的学习率能够使模型跳出在训练过程中遇到的局部最低点和马鞍点。事实上，Dauphin等[3]指出相比于局部最低点，马鞍点更加阻碍收敛。如果马鞍点正好发生在一个巧妙的平衡点，小的学习率通常不能产生足够大的梯度改变使其跳过该点（即使跳过，也需要花费很长时间）。这正是周期性高学习率的作用所在，它能够更快地跳过马鞍点。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/%E9%A9%AC%E9%9E%8D%E7%82%B9.gif" alt="image text"></p><p><em><center>Fig.: A saddle point in the error surface (Img Credit: safaribooksonline)</center></em></p><p>另外一个好处是，最优的LR肯定落在最小值和最大值之间。换言之，我们确实在迭代过程中使用了最好的LR。</p><h4 id="Epoch，iterations-cycles-and-stepsize"><a href="#Epoch，iterations-cycles-and-stepsize" class="headerlink" title="Epoch，iterations, cycles and stepsize"></a>Epoch，iterations, cycles and stepsize</h4><p>首先介绍几个术语，理解这些术语可以更好地理解下面描述的算法和公式。</p><p>我们现在考虑一个包含50000个样本的训练集。</p><p>一个epoch是至将整个训练集训练一轮。如果我们将batch_size, 我们在一个epoch里会得到500个batch或者叫iteration。iteration的数目随着epoch的增加不断积累，在第二个epoch，对应着501到1000次iteration，后面的以此类推。</p><p>一个cycle定义为学习率从低到高，然后从高到低走一轮所用的iteration数。而stepsize指的是cycle迭代步数的一半。注意，cycle不一定必须和epoch相同，但实践上通常将cycle和epoch对应相同的iteration。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-triangle.png" alt="image text"></p><p><em><center>Fig: Triangular LR policy. (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在上图中，两条红线分别表示学习率最小值（base lr）和学习率最大值（max lr）。蓝色的线是学习率随着iteration改变的方式。蓝线上下一次表示一个cycle，stepsize则是其一半。</p><h3 id="Calculating-the-LR"><a href="#Calculating-the-LR" class="headerlink" title="Calculating the LR"></a>Calculating the LR</h3><p>综上所述，接下来我们需要参数作为该算法的输入：</p><ul><li><p>stepsize</p></li><li><p>base_lr</p></li><li><p>max_lr</p></li></ul><p>下面是LR更新的一段代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_triangular_lr</span><span class="params">(iteration, stepsize, base_lr, max_lr)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given the inputs, calculates the lr that should be</span></span><br><span class="line"><span class="string">    applicable for this iteration</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cycle = np.floor(<span class="number">1</span> + iteration/(<span class="number">2</span>  * stepsize))</span><br><span class="line">    x = np.abs(iteration/stepsize - <span class="number">2</span> * cycle + <span class="number">1</span>)</span><br><span class="line">    lr = base_lr + (max_lr - base_lr) * np.maximum(<span class="number">0</span>, (<span class="number">1</span>-x))</span><br><span class="line">    <span class="keyword">return</span> lr</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># Demo of how the LR varies with iterations</span></span><br><span class="line">    num_iterations = <span class="number">10000</span></span><br><span class="line">    stepsize = <span class="number">1000</span></span><br><span class="line">    base_lr = <span class="number">0.0001</span></span><br><span class="line">    max_lr = <span class="number">0.001</span></span><br><span class="line">    lr_trend = list()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        lr = get_triangular_lr(iteration, stepsize, base_lr, max_lr)</span><br><span class="line">        <span class="comment"># Update your optimizer to use this learning rate in this iteration</span></span><br><span class="line">        lr_trend.append(lr)</span><br><span class="line">    </span><br><span class="line">    plt.plot(lr_trend)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如下图所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-graph.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration. We are using the triangular profile.</center></em></p><h3 id="Deriving-the-optimal-base-lr-and-max-lr"><a href="#Deriving-the-optimal-base-lr-and-max-lr" class="headerlink" title="Deriving the optimal base lr and max lr"></a>Deriving the optimal base lr and max lr</h3><p>对于给定的数据集，怎么确定合理的base lr 和max lr呢？</p><p>答案是先跑几个epoch，并且让学习率线性增加，观察准确率的变化，从中选出合适的base 和max lr。</p><p>我们让学习率按照上面的斜率进行增长，跑了几轮，结果如下图所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-deciding-baselr-maxlr.png" alt="image text"></p><p><em><center>Fig: Plot of accuracy vs learning rate (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>可以看出，开始的时候，准确率随着学习率的增加而增加，然后进入平缓起期，然后又开始减小，出现震荡。注意图中准确率开始增长的那一点和达到平衡的那一点（图中红色箭头所示）。这两个点可以作为比较好的base lr 和 max lr。当然，你也可以选择平衡点旁边的准确率峰值点作为max lr， 把base lr 设为其1/3 或者1/4。</p><p>好了，三个参数中已经有两个确定了，那么怎么确定stepsize呢？</p><p>已经有论文做过实验，他们将stepsize设成一个epoch包含的iteration数量的2-10倍。拿我们之前举的例子来说，我们一个epoch包含500个iteration，那么stepsize就设成1000-5000。该论文实验表明，stepsize设成2倍或者10倍，两者结果并没有太大的不同。</p><h3 id="Variants"><a href="#Variants" class="headerlink" title="Variants"></a>Variants</h3><p>上面我们实现的算法中，学习率是按照三角的规律周期性变化。除了这种以外，还有其他几种不同的函数形式。</p><p><strong><em>traiangular2：</em></strong>这里max lr 按cycle进行对半衰减。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-triangular2.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration for the triangular2 approach (Img Credit: Brad Kenstler)</center></em></p><p><strong><em>exp_range：</em></strong>这里max lr按iteration进行指数衰减。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-exp_range.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration for the exp-range approach (Img Credit: Brad Kenstler)</center></em></p><p>这些与固定学习率的指数衰减（exponential decay）相比，有论文表明效果都得到了明显的提升。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>如下图所示，在某神经网络上，CLR提供了一个快速的收敛，因此它的确值得一试。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-cifar10.png" alt="image text"></p><p><em><center>Fig. CLR tested on CIFAR 10 (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在上图的试验中，CLR花了25K次迭代达到了81%的准确率，传统的LR更新方法大约需要70K才能达到同样的水平。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-adam.png" alt="image text"></p><p><em><center>Fig. CLR used with Nesterov and Adam. Much faster convergence with Nesterov (Nesterov is an improvement over SGD) (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在另一项试验中，如上图所示，CLR + Nesterov优化器比著名的Adam收敛的还要快。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>CLR带来了一种新的方案来控制学习率的更新，它可以与SGD以及一些更加高级的优化器上一起使用。CLR应该成为每一个深度学习实践者工具箱里的一项技术。</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol><li><a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">Cyclical Learning Rates for Training Neural Networks, Smith</a></li><li><a href="https://arxiv.org/pdf/1609.04747.pdf" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms, Rudder</a></li><li>Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization.</li><li><a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">SGDR: Stochastic Gradient Descent with Warm Restarts, Loshchilov, Hutter</a></li><li><a href="https://github.com/bckenstler/CLR" target="_blank" rel="noopener">https://github.com/bckenstler/CLR</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文介绍神经网络训练中的周期性学习率技术。&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;学
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="超参数" scheme="http://state-of-art.top/tags/%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    
      <category term="学习率" scheme="http://state-of-art.top/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>图像数据-TFrecord在动态图中的使用</title>
    <link href="http://state-of-art.top/2018/10/27/TFrecord%E5%9C%A8%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://state-of-art.top/2018/10/27/TFrecord在动态图中的使用/</id>
    <published>2018-10-27T15:21:08.000Z</published>
    <updated>2018-10-28T08:50:46.251Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍图片数据使用TFrecord和tf.data.dataset进行存储和读取。</p><p>Tensorflow 提供了四种数据读取方式：</p><ol><li><p>Preloaded data: 用一个tf.constant常量将数据集加载进来，主要用于很小的数据集；</p></li><li><p>Feeding: 使用python代码供给数据，将所有数据加载进内存，然后一个batch一个batch地输入到计算图中， 适用于小数据集；</p></li><li>QueueRunner: 基于队列的输入通道，读取TFrecord静态图使用；</li><li>tf.data API: 能够从不同的输入或文件格式中读取、预处理数据，并且对数据应用一些变换（例如，batching、shuffling、mapping function over the dataset），tf.data API 是旧的 feeding、QueueRunner的升级。值得注意的是， Eager模式必须使用该API来构建输入通道， 一般结合TFrecord使用。该API相比于Queue更容易使用。</li></ol><h2 id="What‘s-TFrecord"><a href="#What‘s-TFrecord" class="headerlink" title="What‘s TFrecord"></a>What‘s TFrecord</h2><p>TFrecord是Tensorflow提供的一种二进制存储格式，可将数据和标签统一存储。从上述读取方式中可以看出，TFrecord在QueueRunner和tf.data API读取中均扮演了重要的角色。</p><h2 id="Why-TFrecord"><a href="#Why-TFrecord" class="headerlink" title="Why TFrecord"></a>Why TFrecord</h2><p>与其他方案相比， 使用TFrecord读取的优点在于：</p><ol><li>可处理大规模数据量，而不会造成其他方案所带来的内存不够用的问题；</li><li>在Feeding方案中，batch读取的IO操作势必会阻塞训练，前一个batch加载完成后，神经网络必须等待下一个batch加载完成后才能继续训练，效率较低。</li></ol><h2 id="How-To-Use"><a href="#How-To-Use" class="headerlink" title="How To Use"></a>How To Use</h2><p>TFrecord的使用主要有两块：一是图片数据转TFrecord格式存储，二是解析存储好的TFrecord文件。下面逐一介绍。</p><h3 id="图片转TFrecord"><a href="#图片转TFrecord" class="headerlink" title="图片转TFrecord"></a>图片转TFrecord</h3><p>本文使用的数据集是Kaggle猫狗数据集。</p><p>该数据集包含train和test两个文件夹， 分别为训练集和测试集，下面以train集为例操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls |wc -w</span><br><span class="line"></span><br><span class="line">25000</span><br></pre></td></tr></table></figure><p>训练集包含25000张图片，猫狗各一半。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls </span><br><span class="line"></span><br><span class="line">cat.124.jpg    cat.3750.jpg  cat.6250.jpg  cat.8751.jpg  dog.11250.jpg  dog.2500.jpg   dog.5000.jpg  dog.7501.jpg</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>图片文件以jpg格式存储，以cat， dog作为文件名开头。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_tfrecord_encode</span><span class="params">(classes, tfrecord_filename, data_path, is_training=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    功能：读取图片转换成tfrecord格式的文件</span></span><br><span class="line"><span class="string">    @params: classes: 标签类别  @type：classes: dict</span></span><br><span class="line"><span class="string">    @params: tfrecord_filename: tfrecord文件保存文件</span></span><br><span class="line"><span class="string">    @type：tfrecord_filename: str</span></span><br><span class="line"><span class="string">    @params: data_path: 原始训练集存储路径</span></span><br><span class="line"><span class="string">    @is_training: 是否为训练集，用来区分训练集和测试集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 初始化一个writer</span></span><br><span class="line">    writer = tf.python_io.TFRecordWriter(tfrecord_filename)</span><br><span class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> tqdm(os.listdir(path)):</span><br><span class="line">        name = img_name.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 使用tf.gfile.FastFile读取图片要比PIL.Image读取处理得到的</span></span><br><span class="line">        <span class="comment"># 最终TFrecod文件小得多，在本案例中，IMAGE方式读取得到的TFrecord大小约为3.7G</span></span><br><span class="line">        <span class="comment"># 而tf.gfile.FastFile得到的约为548M</span></span><br><span class="line">        <span class="keyword">with</span> tf.gfile.FastGFile(os.path.join(path, img_name), <span class="string">'rb'</span>) <span class="keyword">as</span> gf:</span><br><span class="line">            img = gf.read()</span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            <span class="comment"># 构造特征</span></span><br><span class="line">            feature = &#123;</span><br><span class="line">                <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[classes[name]])),</span><br><span class="line">                <span class="string">'img_raw'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),</span><br><span class="line">                <span class="string">'file_name'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()]))</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature = &#123;</span><br><span class="line">                <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[<span class="number">-1</span>])),</span><br><span class="line">                <span class="string">'img_raw'</span>:tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),</span><br><span class="line">                <span class="string">'file_name'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()]))</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="comment"># example 对象将label和image特征进行封装</span></span><br><span class="line">        example = tf.train.Example(features=tf.train.Features(feature=feature))  </span><br><span class="line">        writer.write(example.SerializeToString())   <span class="comment"># 序列化为字符串</span></span><br><span class="line">    writer.close()</span><br><span class="line">    print(<span class="string">'tfrecord writen done!'</span>)</span><br></pre></td></tr></table></figure><p>调用上述函数，可得到猫狗训练集的TFrecord格式文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    classes = &#123;<span class="string">'cat'</span>: <span class="number">0</span>, <span class="string">'dog'</span>: <span class="number">1</span>&#125;</span><br><span class="line">    tfrecord_filename = <span class="string">'cat_and_dog.tfrecord'</span></span><br><span class="line">    data_path = <span class="string">'train/'</span></span><br><span class="line">    img_tfrecord_encode(classes, tfrecord_filename, data_path, is_training=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>上述程序运行大约需要2min。</p><h3 id="使用tf-data读取TFrecord"><a href="#使用tf-data读取TFrecord" class="headerlink" title="使用tf.data读取TFrecord"></a>使用tf.data读取TFrecord</h3><p>在动态图（eager）模式下，QueueRunner不可用，必须使用tf.data进行TFrecord的读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_tfrecord_parse</span><span class="params">(tfrecord_filename, epochs, batch_size, shape,</span></span></span><br><span class="line"><span class="function"><span class="params">                       padded_shapes=None, shuffle=True, buffer_size=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param: tfrecord_filename:tfrecord文件列表   @type:list</span></span><br><span class="line"><span class="string">    @param: epoch:训练轮数（repeating次数）       @type:int</span></span><br><span class="line"><span class="string">    @param：batch_size:批数据大小                @type:int</span></span><br><span class="line"><span class="string">    @param: shape:图片维度                      @type:tuple</span></span><br><span class="line"><span class="string">    @param: padded_shapes:不定长padding        @type:tuple</span></span><br><span class="line"><span class="string">    @param: shuffle:是否打乱                   @type:boolean</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"><span class="comment"># 解析单个example，特征与encode一一对应。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_example</span><span class="params">(serialized_example)</span>:</span></span><br><span class="line">        features = tf.parse_single_example(serialized_example,</span><br><span class="line">                                           features=&#123;</span><br><span class="line">                                               <span class="string">'label'</span>: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">                                               <span class="string">'img_raw'</span>: tf.FixedLenFeature([], tf.string),</span><br><span class="line">                                               <span class="string">'file_name'</span>: tf.FixedLenFeature([], tf.string)</span><br><span class="line">                                           &#125;)</span><br><span class="line">        <span class="comment"># 解码</span></span><br><span class="line">        image = tf.image.decode_jpeg(features[<span class="string">'img_raw'</span>])</span><br><span class="line">        <span class="comment"># 设置shape</span></span><br><span class="line">        image = tf.image.resize_images(image, shape, method=<span class="number">1</span>)</span><br><span class="line">        label = tf.cast(features[<span class="string">'label'</span>], tf.int64)</span><br><span class="line">        file_name = tf.cast(features[<span class="string">'file_name'</span>], tf.string)</span><br><span class="line">        <span class="keyword">return</span> image, label, file_name</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 解析TFrecord</span></span><br><span class="line">    dataset = tf.data.TFRecordDataset(tfrecord_filename).map(parse_example)</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="keyword">if</span> padded_shapes:</span><br><span class="line">            dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).padded_batch(batch_size, padded_shapes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).batch(batch_size)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> padded_shapes:</span><br><span class="line">            dataset = dataset.repeat(epochs).padded_batch(batch_size, padded_shapes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = dataset.repeat(epochs).batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><p>调用上述函数，解析TFrecord得到dataset。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>():</span><br><span class="line">    tfrecord_filename = <span class="string">'cat_and_dog.tfrecord'</span></span><br><span class="line">    epochs = <span class="number">100</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    shape = (<span class="number">227</span>, <span class="number">227</span>)</span><br><span class="line">    dataset = img_tfrecord_parse(tfrecord_filename=tfrecord_filename,</span><br><span class="line">                                epochs=epochs,</span><br><span class="line">                                batch_size=batch_size,</span><br><span class="line">                                shape=shape)</span><br><span class="line">    <span class="comment"># 查看dataset</span></span><br><span class="line">    iterator = dataset.make_one_hot_iterator()</span><br><span class="line">    image, label, file_name = iterator.get_next()</span><br><span class="line">    print(image[<span class="number">0</span>])</span><br><span class="line">    print(label[<span class="number">0</span>])</span><br><span class="line">    print(file_name[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文介绍图片数据使用TFrecord和tf.data.dataset进行存储和读取。&lt;/p&gt;
&lt;p&gt;Tensorflow 提供了四种数据读取方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Preloaded data: 用一个tf.constant常量将数据集加载进来，主要用于很小
      
    
    </summary>
    
      <category term="Tensorflow" scheme="http://state-of-art.top/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://state-of-art.top/tags/Tensorflow/"/>
    
  </entry>
  
</feed>
