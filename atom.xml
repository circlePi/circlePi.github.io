<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>circlepi&#39;s blog</title>
  
  <subtitle>Even with an intractable probelm, one can find a way to do the right thing.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://state-of-art.top/"/>
  <updated>2019-02-03T14:02:04.510Z</updated>
  <id>http://state-of-art.top/</id>
  
  <author>
    <name>circlepi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Configargparser Usage</title>
    <link href="http://state-of-art.top/2019/02/03/Configargparser%20usage/"/>
    <id>http://state-of-art.top/2019/02/03/Configargparser usage/</id>
    <published>2019-02-03T15:30:08.000Z</published>
    <updated>2019-02-03T14:02:04.510Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-Use-Configargparser"><a href="#How-to-Use-Configargparser" class="headerlink" title="How to Use Configargparser"></a>How to Use Configargparser</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this blog, I want to show you somethings about how to use the “Configargparser” module to realize some basic config for your procedures. Configargparser is a more stronger module, but the relevant document is very few.</p><p>There are three main ways that programs get data from their environment when they start up:</p><ul><li>Commandline arguments,</li><li>Configuration files</li><li>Environment variables</li></ul><p>Currently, if a python program wants to parse data from these sources, it needs to first check the environment variables, then check for a configuration file and finally look at the commandline arguments. The general cascade is that commandline arguments should override config files which should override environment variables. Unfortunately, checking all of these places is a major pain – which is what configargparser is meant to address. </p><p>It’s known that Python has provided two modules, argparser for commandline arguments parsing and configparser for configuration files.  While Configparser can address those in a single module.   </p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><p>If Internet is available, you can install Configargparse with:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install configargparser</span><br></pre></td></tr></table></figure><p>If not, you can download the configargparser package at:</p><p><a href="https://pypi.org/project/ConfigArgParse/" target="_blank" rel="noopener">https://pypi.org/project/ConfigArgParse/</a></p><p>and then install from source by:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><p>By the way, the repo on Github is also available at:</p><p><a href="https://github.com/mgilson/configargparser" target="_blank" rel="noopener">https://github.com/mgilson/configargparser</a></p><h2 id="Basic-Usage"><a href="#Basic-Usage" class="headerlink" title="Basic Usage"></a>Basic Usage</h2><p>If you are similar with argparse, you can skip this section, because configargparser use the same way as argparser.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> configargparse</span><br><span class="line"></span><br><span class="line">p = configargparse.ArgumentParser()</span><br><span class="line">p.add(<span class="string">'-v'</span>,help=<span class="string">'verbose'</span>,action=<span class="string">'store_true'</span>)</span><br><span class="line">p.add(<span class="string">'-s'</span>,<span class="string">'--size'</span>, type=int, choices=[<span class="number">5</span>, <span class="number">10</span>])</span><br><span class="line">opt = p.parse_args()</span><br><span class="line">print(opt)</span><br><span class="line">print(<span class="string">'size is %d'</span>%opt.size)</span><br></pre></td></tr></table></figure><p>Run the file by:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py -v -s <span class="number">10</span></span><br></pre></td></tr></table></figure><p>You can see:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Namespace(size=<span class="number">10</span>, v=<span class="keyword">True</span>)</span><br><span class="line">size <span class="keyword">is</span> <span class="number">10</span></span><br></pre></td></tr></table></figure><h2 id="From-Config-File"><a href="#From-Config-File" class="headerlink" title="From Config File"></a>From Config File</h2><p>We can also load the config parameters from config file, a <em>.ini file or </em>.yml file. For example, we first prepare a test.yml file with a key: value format:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data: /home/daizelin</span><br><span class="line">size: <span class="number">10</span></span><br><span class="line">embedding_dim: <span class="number">512</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> configargparse</span><br><span class="line"></span><br><span class="line">p = configargparse.ArgumentParser(default_config_files=[<span class="string">'/home/zelin/parser_test/test.yml'</span>],config_file_parser_class=configargparse.YAMLConfigFileParser)</span><br><span class="line"><span class="comment"># You can also specify the config file path with a -c parameter if the following code works.</span></span><br><span class="line"><span class="comment">## p.add('-c','--custom-config', required=True, is_config_file=True,help='config_file_path')</span></span><br><span class="line">p.add_argument(<span class="string">'-data'</span>,<span class="string">'--data'</span>, help=<span class="string">'data'</span>)</span><br><span class="line">p.add_argument(<span class="string">'-size'</span>,<span class="string">'--size'</span>, type=int, help=<span class="string">'size'</span>)</span><br><span class="line">p.add_argument(<span class="string">'-embedding_dim'</span>,<span class="string">'--embedding_dim'</span>, type=int,help=<span class="string">'embedding_dim'</span>)</span><br><span class="line">opt = p.parse_args()</span><br><span class="line">print(opt)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">68</span>)</span><br><span class="line">print(p.format_help())</span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">68</span>)</span><br><span class="line">print(p.format_values())</span><br></pre></td></tr></table></figure><p>Run the file by:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py</span><br></pre></td></tr></table></figure><p>You can see:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Namespace(data=<span class="string">'/home/daizelin'</span>, embedding_dim=<span class="number">512</span>, size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>Run the file by:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py -embedding_dim <span class="number">128</span></span><br></pre></td></tr></table></figure><p>You can see:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Namespace(data=<span class="string">'/home/daizelin'</span>, embedding_dim=<span class="number">128</span>, size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>This means that the command line arguments will overwrite the config file arguments.</p><p>Some tips:</p><ul><li><p>The parameters in the yml file adapt the format as key: value. Note the whitespace after :</p></li><li><p>When reading from a config file, you need also add the arguments to the parser by:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.add_argument(<span class="string">'-data'</span>,<span class="string">'--data'</span>, help=<span class="string">'data'</span>)</span><br></pre></td></tr></table></figure></li></ul><h2 id="Group"><a href="#Group" class="headerlink" title="Group"></a>Group</h2><p>For clarity, we usually use argument group for different codes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> configargparse</span><br><span class="line"></span><br><span class="line">p = configargparse.ArgumentParser()</span><br><span class="line">group_1 = p.add_argument_group(<span class="string">"Model"</span>)</span><br><span class="line">group_1.add(<span class="string">'--batch_size'</span>, type=int, default=<span class="number">64</span>)</span><br><span class="line">group_1.add(<span class="string">'--learning_rate'</span>, type=float, default=<span class="number">0.001</span>)</span><br><span class="line">group_2 = p.add_argument_group(<span class="string">"Preprocessing"</span>)</span><br><span class="line">group_2.add(<span class="string">'--max_len'</span>, type=int, default=<span class="number">50</span>)</span><br><span class="line">opt = p.parse_args()</span><br><span class="line">print(opt)</span><br></pre></td></tr></table></figure><p>Run the file by:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python group.py</span><br></pre></td></tr></table></figure><p>You will see:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Namespace(batch_size=<span class="number">64</span>, learning_rate=<span class="number">0.001</span>, max_len=<span class="number">50</span>)</span><br><span class="line">----------------------------------------------</span><br><span class="line">usage: group.py [-h][--batch_size BATCH_SIZE] [--learning_rate LEARNING_RATE][--max_len MAX_LEN]</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message <span class="keyword">and</span> exit</span><br><span class="line"></span><br><span class="line">Model:</span><br><span class="line">  --batch_size BATCH_SIZE</span><br><span class="line">  --learning_rate LEARNING_RATE</span><br><span class="line"></span><br><span class="line">Preprocessing:</span><br><span class="line">  --max_len MAX_LEN</span><br><span class="line"></span><br><span class="line">-----------------------------------------------</span><br><span class="line"></span><br><span class="line">Defaults:</span><br><span class="line">  --batch_size:      <span class="number">64</span></span><br><span class="line">  --learning_rate:   <span class="number">0.001</span></span><br><span class="line">  --max_len:         <span class="number">50</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;How-to-Use-Configargparser&quot;&gt;&lt;a href=&quot;#How-to-Use-Configargparser&quot; class=&quot;headerlink&quot; title=&quot;How to Use Configargparser&quot;&gt;&lt;/a&gt;How to U
      
    
    </summary>
    
      <category term="Python" scheme="http://state-of-art.top/categories/Python/"/>
    
      <category term="Configargparser" scheme="http://state-of-art.top/categories/Python/Configargparser/"/>
    
    
      <category term="configargParser" scheme="http://state-of-art.top/tags/configargParser/"/>
    
  </entry>
  
  <entry>
    <title>Transformer原理和实现 从入门到精通</title>
    <link href="http://state-of-art.top/2019/01/17/Transformer%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/"/>
    <id>http://state-of-art.top/2019/01/17/Transformer原理和实现-从入门到精通/</id>
    <published>2019-01-17T15:20:08.000Z</published>
    <updated>2019-01-17T12:48:10.451Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer原理和实现-从入门到精通"><a href="#Transformer原理和实现-从入门到精通" class="headerlink" title="Transformer原理和实现 从入门到精通"></a>Transformer原理和实现 从入门到精通</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>所谓 ”工预善其事，必先利其器“， Bert之所以取得这么惊才绝艳的效果，很大一部分原因源自于Transformer。为了后面更好、更快地理解BERT模型，这一节从Transformer的开山鼻祖说起，先来跟着”Attention is All You Need[1]“ 这篇文章，走近transformer的世界，在这里你再也看不到熟悉的CNN、RNN的影子，取而代之的是，你将看到Attention机制是如何被发挥的淋漓尽致、妙至毫颠，以及它何以从一个为CNN、RNN跑龙套的配角实现华丽逆袭。对于Bert来说，transformer真可谓天纵神兵，出匣自鸣！</p><p>看完本文，你大概能够：</p><ul><li>掌握Encoder-Decoder框架</li><li>掌握残差网络</li><li>掌握BatchNormalization(批归一化)和LayerNormalization（层归一化）</li><li>掌握Position Embedding（位置编码）</li></ul><p>当然，最重要的，你能了解Transformer的原理和代码实现。</p><p>Notes: 本文代码参考哈弗大学的The Annotated Transformer</p><h2 id="Encoder-Decoder框架"><a href="#Encoder-Decoder框架" class="headerlink" title="Encoder-Decoder框架"></a>Encoder-Decoder框架</h2><p>Encoder-Decoder是为seq2seq（序列到序列）量身打造的一个深度学习框架，在机器翻译、机器问答等领域有着广泛的应用。这是一个抽象的框架，由两个组件：Encoder（编码器）和Decoder(解码器)组成。对于给定的输入source（x1, x2, x3, …,xn）, 首先编码器将其编码成一个中间表示向量z=（z1, z2, …, zn）。接着，解码器根据z和解码器自身前面的输出，来生成下一个单词（如<strong>Figure 1</strong>所示）。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/1.PNG" alt=""></p><center>Figure 1</center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure><p>上述代码呈现了一个标准的Encoder-Decoder框架。在实际应用中，编码器和解码器可以有多种组合，比如{RNN， RNN}、{CNN，RNN}等等，这就是传统的seq2seq框架。后来引入了attention机制，上述框架也被称为”分心模型“。为什么说他”分心“呢？因为对于解码器来说，他在生成每一个单词的时候，中间向量的每一个元素对当前生成词的贡献都是一样的。Attention的思想则是对于当前生成的单词，中间向量z的每个元素对其贡献的重要程度不同，跟其强相关的赋予更大的权重，无关的则给一个很小的权重。</p><p>举个栗子：假如我们要将 “knowledge is power” 翻译成 中文，在翻译”knowledge“这个单词时， 显然”knowledge“这个单词对翻译出来的”知识“贡献最大，其他两个单词贡献就很小了。这实际上让模型有个区分度，不会被无关的东西干扰到，翻译出来的准确度当然也就更高了。</p><p>在这里Attention其实还是一个小弟，主角仍然是RNN、CNN这些大佬。</p><p>我们不妨先顺着这个思路往下想，attention在这里充当了Encoder和Decoder的一个桥梁，事实证明有很好的效果。既然效果这么好，那在Encoder中是不是也可以用呢？文本自身对自身的编码进行有区分度的表示，事实上，这在以往的很多文本分类的工作中已被采用[2]。这看上去已经是个值得尝试的good idea了。继续开脑洞，Encoder都用了，Decoder能落后吗，好歹人家是一对CP，当然要妇唱夫随了。于是，Encoder和Decoder都用了自注意力（self-attention）。回想一下，到这里我们已经在三个地方用到了注意力机制了。这时候RNN大佬不愿意了，原本我的名声地盘都被你们分走了，散伙！Attention反正是初生牛犊不怕虎，说好，分分账分道扬镳吧，反正你的序列计算并行不起来一直让人诟病，没你我可能更潇洒。于是两兄弟就分开了。相见时难别亦难，RNN老大哥深谋远虑，临走时不忘嘱咐一句”苟富贵，勿相忘！“。于是一个故事的结束就成了另一个故事的开始，注意力就此开启创业之路，寒来暑往，春去秋来，在黑暗中不断寻找光亮，学习PPT技巧…终于有一天，它的PPT做完了，找到了融资，破茧成蝶，横空出道，并给自己取了个亮闪闪的名字：<strong>Transformer</strong>， 自此，一个新的时代开始了…</p><hr><p>呃， 醒醒…黄粱熟了…</p><hr><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>这部分我们来看看Transformer的架构。如<strong>Figure 2</strong> 所示， Transformer遵循了Encoder-Decoder的架构。在Encoder方面，6个编码器组件协同工作，组成一个大的编码器，解码器同样由6个解码器组件组成。我们先看Encoder。6个编码器组件依次排列，每个组件内部都是由一个多头attention加上一个前馈网络，attenion和前馈的输出都经过层归一化（LayerNormalization），并且都有各自的残差网络 。Decoder呢，组件的配置基本相同， 不同的是Decoder有两个多头attention机制，一个是其自身的mask自注意力机制，另一个则是从Encoder到Decoder的注意力机制，而且是Decoder内部先做一次attention后再接收Encoder的输出。说完了Encoder和Decoder，再说说输入，模型的输入部分由词向量（embedding）经位置编码（positional Encoding）后输入到Encoder和Decoder。编码器的输出由一个线性层和softmax组成，将浮点数映射成具体的符号输出。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/2.PNG" alt=""></p><center>Figure 2</center><p>那么，下面我们将结合原理和代码来逐一了解这些部分。</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>我们先来看下Encoder的实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>以上便是Encoder的核心实现。它由N个encoderLayer组成。输入一次通过每个encoderLayer，然后经过一个归一化层。下面来看下encoderLayer和LayerNorm是什么样子。</p><h3 id="EncoderLayer和残差网络"><a href="#EncoderLayer和残差网络" class="headerlink" title="EncoderLayer和残差网络"></a>EncoderLayer和残差网络</h3><p>EncoderLayer如<strong>Figure 3</strong>所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/3.png" alt=""></p><center>Figure 3</center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><p>这里的代码初看上去有点绕，不过没关系，听我娓娓道来。我们先看什么是残差网络（即代码中的SublayerConnection），见<strong>Figure 4</strong> 。其实非常简单，就是在正常的前向传播基础上开一个绿色通道，这个通道里x可以无损通过。这样做的好处不言而喻，避免了梯度消失（求导时多了一个常数项）。最终的输出结果就等于绿色通道里的x加上sublayer层的前向传播结果。注意，这里输入进来的时候做了个norm归一化，关于norm我们后面再说。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/4.png" alt=""></p><center>Figure 4</center><p>理解了残差网络，EncoderLayer的代码就很好看懂了。sublayer有两个，一个是多头self-attention层，另一个是前馈网络（feed_forward）。输入x先进入多头self-attention，用一个残差网络加成，接着通过前馈网络， 再用一个残差网络加成。</p><p>让我们从输入x开始，再从头理一遍这个过程：</p><ul><li>输入x</li><li>x做一个层归一化： x1 = norm(x)</li><li>进入多头self-attention: x2 = self_attn(x1)</li><li>残差加成：x3 = x + x2</li><li>再做个层归一化：x4 = norm(x3)</li><li>经过前馈网络: x5 = feed_forward(x4)</li><li>残差加成: x6 = x3 + x5</li><li>输出x6</li></ul><p>以上就是一个Encoder组件所做的全部工作了。里面有两点暂未说明，一个是多头attention， 另一个是层归一化。喝杯茶之后精彩继续…</p><h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><p>多头注意力机制在敝人之前的博客已经做了详尽的原理和代码解析（<a href="https://state-of-art.top/2019/01/06/BERT%E8%A7%A3%E8%AF%BB(%E4%B8%80">请戳</a>self-Attention/#more))，这里不再赘述，仅贴一下代码和注释，这里使用的是点乘attention，而不是加性（additive）attention。但是再提一点，在encoder和decoder的自注意力中，attention层的输入分为self_attn(x, x, x, mask)和self_attn(t, t, t, mask)， 这里的x和t分别为source和target输入。后面会看到，从encoder到decoder层的注意力输入时attn(t, m, m), 这里的m是Encoder的输出。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"""因子化的点乘Attention-矩阵形式</span></span><br><span class="line"><span class="string">    Query： 查询 （batch_size, heads, max_seq_len, d_k）</span></span><br><span class="line"><span class="string">    Key: 键 (batch_size, heads, max_seq_len_d_k)</span></span><br><span class="line"><span class="string">    Value: 值 （batch_size, heads, max_seq_len, d_v）</span></span><br><span class="line"><span class="string">    d_v = d_k</span></span><br><span class="line"><span class="string">    Q=K=V</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># (batch_size, heads, max_seq_len, d_k) * (batch_size, heads, d_k, max_seq_len)</span></span><br><span class="line">    <span class="comment">#  = (batch_size, heads, max_seq_len, max_seq_len)</span></span><br><span class="line">    <span class="comment"># 为了方便说明，只看矩阵的后两维 (max_seq_len, max_seq_len), 即</span></span><br><span class="line">    <span class="comment">#       How  are  you</span></span><br><span class="line">    <span class="comment"># How [[0.8, 0.2, 0.3]</span></span><br><span class="line">    <span class="comment"># are  [0.2, 0.9, 0.6]</span></span><br><span class="line">    <span class="comment"># you  [0.3, 0.6, 0.8]]</span></span><br><span class="line">    <span class="comment"># 矩阵中每个元素的含义是，他对其他单词的贡献（分数）</span></span><br><span class="line">    <span class="comment"># 例如，如果我们想得到所有单词对单词“How”的打分，取矩阵第一列[0.8, 0.2, 0.3], 然后做softmax</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="comment"># 对于padding部分，赋予一个极大的负数，softmax后该项的分数就接近0了，表示贡献很小</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="comment"># 接着与Value做矩阵乘法:</span></span><br><span class="line">    <span class="comment"># (batch_size, heads, max_seq_len, max_seq_len) * (batch_size, heads, max_seq_len, d_k)</span></span><br><span class="line">    <span class="comment"># = (batch_size, heads, max_seq_len, d_k)</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span>, <span class="string">"heads is not a multiple of the number of the in_features"</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="keyword">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        这里的query, key, value与attention函数中的含义有所不同，这里指的是原始的输入.</span></span><br><span class="line"><span class="string">        对于Encoder的自注意力来说，输入query=key=value=x</span></span><br><span class="line"><span class="string">        对于Decoder的自注意力来说，输入query=key=value=t</span></span><br><span class="line"><span class="string">        对于Encoder和Decoder之间的注意力来说， 输入query=t， key=value=m</span></span><br><span class="line"><span class="string">        其中m为Encoder的输出，即给定target，通过key计算出m中每个输出对当前target的分数，在乘上m</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        <span class="comment">##   x: (batch_size, heads, max_seq_len, d_k)</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask,</span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment">##   x: (batch_size, max_seq_len, d_k*h)</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">            .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment">## output: (batch_size, max_seq_len, d_model)</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure><p>注意下attention当中的mask。我们之前提到，在三个地方用到了attention。在Encoder的自注意力机制中，mask是用来过滤padding部分的作用，对于source中的每一个词来讲，其他的词对他都是可见的，都可以做出贡献的。但是在Decoder中，mask的作用就有所不同了。这可能又要从Encoder-Decoder框架说起。在这个框架下，解码器实际上可看成一个神经网络语言模型，预测的时候，target中的每一个单词是逐个生成的，当前词的生成依赖两方面：一是Encoder的输出，二是target的前面的单词。例如，在生成第一个单词是，不仅依赖于Encoder的输出，还依赖于起始标志[CLS]；生成第二个单词是，不仅依赖Encoder的输出，还依赖起始标志和第一个单词…依此类推。这其实是说，在翻译当前词的时候，是看不到后面的要翻译的词。由上可以看出，这里的mask是动态的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    <span class="comment"># size: 序列长度</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    <span class="comment"># 生成一个上三角矩阵</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure><p>下面详细介绍下subsequent_mask是如何起作用的。函数的参数size指的是target句子的长度。以”[CLS] That is it“这个长度为4的target输入为例，这个函数的输出是什么呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(subsequent_mask(size=<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure><p>可以看到，输出为一个下三角矩阵，维度为（1,4,4）。现在我们再来看下attention函数，mask起作用的地方是在Query和Key点乘后，结果矩阵的维度为（batch_size, heads, max_seq_len, max_seq_len）。为方便起见，我们只看一条数据，即batch_size=1。进入多头attention时，注意到对mask做了一步操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">mask：</span><br><span class="line">tensor([[[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure><p>这时mask的维度变成了（1,1,4,4）.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">target:      </span><br><span class="line">         CLS  That <span class="keyword">is</span>   it</span><br><span class="line"> CLS [[[[<span class="number">0.8</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]</span><br><span class="line"> That   [<span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.6</span>, <span class="number">0.4</span>]</span><br><span class="line"> <span class="keyword">is</span>     [<span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.7</span>]</span><br><span class="line"> it     [<span class="number">1.2</span>, <span class="number">0.6</span>, <span class="number">2.1</span>, <span class="number">3.2</span>]]]]</span><br><span class="line"> </span><br><span class="line">mask：</span><br><span class="line">       [[[[<span class="number">1</span>,  <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>,  <span class="number">1</span>,   <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>,  <span class="number">1</span>,   <span class="number">1</span>,  <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>,  <span class="number">1</span>,   <span class="number">1</span>,  <span class="number">1</span>]]]]</span><br></pre></td></tr></table></figure><p>写成了上面的样子，mask的作用就很显然了。例如，对于”CLS“来说，预测它下一个词时，只有”CLS“参与了attention，其他的词（相对于CLS为未来的词）都被mask_fill掉了，不起作用。后面的情况依此类推。</p><p>细心的小伙伴可能发现了，这里的解释并没有考虑padding部分。事实上，就算加了padding部分（为0），也不影响上述过程，有兴趣的话可以在上面it后面加上个0，下面的矩阵加一列[0 0 0 0 ]， 就可以一目了然。</p><h3 id="层归一化"><a href="#层归一化" class="headerlink" title="层归一化"></a>层归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><p>在前面多次用到了层归一化（LayerNormalization），那么它是何方神圣呢？或许你对BatchNormalization比较熟悉，但千万不要在这里错以为是它。可以说层归一化是BatchNormalization的2.0版本，它是由Hinton神和他的学生提出的[3]。</p><h4 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h4><p>BatchNormalization的出现无疑是广大AI调参侠的福音，将大家从繁琐的权重初始化、学习率调节中释放出来。它不仅能够大大加快收敛速度，还自带正则化功能，是Google 2015年提出的[4]。</p><p>机器学习的一个重要的假设是：<strong>数据是独立同分布的</strong>。训练集合测试集的数据是同分布的，这样模型才有好的泛化效果。神经网络其实也是在学习这个分布。在这个假设前提下，一旦我们知道了（x，y）的联合分布，很多问题就能通过条件概率P(x|y)计算出来了。但是在实际训练过程中，数据经过前一个隐藏层向后一个隐藏层传播（线性+非线性运算），分布通常会发生变化（作者称之为<strong>Internal Covariate Shift</strong>），这会导致网络学习变慢。我们从两个方面来稍微理解一下这个问题。</p><p>一方面：我们现在只看两个隐藏层（隐藏层A和隐藏层B）之间的传播。第一轮，来自A的数据经过线性操作和激活函数后到达B，反向传播时，B层为了学习到这个分布（满足A的需求），调整了权重W1。接着又进行第二轮传播了，A数据一到B，A说，现在需求变了，要这样这样。B一脸懵，盘算了一下，发现前面的白学了，没办法，换个方向重来…就这样，A一直在变，B就得跟着变，来来回回磨合…这听起来就是个非常耗时的工作。就好比A说今天要吃汤圆，B和好了面粉，准备了调料，A又说我要吃饭…虽然在B的不懈努力下A最后能吃上饭，但如果一开始A就告诉B我要吃饭不是更快一点？…网络越深，这个问题就越严重。</p><p>另一方面则是与激活函数有关，我们用sigmoid为例来说明一下。假设两层传播之间可表示为<br>$$<br>z = g（Wu+b）<br>$$<br>其中g是sigmoid函数，我们令<br>$$<br>x=Wu+b<br>$$<br>那么：<br>$$<br>z = sigmoid(x) = g(x) = \frac{1}{1+exp(-x)}<br>$$<br>计算下梯度：<br>$$<br>\frac{\partial z}{\partial W} = \frac{\partial z}{\partial g}\cdot\frac{\partial g}{\partial x}\cdot\frac{\partial x}{\partial u}<br>$$<br>我们关注一下中间那一项，是sigmoid函数的导数，它的分布如<strong>Figure 5</strong> 所示，可见随着|x|不断增大，该项趋近于0，这也就意味着整个梯度趋近于0，进入饱和区了，导致的结果就是收敛变慢！要想加快收敛怎么办，把|x|拉到靠近0的位置就行了，这里导数值最大。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/5.png" alt=""></p><center>Figure 5</center><p>BatchNormalization就是解决这两个问题的。首先，它将隐藏层的输入强行变换为同一分布（解决了第一个问题），这个分布就是正态分布（解决了第二个问题）。</p><p>它的具体做法如<strong>Figure 6</strong>所示。对每一个Mini-Batch的<strong>所有样本</strong>的<strong>每一维特征</strong>，计算两个统计量：均值和方差，然后做一个归一化操作，这样就变成了正态分布了。但是只这样做也有问题，首先，谁说数据一定是正态分布的，偏正态不行吗？第二，把数据全部拉到接近0的位置，sigmoid不就接近于一个线性函数了吗，没有起到激活的作用啊（线性激活函数+线性操作等价于一层线性操作）。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/6.png" alt=""></p><center>Figure 6</center><p>为了解决这两个问题，作者又做了一步操作，引入了两个参数gamma和beta（图中的最后一步）, 这两个参数是在训练过程中学习的！相当于将这个正态分布左右挪了挪，变胖或者变瘦，在加快收敛速度和保持非线性的之间找到一个平衡。当然这是臆测的，作者并没有明确这么说。如果你非要问为什么，那我只能告诉你深度学习是个实验领先于理论的学科。</p><p>上面说了一下训练过程。那么预测的时候呢？假如只预测一个样本，一个样本的均值…方差…怎么算?没意义是吧。事实上，预测的时候用的是全局的均值和方差，这个全局的均值和方差是怎么得到的呢？很简单，训练过程中记录下每个Mini-Batch的均值和方差，求个期望就是全局的均值和方差了。</p><h4 id="LayerNormalization"><a href="#LayerNormalization" class="headerlink" title="LayerNormalization"></a>LayerNormalization</h4><p>BatchNormalization简直是个救世主啊，它令调参工作变得从未如此容易，让调参侠们不费吹灰之力，谈笑间到达收敛的彼岸。但毛主席曾经说过，万物都是辩证的，它同样存在两个问题：</p><ul><li>对batch_size非常敏感。BatchNormalization的一个重要出发点是保持每层输入的数据同分布。回想下开始那个独立同分布的假设。假如取的batch_size很小，那显然有些Mini-Batch的数据分布就很可能与整个数据集的分布不一致了，又出现了那个问题，数据分布不一致…这就等于说没起到同分布的作用了，或者说同分布得不充分。实验也证明，batch_size取得大一点， 数据shuffle的好一点，BatchNormalization的效果就越好。</li><li>不能很方便地用于RNN。这其实是第一个问题的引申。我们再来看一下<strong>Figure 6</strong>中的均值和方差的计算公式。对所有样本求均值。对于图片这类等长的输入来说，这很容易操作，在每个维度加加除除就可以了，因为维度总是一致的。而对于不等长的文本来说，RNN中的每个time step共享了同一组权重。在应用BatchNormalization时，这就要求对每个time step的batch_size个输入计算一个均值和方差。那么问题就来了，假如有一个句子S非常长，那就意味着对S而言，总会有个time_step的batch_size为1，均值方差没意义，这就导致了BatchNormalization在RNN上无用武之地了。</li></ul><p>为了避免这两个问题，LayerNormalization就应运而生了。</p><p>LayerNormalization的主要变化在于：</p><ul><li><p>不再对Mini-Batch中的N的样本在各个维度做归一化，而是针对同一层的所有神经元做归一化。归一化公式为：<br>$$<br>\mu^l = \frac{1}{H}\Sigma_1^Ha_i^l<br>$$</p><p>$$<br>\sigma^l = \sqrt{\frac{1}{H}\Sigma_1^H(a_i^l-\mu^l)}<br>$$</p><p>其中，H指的是一层神经网络的神经元个数。我们再回想下BatchNormalization，其实它是在每个神经元上对batch_size个数据做归一化，每个神经元的均值和方差均不相同。而LayerNormalization则是对<strong>所有神经元</strong>做一个归一化，这就跟batch_size无关了。哪怕batch_size为1，这里的均值和方差只和神经元的个数有关系（如果读到这里仍然感到不是特别清楚，再读两遍…还困惑也没关系，待会看<strong>Figure 7</strong>）。</p></li><li><p>测试的时候可以直接利用LN，所以训练时不用保存均值和方差，这节省了内存空间。</p></li></ul><p><strong>Figure 7</strong>示意了两种方式的区别。假设有N个样本，每个样本的特征维度为4，图中每个小圆代表一个特征，特征1，特征2，…，特征4。BatchNormalization是在N个同一特征（如特征1）上求均值和方差，这里要对每个特征求1次，共4次。对照一下上面说的，万一有个样本有5个特征，是不是就没法玩了。LayerNormalization呢，别的样本都和我没啥关系，有多少个特征我把这些特征求个均值方差就好了。这也就是为什么一个叫”批归一化“，另一个叫”层归一化“了。理解了这一点，也就理解了为什么Transformer中使用LN而不是BN。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/7.png" alt=""></p><center>Figure 7</center><p>当然BatchNormalization也不是吃素的，虽然它在处理不等长序列上存在天生的缺陷，但是除此之外，它的效果都要好于其他Normalization方式（比如LN，WN，IN）。直觉上，BN貌似更好理解一点，LN似乎有种胡子眉毛一把抓的感觉…</p><p>回到层归一化的代码中，注意到这里的求均值和方差均是应用在x的最后一维上。这一维其实就是in_features，即神经元个数，同BN一样，这里也引入两个参数，参与训练。如果你想问这样做为什么也会有效，我愿意再次告诉你深度学习是个实验领先于理论的学科^_^，当然，不排除某位数学大神能够看出两者之间存在某种等价性。</p><h3 id="前馈网络"><a href="#前馈网络" class="headerlink" title="前馈网络"></a>前馈网络</h3><p>每个encoderLayer中，多头attention后会接一个前馈网络。这个前馈网络其实是两个全连接层，进行了如下操作：<br>$$<br>\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''Implements FFN equation.</span></span><br><span class="line"><span class="string">    d_model=512</span></span><br><span class="line"><span class="string">    d_ff=2048</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="comment"># self.w_1 = nn.Conv1d(in_features=d_model, out_features=d_ff, kenerl_size=1)</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        <span class="comment"># self.w_2 = nn.Conv1d(in_features=d_ff, out_features=d_model, kenerl_size=1)</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure><p>这两层的作用等价于两个 kenerl_size=1的一维卷积操作。</p><h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>这里就是普通的不能再普通的词向量，将词语变成d_model维的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>由于Transformer没有用到CNN和RNN，因此，句子单词之间的位置信息就没有利用到。显然，这些信息对于翻译来说是非常有用的，同样一句话，每个单词的意思能够准确的翻译出来，但如果顺序不对，表达出来的意思就截然不同了。举个栗子感受一下，原句：”A man went through the Big Buddhist Temple“, 翻译成：”人过大佛寺“和”寺佛大过人“，意思就完全不同了。</p><p>那么如何表达一个序列的位置信息呢？对于某一个单词来说，他的位置信息主要有两个方面：一是绝对位置，二是相对位置。绝对位置决定了单词在一个序列中的第几个位置，相对位置决定了序列的流向。作者利用了正弦函数和余弦函数来进行位置编码：<br>$$<br>PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})<br>$$</p><p>$$<br>PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})<br>$$</p><p>其中pos是单词处于句子的第几个位置。我们来考察一下第一个公式，看是否每个位置都能得到一个唯一的值作为编码。为简单起见，不妨令i=0，那么：<br>$$<br>PE_{(pos,0)} = sin(pos)<br>$$<br>我们反过来想，假如存在位置j和k的编码值相同，那么就有：<br>$$<br>sin(i)=sin(j)<br>$$</p><p>$$<br>i, j 为非负整数且i不等于j<br>$$</p><p>以上两式需要同时满足，可等价为：<br>$$<br>i=(-1)^k\cdot j+k\cdot{\pi}<br>$$</p><p>$$<br>i, j 为非负整数且i不等于j且k为整数<br>$$</p><p>同时成立，这就意味着：<br>$$<br>\pi=\frac{[i-(-1)^k\cdot j]}{k}<br>$$<br>这显然是不可能的，因为左边是个无理数（无限不循环小数），而右边是个有理数。通过反证法就证明了在这种表示下，每个位置确实有唯一的编码。</p><p>上面的讨论并未考虑i的作用。i决定了频率的大小，不同的i可以看成是不同的频率空间中的编码，是相互正交的，通过改变i的值，就能得到多维度的编码，类似于词向量的维度。这里2i&lt;=512（d_model）, 一共512维。想象一下，当2i大于d_model时会出现什么情况，这时sin函数的周期会变得非常大，函数值会非常接近于0，这显然不是我们希望看到的，因为这样和词向量就不在一个量级了，位置编码的作用被削弱了。另外，值得注意的是，位置编码是不参与训练的，而词向量是参与训练的。作者通过实验发现，位置编码参与训练与否对最终的结果并无影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><p>之所以对奇偶位置分别编码，是因为编码前一个位置是可以由另一个位置线性表示的（公差为1的等差数列），在编码之后也希望能保留这种线性。我们以第1个位置和第k+1个位置为例，还是令i=0：<br>$$<br>PE(1, 0)=cos(1)<br>$$</p><p>$$<br>PE(k,0)=sin(1+k)=sin(1)cos(k)+cos(1)sin(k)=A+B\cdot PE(1,0)<br>$$</p><hr><p>至此，我们就把Encoder部分的细节介绍完了，下面来看下Decoder部分</p><hr><h2 id="Deocder"><a href="#Deocder" class="headerlink" title="Deocder"></a>Deocder</h2><p>我们先在看一眼刚开始的那张框架图。左半部分是Encoder，右半部分是Decoder。不难看出，Decoder和Encoder极其相似。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/Transfomer/2.PNG" alt=""></p><p>首先，Decoder也是由6个相同的decoder组件构成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>每个组件长什么样子呢？首先输入经过词向量和位置编码，进入target的自注意力层，这里和Encoder一样，也是用了残差和层归一化。然后呢，这个输出再和Encoder的输出做一次context attention，相当于把上面的那层重复了一次，唯一不同的是，这次的attention有点不一样的，不再是自注意力，所有的技术细节都可以参照Encoder部分，这里不再复述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h2 id="线性层和softmax"><a href="#线性层和softmax" class="headerlink" title="线性层和softmax"></a>线性层和softmax</h2><p>这是整个模型的最后一步了。从Decoder拿到的输出是维度为（batch_size, max_seq_len, d_model）的浮点型张量，我们希望得到最终每个单词预测的结果，首先用一个线性层将d_model映射到vocab的维度，得到每个单词的可能性，然后送入softmax，找到最可能的单词。</p><p>线性层的参数个数为d_model <em> vocab_size， 一般来说，vocab_size会比较大，拿20000为例，那么只这层的参数就有512</em>20000个，约为10的8次方，非常惊人。而在词向量那一层，同样也是这个数值，所以，一种比较好的做法是将这两个全连接层的参数共享，会节省不少内存，而且效果也不会差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>至此，整个Transformer模型的介绍就告一段落了，希望读者能够有所收获。以上恭疏短引，已竭鄙怀。请洒潘江，各倾陆海云尔！</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is all your need</a></li><li><a href="https://arxiv.org/abs/1707.00896v1" target="_blank" rel="noopener">Multilingual Hierarchical Attention Networks for Document Classification</a></li><li><a href="https://arxiv.org/pdf/1607.06450v1" target="_blank" rel="noopener">Layer Normalization</a></li><li><a href="https://arxiv/pdf/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Transformer原理和实现-从入门到精通&quot;&gt;&lt;a href=&quot;#Transformer原理和实现-从入门到精通&quot; class=&quot;headerlink&quot; title=&quot;Transformer原理和实现 从入门到精通&quot;&gt;&lt;/a&gt;Transformer原理和实现 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="Attention" scheme="http://state-of-art.top/tags/Attention/"/>
    
      <category term="Transformer" scheme="http://state-of-art.top/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>self-attention</title>
    <link href="http://state-of-art.top/2019/01/06/BERT%E7%B3%BB%E5%88%97(%E4%B8%80)Self-attention/"/>
    <id>http://state-of-art.top/2019/01/06/BERT系列(一)Self-attention/</id>
    <published>2019-01-06T15:30:08.000Z</published>
    <updated>2019-02-03T12:35:52.658Z</updated>
    
    <content type="html"><![CDATA[<h1 id="self-Attention"><a href="#self-Attention" class="headerlink" title="self-Attention"></a>self-Attention</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>讲到BERT就不能不提到Transformer，而self-attention则是Transformer的精髓所在。简单来说，可以将Transformer看成和RNN类似的特征提取器，而其有别于RNN、CNN这些传统特征提取器的是，它另辟蹊径，采用的是attention机制对文本序列进行特征提取。</p><p>所以我们从self-Attention出发。</p><p>文章内容参考<a href="https://jalammar.github.io/illustrated-transformer/，jalammar的博客十分通俗易懂，且切中要害，本系列内容不乏很多翻译自jalammar的博客。" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/，jalammar的博客十分通俗易懂，且切中要害，本系列内容不乏很多翻译自jalammar的博客。</a></p><h2 id="Attention-is-all-your-need"><a href="#Attention-is-all-your-need" class="headerlink" title="Attention is all your need"></a>Attention is all your need</h2><p>尽管attention机制由来已久，但真正令其声名大噪的是google 2017年的这篇名为《attention is all your need》的论文。</p><p>让我们从一个简单的例子看起：</p><p>假设我们想用机器翻译的手段将下面这句话翻译成中文：</p><p>“The animal didn’t cross the street because it was too tired”</p><p>当机器读到“it”时，“it”代表“animal”还是“street”呢？对于人类来讲，这是一个极其简单的问题，但是对于机器或者说算法来讲却十分不容易。</p><p>self-Attention则是处理此类问题的一个解决方案，也是目前看起来一个比较好的方案。当模型处理到“it”时，self-Attention可以将“it”和“animal‘联系到一起。</p><p>它是怎么做到的呢？</p><p>通俗地讲，当模型处理一句话中某一个位置的单词时，self-Attention允许它看一看这句话中其他位置的单词，看是否能够找到能够一些线索，有助于更好地表示（或者说编码）这个单词。</p><p>如果你对RNN比较熟悉的话，我们不妨做一个比较。RNN通过保存一个隐藏态，将前面词的信息编码后依次往后面传递，达到利用前面词的信息来编码当前词的目的。而self-Attention仿佛有个上帝之眼，纵观全局，看看上下文中每个词对当前词的贡献。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/1.png" alt=""></center><p>下面来看下具体是怎么实现的。</p><h2 id="Self-Attention-in-Detail"><a href="#Self-Attention-in-Detail" class="headerlink" title="Self-Attention in Detail"></a>Self-Attention in Detail</h2><p>首先，来看下怎样使用向量来计算self-attention，紧接着看如何用矩阵来计算self-attention。</p><h3 id="使用向量"><a href="#使用向量" class="headerlink" title="使用向量"></a>使用向量</h3><p>如下图所示，一般而言，输入的句子进入模型的第一步是对单词进行embedding，每个单词对应一个embedding。对于每个embedding，我们创建三个向量，Query、Key和Value向量。我们如何来创建三个向量呢？如图，我们假设embedding的维度为4，我们希望得到一个维度为3的Query、Key和Value向量，只需将每个embedding乘上一个维度为4*3的矩阵即可。这些矩阵就是训练过程中要学习的。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/2.png" alt=""><br></center><p>那么，Query、Key和Value向量代表什么呢？他们在attention的计算中发挥了什么样的作用呢？</p><p>我们用一个例子来说明：</p><p>首先要明确一点，self-attention其实是在计算每个单词对当前单词的贡献，也就是对每个单词对当前单词的贡献进行打分score。假设我们现在要计算下图中所有单词对第一个单词”Thinking”的打分。那么分分数如何计算呢，只需要将该单词的Query向量和待打分单词的Key向量做点乘即可。比如，第一个单词对第一个单词的分数为q1× k1，第二个单词对第一个单词的分数为q1×k2。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/3.png" alt=""><br></center><p>我们现在得到了两个单词对第一个单词的打分（分数是个数字了），然后将其进行softmax归一化。需要注意的是，在BERT模型中，作者在softmax之前将分数除以了Key的维度的平方根（据说可以保持梯度稳定）。softmax得到的是每个单词在Thinking这个单词上的贡献的权重。显然，当前单词对其自身的贡献肯定是最大的。</p><p>接着就是Value向量登场的地方了。将上面的分数分别和Value向量相乘，注意这里是对应位置相乘。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/4.png" alt=""><br></center><p>最后，将相乘的结果求和，这就得到了self-attention层对当前位置单词的输出。对每个单词进行如上操作，就能得到整个句子的attention输出了。在实际使用过程中，一般采用矩阵计算使整个过程更加高效。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/5.png" alt=""><br></center><p>在开始矩阵计算之前，先回顾总结一下上面的步骤：</p><ul><li>创建Query、Key、Value向量</li><li>计算每个单词在当前单词的分数</li><li>将分数归一化后与Value相乘</li><li>求和</li></ul><p>值得注意的是，上面阐述的过程实际上是Attention机制的计算流程，对于self-Attention，Query=Key=Value。</p><h3 id="Matrix-Calculation-of-Self-Attention"><a href="#Matrix-Calculation-of-Self-Attention" class="headerlink" title="Matrix Calculation of Self-Attention"></a>Matrix Calculation of Self-Attention</h3><p>其实矩阵计算就是将上面的向量放在一起，同时参与计算。</p><p>首先，将embedding向量pack成一个矩阵X。假设我们有一句话有长度为10，embedding维度为4，那么X的维度为（10 × 4）.</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/6.png" alt=""><br></center><p>假设我们设定Q、K、V的维度为3.第二步我们构造一个维度为（4×3）的权值矩阵。将其与X做矩阵乘法，得到一个10×3的矩阵，这就能得到Query了。依样画葫芦，同样可以得到Key和Value。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/7.png" alt=""><br></center><p>最后，将Query和Key相乘，得到打分，然后经过softmax，接着乘上V的到最终的输出。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/8.png" alt=""><br></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;self-Attention&quot;&gt;&lt;a href=&quot;#self-Attention&quot; class=&quot;headerlink&quot; title=&quot;self-Attention&quot;&gt;&lt;/a&gt;self-Attention&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="BERT" scheme="http://state-of-art.top/tags/BERT/"/>
    
      <category term="Attention" scheme="http://state-of-art.top/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>Torchtext读取JSON数据</title>
    <link href="http://state-of-art.top/2018/12/31/TORCHTEXT%E8%AF%BB%E5%8F%96JSON%E6%95%B0%E6%8D%AE/"/>
    <id>http://state-of-art.top/2018/12/31/TORCHTEXT读取JSON数据/</id>
    <published>2018-12-31T15:50:08.000Z</published>
    <updated>2018-12-31T07:38:16.994Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在文本预处理一节，介绍了如何利用torchtext读取tsv格式的文本数据。对于分类问题，这是足够的。但是在处理如NER和机器翻译等问题时，我们构造的输入通常就不是（类别，序列）这样的结构了，而是（序列，序列）。另一方面，在搭建混合网络时，有时我们希望能够给模型多个输入（例如cnn-bilstm-crf中，既需要字符又需要单词输入），这超过了tsv所能。因此要另辟蹊径。</p><p>尽管Torchtext封装了一个SequenceTaggingDataset类用于构造NER数据，但是在实际使用中发现，十分不方便生成batch。</p><p>json格式是采取字典的方式存储数据，这带来了很大的灵活性。但是关于其完整使用的相关文章非常有限，官方文档也未给出详细案例（不得不吐槽一下torchtext的说明文档）。</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>下面以NER任务为例， 进行json使用说明，以抛砖引玉。事实上，这种方法可以无缝地拓展到其他的任务上去，如文本分类，机器翻译等。</p><p>对于NER任务，标签(target)和输入(source)都是同样长度的序列。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">source: </span><br><span class="line">人 民 网 <span class="number">1</span> 月 <span class="number">1</span> 日 讯 据 《 纽 约 时 报 》 报 道 , 美 国 华 尔 街 股 市 在 <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">3</span> 年 的 最 后 一 天 继 续 上 涨 , 和 全 球 股 市 一 样 , 都 以 最 高 纪 录 或 接 近 最 高 纪 录 结 束 本 年 的 交 易 。</span><br><span class="line">target:</span><br><span class="line">O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O O O O B_LOC I_LOC I_LOC I_LOC I_LOC O O O B_T I_T I_T I_T I_T O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O</span><br></pre></td></tr></table></figure><h3 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h3><p>一般来说，为了节省内存，会先将其做个字符-ID映射(这里只是举例说明，和上面不是同一段文本)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">source:</span><br><span class="line"><span class="number">5627</span> <span class="number">5580</span> <span class="number">5550</span> <span class="number">5636</span> <span class="number">4509</span> <span class="number">5192</span> <span class="number">5466</span> <span class="number">5463</span> <span class="number">5624</span> <span class="number">5520</span> <span class="number">4871</span> <span class="number">5637</span> <span class="number">5607</span> <span class="number">5411</span> <span class="number">5313</span> <span class="number">5251</span> <span class="number">5528</span> <span class="number">5628</span> <span class="number">5580</span> <span class="number">5612</span> <span class="number">5292</span> <span class="number">5636</span> <span class="number">5626</span> <span class="number">5637</span> <span class="number">4810</span></span><br><span class="line">target:</span><br><span class="line"><span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="构造Json格式的文件"><a href="#构造Json格式的文件" class="headerlink" title="构造Json格式的文件"></a>构造Json格式的文件</h3><p>因为我们最终是通过torchtext来生成batch，所以首先要把数据存储为torchtext能够读取的json格式。值得注意的是，torchtext能够读取的json文件和我们一般意义上的json文件格式是不同的（这也是比较坑的地方），我们需要把上面的数据处理成如下格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br></pre></td></tr></table></figure><p>可以看到，里面的内容和通常的Json并无区别，每个字段采用字典的格式存储。不同的是，多个json序列中间是以换行符隔开的，而且最外面没有列表。</p><p>那么怎么构造这样的数据呢？</p><p>我们知道python中的json模块是用来处理json文件的，但是对所有序列进行json.dump()后的结果并非我们想要的（序列之间不是以换行符隔开的），几经尝试，找到如下的处理方式，仅做参考，可能还有更好的处理方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">    <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">        sent = <span class="string">' '</span>.join([str(w) <span class="keyword">for</span> w <span class="keyword">in</span> sent])</span><br><span class="line">        label = <span class="string">' '</span>.join([str(l) <span class="keyword">for</span> l <span class="keyword">in</span> label])</span><br><span class="line">        df = &#123;<span class="string">"source"</span>: sent, <span class="string">"target"</span>: label&#125;</span><br><span class="line">        encode_json = json.dumps(df)</span><br><span class="line">        <span class="comment"># 一行一行写入，并且采用print到文件的方式</span></span><br><span class="line">        print(encode_json, file=fw)</span><br></pre></td></tr></table></figure><p>这里采用的是一行一行进行dumps， 然后print到文件，就能得到我们想要的格式了。</p><p>接下来就是使用torchtext读取了，这个和之前处理tsv文件并无太大差异。</p><h3 id="Torchtext读取"><a href="#Torchtext读取" class="headerlink" title="Torchtext读取"></a>Torchtext读取</h3><p>这里和之前处理tsv类似，不多赘述，只是将几个不同的点提出来说一下。</p><p>（1）Field的定义，这里source和target都是序列，因此两个字段的定义方式基本相同</p><p>（2）传入TabularDataset的fields和tsv的定义有所不同，这里定义成字典-元组格式</p><p>（3） TabularDataset的format要指定成json格式</p><p>（4）pad_token根据需要，一般来说source使用0作为padding， target使用-1进行padding</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line">    SOURCE = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                 use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                 fix_length=self.fix_length,   <span class="comment">#  如需静态padding,则设置fix_length, 但要注意要大于文本最大长度</span></span><br><span class="line">                 eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                 include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    TARGET = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                 use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                 fix_length=self.fix_length,   <span class="comment">#  如需静态padding,则设置fix_length, 但要注意要大于文本最大长度</span></span><br><span class="line">                 eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                 include_lengths=<span class="keyword">False</span>, pad_token=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    fields = &#123;<span class="string">'source'</span>: (<span class="string">'source'</span>, SOURCE), <span class="string">'target'</span>: (<span class="string">'target'</span>, TARGET)&#125;</span><br><span class="line"></span><br><span class="line">    train, valid = TabularDataset.splits(</span><br><span class="line">        path=config.ROOT_DIR,</span><br><span class="line">        train=self.train_path, validation=self.valid_path,</span><br><span class="line">        format=<span class="string">"json"</span>,</span><br><span class="line">        skip_header=<span class="keyword">False</span>,</span><br><span class="line">        fields=fields)</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_iterator</span><span class="params">(self, train, valid)</span>:</span></span><br><span class="line">    train_iter = BucketIterator(train,</span><br><span class="line">                                batch_size=self.batch_size,</span><br><span class="line">                                device = torch.device(<span class="string">"cpu"</span>),  <span class="comment"># cpu by -1, gpu by 0</span></span><br><span class="line">                                sort_key=<span class="keyword">lambda</span> x: len(x.source), <span class="comment"># field sorted by len</span></span><br><span class="line">                                sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                repeat=<span class="keyword">False</span>)</span><br><span class="line">    val_iter = BucketIterator(valid,</span><br><span class="line">                                batch_size=self.batch_size,</span><br><span class="line">                                device=torch.device(<span class="string">"cpu"</span>),  <span class="comment"># cpu by -1, gpu by 0</span></span><br><span class="line">                                sort_key=<span class="keyword">lambda</span> x: len(x.source),  <span class="comment"># field sorted by len</span></span><br><span class="line">                                sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                repeat=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;在文本预处理一节，介绍了如何利用torchtext读取tsv格式的文本数据。对于分类问题，这是足够的。但是在处理如NER和机器翻译等问题时，
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="torchtext" scheme="http://state-of-art.top/tags/torchtext/"/>
    
      <category term="json" scheme="http://state-of-art.top/tags/json/"/>
    
      <category term="batch" scheme="http://state-of-art.top/tags/batch/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的代码实现（四）</title>
    <link href="http://state-of-art.top/2018/12/31/BiLSTM%E6%A8%A1%E5%9E%8B%E4%B8%ADCRF%E5%B1%82%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%EF%BC%88%E5%9B%9B%EF%BC%89%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>http://state-of-art.top/2018/12/31/BiLSTM模型中CRF层的运行原理（四） 代码实现/</id>
    <published>2018-12-31T15:30:08.000Z</published>
    <updated>2018-12-31T07:37:02.217Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现"><a href="#CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现" class="headerlink" title="CRF ON THE TOP OF BILSTM（四） 代码实现"></a>CRF ON THE TOP OF BILSTM（四） 代码实现</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>前面我们重点介绍了CRF的原理，损失函数以及分数的计算。本节将结合前面的相关内容，介绍BILSTM+CRF代码实现（pytorch 0.4.0）及一些需要注意的细节。</p><h2 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h2><p>BILSTM+CRF模型由BILSTM， CRF， 损失函数， 预测函数几部分组成。BILSTM的输出作为CRF的输入，损失函数定义在CRF中， 损失函数使用前向算法，预测函数使用Viterbi算法，下面逐一介绍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BISLTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word_embedding_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word2id,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, bi_flag,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layer, input_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cell_type, dropout,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_tag, checkpoint_dir)</span>:</span></span><br><span class="line">        super(BISLTM_CRF, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, word_embedding_dim)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            p.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.embedding.weight.data.copy_(torch.from_numpy(get_embedding(vocab_size,</span><br><span class="line">                                                                        word_embedding_dim,</span><br><span class="line">                                                                        word2id)))</span><br><span class="line"></span><br><span class="line">        self.rnn = RNN(hidden_size, bi_flag,</span><br><span class="line">                       num_layer, input_size,</span><br><span class="line">                       cell_type, dropout, num_tag)</span><br><span class="line"></span><br><span class="line">        self.crf = CRF(num_tag=num_tag)</span><br><span class="line"></span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        rnn_output = self.rnn(embeddings, length)     <span class="comment"># (batch_size, time_steps, num_tag+2)</span></span><br><span class="line">        <span class="keyword">return</span> rnn_output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(self, rnn_output, labels, length)</span>:</span></span><br><span class="line">        loss = self.crf.negative_log_loss(inputs=rnn_output, length=length, tags=labels)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, rnn_output, length)</span>:</span></span><br><span class="line">        best_path = self.crf.get_batch_best_path(rnn_output, length)</span><br><span class="line">        <span class="keyword">return</span> best_path</span><br></pre></td></tr></table></figure><h2 id="BILSTM"><a href="#BILSTM" class="headerlink" title="BILSTM"></a>BILSTM</h2><p>BILSTM模型这里采用的是双向GRU。GRU的输出（维度为 batch_size <em> time_steps </em> hidden_size）经过线性层（hidden_size <em> num_tag）变为(batch_size </em> time_steps <em> num_tag), 这是最终的输出结果。其中，hidden_size是GRU隐藏层个数</em>2(因为是双向)， num_tag 为需要标注的标签个数。</p><h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>对于一个句子，损失函数为这个句子的所有可能的路径分数之和减去真实路径分数。值得注意的是，这里的“路径”指的句子中的有小部分，不包括padding部分。</p><p>对于一个batch，我们将所有句子的损失函数相加，然后除以总的句子长度，将损失分配到每个字上，作为训练的损失函数。</p><p>这里的重点在于计算真实路径分数和所有路径分数之和，下面来看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">negative_log_loss</span><span class="params">(self, inputs, length, tags)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    features:(batch_size, time_step, num_tag)</span></span><br><span class="line"><span class="string">    target_function = P_real_path_score/P_all_possible_path_score</span></span><br><span class="line"><span class="string">                    = exp(S_real_path_score)/ sum(exp(certain_path_score))</span></span><br><span class="line"><span class="string">    我们希望P_real_path_score的概率越高越好，即target_function的值越大越好</span></span><br><span class="line"><span class="string">    因此，loss_function取其相反数，越小越好</span></span><br><span class="line"><span class="string">    loss_function = -log(target_function)</span></span><br><span class="line"><span class="string">                  = -S_real_path_score + log(exp(S_1 + exp(S_2) + exp(S_3) + ...))</span></span><br><span class="line"><span class="string">                  = -S_real_path_score + log(all_possible_path_score)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.use_cuda:</span><br><span class="line">        inputs = inputs.cpu()</span><br><span class="line">        length = length.cpu()</span><br><span class="line">        tags = tags.cpu()</span><br><span class="line"></span><br><span class="line">    loss = Variable(torch.tensor(<span class="number">0.</span>), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    num_chars = torch.sum(length.data).float()      <span class="comment"># 所有字的个数</span></span><br><span class="line">    <span class="keyword">for</span> ix, (features, tag) <span class="keyword">in</span> enumerate(zip(inputs, tags)):</span><br><span class="line">        features = features[:length[ix]]</span><br><span class="line">        tag = tag[:length[ix]]</span><br><span class="line">        real_score = self.real_path_score(features, tag)      <span class="comment"># 真实路径分数</span></span><br><span class="line">        total_score = self.all_possible_path_score(features)  <span class="comment"># 所有可能的路径分数</span></span><br><span class="line">        cost = total_score - real_score</span><br><span class="line">        loss  = loss + cost</span><br><span class="line">    <span class="keyword">return</span> loss/num_chars             <span class="comment"># 分配到每个字的损失</span></span><br></pre></td></tr></table></figure><h3 id="转移矩阵"><a href="#转移矩阵" class="headerlink" title="转移矩阵"></a>转移矩阵</h3><p>在介绍分数计算之前，先来看下CRF中的一个重要的训练参数，转移矩阵。转移矩阵的定义决定了后面前向算法和维特比算法的实现，因此十分重要。</p><p>我们的标签序列id映射定义为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tag_to_ix = &#123;</span><br><span class="line">    <span class="string">"B_PER"</span>: <span class="number">0</span>,   <span class="comment"># 人名</span></span><br><span class="line">    <span class="string">"I_PER"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"B_LOC"</span>: <span class="number">2</span>,   <span class="comment"># 地点</span></span><br><span class="line">    <span class="string">"I_LOC"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"B_ORG"</span>: <span class="number">4</span>,   <span class="comment"># 机构</span></span><br><span class="line">    <span class="string">"I_ORG"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"B_T"</span>: <span class="number">6</span>,     <span class="comment"># 时间</span></span><br><span class="line">    <span class="string">"I_T"</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">"O"</span>: <span class="number">8</span>,       <span class="comment"># 其他</span></span><br><span class="line">    <span class="string">"SOS"</span>: <span class="number">9</span>,     <span class="comment"># 起始符</span></span><br><span class="line">    <span class="string">"EOS"</span>:<span class="number">10</span>      <span class="comment"># 结束符</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>转移矩阵的大小为：(num_tag +2, num_tag +2)。+2是因为包含了起始符号和结束符号。</p><p>并且，P_jk 表示从tag_j到tag_k的分数，这样就有：</p><p>（1）P_j* 表示所有从tag_j出发的边</p><p>（2） P_*k 表示所有到tag_k的边</p><p>此外，在均匀初始化时，设定起始符号和结束符号对应的转移分数，使得：</p><p>（1） 从EOS-&gt;其他标签为不可能事件, 如果发生，则产生一个极大的损失</p><p>（2） 从其他标签-&gt;SOS为不可能事件，如果发生，则产生一个极大的损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tag, use_cuda=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> num_tag &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Invalid value of num_tag: %d"</span> % num_tag)</span><br><span class="line">    super(CRF, self).__init__()</span><br><span class="line">    self.num_tag = num_tag</span><br><span class="line">    self.start_tag = num_tag</span><br><span class="line">    self.end_tag = num_tag + <span class="number">1</span></span><br><span class="line">    self.use_cuda = use_cuda</span><br><span class="line">    <span class="comment"># 转移矩阵transitions：P_jk 表示从tag_j到tag_k的分数</span></span><br><span class="line">    <span class="comment"># P_j* 表示所有从tag_j出发的边</span></span><br><span class="line">    <span class="comment"># P_*k 表示所有到tag_k的边</span></span><br><span class="line">    self.transitions = nn.Parameter(torch.Tensor(num_tag + <span class="number">2</span>, num_tag + <span class="number">2</span>))</span><br><span class="line">    nn.init.uniform_(self.transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">    self.transitions.data[self.end_tag, :] = <span class="number">-10000</span>   <span class="comment"># 表示从EOS-&gt;其他标签为不可能事件, 如果发生，则产生一个极大的损失</span></span><br><span class="line">    self.transitions.data[:, self.start_tag] = <span class="number">-10000</span>   <span class="comment"># 表示从其他标签-&gt;SOS为不可能事件, 同上</span></span><br></pre></td></tr></table></figure><h3 id="真实路径分数"><a href="#真实路径分数" class="headerlink" title="真实路径分数"></a>真实路径分数</h3><p>真实路径分数由发射分数和转移分数组成。其中，转移分数是CRF需要训练的参数，我们随机初始化；发射分数是BISTLM的输出矩阵（对于每个句子，其维度为 time_steps * num_tag）， 对于每个tag， 该矩阵都给出一个分数，我们知道了真实的标签序列， 拿这个标签去索引该矩阵，即可得到真实路径的发射分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">real_path_score</span><span class="params">(self, features, tags)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    features: (time_steps, num_tag)</span></span><br><span class="line"><span class="string">    real_path_score表示真实路径分数</span></span><br><span class="line"><span class="string">    它由Emission score和Transition score两部分相加组成</span></span><br><span class="line"><span class="string">    Emission score由LSTM输出结合真实的tag决定，表示我们希望由输出得到真实的标签</span></span><br><span class="line"><span class="string">    Transition score则是crf层需要进行训练的参数，它是随机初始化的，表示标签序列前后间的约束关系（转移概率）</span></span><br><span class="line"><span class="string">    Transition矩阵存储的是标签序列相互间的约束关系</span></span><br><span class="line"><span class="string">    在训练的过程中，希望real_path_score最高，因为这是所有路径中最可能的路径</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    r = torch.LongTensor(range(features.size(<span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">if</span> self.use_cuda:</span><br><span class="line">        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.start_tag]), tags])</span><br><span class="line">        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.end_tag])])</span><br><span class="line">        r = r.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pad_start_tags = torch.cat([torch.LongTensor([self.start_tag]), tags])</span><br><span class="line">        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.end_tag])])</span><br><span class="line">    <span class="comment"># Transition score + Emission score</span></span><br><span class="line">    score = torch.sum(self.transitions[pad_start_tags, pad_stop_tags]).cpu() + torch.sum(features[r, tags])</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><h3 id="所有路径分数之和"><a href="#所有路径分数之和" class="headerlink" title="所有路径分数之和"></a>所有路径分数之和</h3><p>所有路径分数之和采用前向算法计算，算法复杂度为O(N <em> N </em> T)， 其中N为标签个数，T为序列长度。</p><p>这里， 将start_tag的发射分数初始化为0，从start_tag开始，沿着序列一个字一个字地计算前向分数。</p><p>最后，加上end_tag的转移分数，即为所有START_TAG -&gt; 1st word -&gt; 2nd word -&gt;…-&gt;END_TAG的分数。</p><p>事实上，前向算法的精髓在于，用指数将每个字的分数用三个数存起来，而这三个数后面又是可以拆分的(指数的计算特性，乘法拆成加法。）ps：如果对前向算法不是很明白或者大致明白但不是特别清晰，建议画个图看下就一目了然了。</p><p>需要注意的是，根据前向算法，需要计算指数和的log， log_sum_exp， 由于前向方法是一个不断累积的过程， 会导致exp之和趋于无穷大，超过计算机的浮点数最大值限制，出现“上溢”。避免这种做法的手段是，找到整个矩阵的最大值，将矩阵减去该最大值进行指数求和，最终结果再加上该最大值即可。减去最大值后求指数和尽管也会出现无穷大的情况，但是这时是在分母上，该项可以计算为0，这样我们取log后仍可以得到一个合理的数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_possible_path_score</span><span class="params">(self, features)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算所有可能的路径分数的log和：前向算法</span></span><br><span class="line"><span class="string">    step1: 将forward列expand成9*9</span></span><br><span class="line"><span class="string">    step2: 将下个单词的emission行expand成9*9</span></span><br><span class="line"><span class="string">    step3: 将1和2和对应位置的转移矩阵相加</span></span><br><span class="line"><span class="string">    step4: 更新forward，合并行</span></span><br><span class="line"><span class="string">    step5: 取forward指数的对数计算total</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    time_steps = features.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    forward = Variable(torch.zeros(self.num_tag))     <span class="comment"># 初始化START_TAG的发射分数为0</span></span><br><span class="line">    <span class="keyword">if</span> self.use_cuda:</span><br><span class="line">        forward = forward.cuda()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, time_steps):  <span class="comment"># START_TAG -&gt; 1st word -&gt; 2nd word -&gt;...-&gt;END_TAG</span></span><br><span class="line">        emission_start = forward.expand(self.num_tag, self.num_tag).t()</span><br><span class="line">        emission_end = features[i,:].expand(self.num_tag, self.num_tag)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            trans_score = self.transitions[self.start_tag, :self.start_tag].cpu()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_score = self.transitions[:self.start_tag, :self.start_tag].cpu()</span><br><span class="line">        sum = emission_start + emission_end + trans_score</span><br><span class="line">        forward = log_sum(sum, dim=<span class="number">0</span>)</span><br><span class="line">    forward = forward + self.transitions[:self.start_tag, self.end_tag].cpu()  <span class="comment"># END_TAG</span></span><br><span class="line">    total_score = log_sum(forward, dim=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> total_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum</span><span class="params">(matrix, dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向算法是不断累积之前的结果，这样就会有个缺点</span></span><br><span class="line"><span class="string">    指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf</span></span><br><span class="line"><span class="string">    为了避免这种情况，我们做了改动：</span></span><br><span class="line"><span class="string">    1. 用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算</span></span><br><span class="line"><span class="string">    SUM = log(exp(s1)+exp(s2)+...+exp(s100))</span></span><br><span class="line"><span class="string">        = log&#123;exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]&#125;</span></span><br><span class="line"><span class="string">        = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]</span></span><br><span class="line"><span class="string">    where clip=max</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    clip_value = torch.max(matrix)                 <span class="comment"># 极大值</span></span><br><span class="line">    clip_value = int(clip_value.data.tolist())</span><br><span class="line">    log_sum_value = clip_value + torch.log(torch.sum(torch.exp(matrix-clip_value), dim=dim))</span><br><span class="line">    <span class="keyword">return</span> log_sum_value</span><br></pre></td></tr></table></figure><p>整个训练部分至此就介绍完了。下面介绍使用维特比算法进行预测。</p><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>预测算法迭代过程和前向算法类似。不同的是，forward不再是到该点的路径分数之和，而是到该点的所有路径中最大分数， 这样我们就有了到该点的最大路径。同时，在迭代过程中 ，我们将最大路径中到当前节点（具有最大分数）的前一个节点的索引存储下来，作为最佳路径点。</p><p>中间的迭代过程较容易理解，比较绕的是</p><p>（1） 如何处理起始符号和结束符号</p><p>（2） 最后一个节点怎么找</p><p>我们只需要把握一个点，上面的问题就能迎刃而解：</p><p>预测和回溯的时候，我们要从START_TAG -&gt; 1st word -&gt; 2nd word -&gt;…-&gt;END_TAG，一个都不能少！</p><p>最后奉上一份PPT<url><a href="https://baidupan.com，" target="_blank" rel="noopener">https://baidupan.com，</a> 里面用图形化的方式对CRF原理进行了详细的描述。</url></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        time_steps = features.size(<span class="number">0</span>)</span><br><span class="line">        forward = Variable(torch.zeros(self.num_tag))  <span class="comment"># START_TAG</span></span><br><span class="line">        <span class="keyword">if</span> self.use_cuda:</span><br><span class="line">            forward = forward.cuda()</span><br><span class="line">        <span class="comment"># back_points 到该点的最大分数  last_points 前一个点的索引</span></span><br><span class="line">        back_points, index_points = [self.transitions[self.start_tag, :self.start_tag].cpu()], [torch.LongTensor([<span class="number">-1</span>]).expand_as(forward)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, time_steps):  <span class="comment"># START_TAG -&gt; 1st word -&gt; 2nd word -&gt;...-&gt;END_TAG</span></span><br><span class="line">            emission_start = forward.expand(self.num_tag, self.num_tag).t()</span><br><span class="line">            emission_end = features[i,:].expand(self.num_tag, self.num_tag)</span><br><span class="line">            trans_score = self.transitions[:self.start_tag, :self.start_tag].cpu()</span><br><span class="line">            sum = emission_start + emission_end + trans_score</span><br><span class="line">            forward, index = torch.max(sum.detach(), dim=<span class="number">0</span>)</span><br><span class="line">            back_points.append(forward)</span><br><span class="line">            index_points.append(index)</span><br><span class="line">        back_points.append(forward + self.transitions[:self.start_tag, self.end_tag].cpu())  <span class="comment"># END_TAG</span></span><br><span class="line">        <span class="keyword">return</span> back_points, index_points</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_best_path</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        back_points, index_points = self.viterbi(features)</span><br><span class="line">        <span class="comment"># 找到线头</span></span><br><span class="line">        best_last_point = argmax(back_points[<span class="number">-1</span>])</span><br><span class="line">        index_points = torch.stack(index_points)   <span class="comment"># 堆成矩阵</span></span><br><span class="line">        m = index_points.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 初始化矩阵</span></span><br><span class="line">        best_path = [best_last_point]</span><br><span class="line">        <span class="comment"># 循着线头找到其对应的最佳路径</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            best_index_point = index_points[i][best_last_point]</span><br><span class="line">            best_path.append(best_index_point)</span><br><span class="line">            best_last_point = best_index_point</span><br><span class="line">        best_path.reverse()</span><br><span class="line">        <span class="keyword">return</span> best_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch_best_path</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.use_cuda:</span><br><span class="line">            inputs = inputs.cpu()</span><br><span class="line">            length = length.cpu()</span><br><span class="line">        max_len = inputs.size(<span class="number">1</span>)</span><br><span class="line">        batch_best_path = []</span><br><span class="line">        <span class="keyword">for</span> ix, features <span class="keyword">in</span> enumerate(inputs):</span><br><span class="line">            features = features[:length[ix]]</span><br><span class="line">            best_path = self.get_best_path(features)</span><br><span class="line">            best_path = torch.Tensor(best_path).long()</span><br><span class="line">            best_path = padding(best_path, max_len)</span><br><span class="line">            batch_best_path.append(best_path)</span><br><span class="line">        batch_best_path = torch.stack(batch_best_path, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> batch_best_path</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(matrix, dim=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""(0.5, 0.4, 0.3) —&gt; 0"""</span></span><br><span class="line">    _, index = torch.max(matrix, dim=dim)</span><br><span class="line">    <span class="keyword">return</span> index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding</span><span class="params">(vec, max_len, pad_token=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    new_vec = torch.zeros(max_len).long()</span><br><span class="line">    new_vec[:vec.size(<span class="number">0</span>)] = vec</span><br><span class="line">    new_vec[vec.size(<span class="number">0</span>):] = pad_token</span><br><span class="line">    <span class="keyword">return</span> new_vec</span><br></pre></td></tr></table></figure><p>完整代码请戳 <a href="https://github.com/circlePi/NER/BILSTM_CRF" target="_blank" rel="noopener">https://github.com/circlePi/NER/BILSTM_CRF</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现&quot;&gt;&lt;a href=&quot;#CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现&quot; class=&quot;headerlink&quot; title=&quot;CRF ON THE TOP OF BILSTM（四） 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的运行原理(三)</title>
    <link href="http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--3/"/>
    <id>http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--3/</id>
    <published>2018-12-31T15:27:08.000Z</published>
    <updated>2018-12-31T07:22:24.487Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的部分中，我们学习了BiLSTM-CRF模型的结构和CRF损失函数的细节。您可以通过各种开源框架（Keras，Chainer，TensorFlow等）实现您自己的BiLSTM-CRF模型。最重要的事情之一是在这些框架上自动计算模型的反向传播，因此您不需要自己实现反向传播来训练模型（即计算梯度和更新参数）。此外，一些框架已经实现了CRF层，因此只需添加一行代码就可以非常轻松地将CRF层与您自己的模型相结合。</p><p>在本节中，我们将探讨如何预测句子的标签序列。</p><a id="more"></a><h2 id="1-预测句子的标签序列"><a href="#1-预测句子的标签序列" class="headerlink" title="1. 预测句子的标签序列"></a>1. 预测句子的标签序列</h2><p>第1步：BiLSTM-CRF模型的emission score和transition score<br>假设句子$x$包含３个字符：$x=[w_0, w_1, w_2]$。此外，假设我们已经获得了BiLSTM模型的emission score和CRF层的transition score：</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$w_0$</td><td>$w_{01}$</td><td>$w_{02}$</td></tr><tr><td>$w_1$</td><td>$w_{11}$</td><td>$w_{12}$</td></tr><tr><td>$w_2$</td><td>$w_{21}$</td><td>$w_{22}$</td></tr></tbody></table><p>$x_{ij}$表示$w_i$被标记为$l_j$的得分。</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$l_1$</td><td>$t_{11}$</td><td>$t_{12}$</td></tr><tr><td>$l_2$</td><td>$t_{21}$</td><td>$t_{22}$</td></tr></tbody></table><p>$t_{ij}$表示标签$i$转移到标签$j$的分数</p><p>第2步：预测<br>如果您熟悉Viterbi算法，这部分对您来说很容易。如果你不太了解的话，请不要担心，我将逐步解释算法。对于一个句子，我们将按从左到右的方式进行预测，即:</p><p>$w_0$<br>$w_0$–&gt;$w_1$<br>$w_0$–&gt;$w_1$–&gt;$w_2$</p><p>你将看到两个变量：$obs$和$previous$。$previous$存储前面步骤的最终结果。$obs$表示来自当前单元的信息。</p><hr><p>首先，我们来看看$w_0$，即:</p><p>$obs=[x_{01}, x_{02}]$</p><p>$previous=None$</p><p>对于第一个字符$w_0$。$w_0$的最优标签很简单。例如，如果$obs=[x_{01}=0.2,   x_{02}=0.8]$，显然，$w_0$的最优标签是$l_2$，由于只有一个字符从而无标签到标签之间的transition，因此不用计算transition scores。</p><hr><p>接着，对于$w_0$ –&gt; $w_1$：</p><p>$obs=[x_{11,  x_{12}}]$</p><p>$previous=[x_{01}, x_{02}]$</p><p>1).扩展$previous$为：</p><p>$$previous=\left(^{previous[0] \quad previous[0]}<em>{previous[1] \quad previous[1]}\right)=\left(^{x</em>{01} \quad x_{01}}_{x_{02} \quad x_{02}}\right)$$</p><p>2).扩展$obs$为：</p><p>$$obs=\left(^{obs[0] \quad obs[1]}<em>{obs[0] \quad obs[1]}\right)=\left(^{x</em>{11} \quad x_{12}}_{x_{11} \quad x_{12}}\right)$$</p><p>3).将$previous$，$obs$和transition score进行求和:</p><p>$$scores=\left(^{x_{01} \quad x_{01}}_{x_{02} \quad x_{02}}\right) + \left(^{x_{11} \quad x_{12}}_{x_{11} \quad x_{12}}\right) + \left(^{t_{11} \quad t_{12}}_{t_{21} \quad t_{22}}\right)$$</p><p>即:</p><p>$$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}_{x_{02}+x_{11}+t{21} \quad x_{02}+x_{12}+t_{22}}\right)$$</p><p>当我们计算所有可能标签序列组合的总得分时，您可能想知道与前一部分没有有区别。接下来我们将看到其中的差异性。</p><p>更新$previous$值：</p><p>$$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])]$$</p><p>假设，我们得到的score为：</p><p>$$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}_{x_{02}+x_{11}+t{21} \quad x_{02}+x_{12}+t_{22}}\right)=\left( ^{0.2 \quad 0.3}_{0.5 \quad 0.4}\right)$$</p><p>则$previous$将更新为:</p><p>$$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])] = [0.5, 0.4]$$</p><p>$previous$是什么意思？$previous$列表存储当前字符对每个标签的最大得分。</p><h3 id="1-1-案例"><a href="#1-1-案例" class="headerlink" title="1.1 案例"></a>1.1 案例</h3><p>假设，在语料库中，我们总共有2个标签，$label1(l_1)$和$label2(l_2)$，这两个标签的索引分别为0和1。</p><p>$previous[0]$是以第0个标签$l_1$结束的序列的最大得分，类似的$previous[1]$是以$l_2$结束的序列的最大得分。在每次迭代过程中，我们仅仅保留每个标签对应的最优序列的信息($previous=[\max(scores[00], scores[10]),\max( scores[01], scores[11])]$)。分数较少的序列信息将被丢弃。</p><p>回到我们的主要任务：</p><p>同时，我们还有两个变量来存储历史信息（分数和索引），即$alpha_0$和$alpha_1$。<br>每次迭代，我们都将最好的得分追加到$alpha_0$。为方便起见，每个标签的最高分都有下划线。</p><p>$$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}<em>{\underline{x</em>{02}+x_{11}+t{21}} \quad \underline{x_{02}+x_{12}+t_{22}}}\right)=\left( ^{0.2 \quad 0.3}_{\underline{0.5} \quad \underline{0.4}}\right)$$</p><p>$$alpha_0=[(scores[10],scores[11])]=[(0.5,0.4)]$$</p><p>另外，相应的列的索引被保存在$alpha_1$：</p><p>$$alpha_1=[(ColumnIndex(scores[10]),ColumnIndex(scores[11]))]=[(1,1)]$$</p><p>其中，$l_1$的索引是0，$l_2$的索引是1，所以$(1, 1)=(l_2, l_2)$，这意味着对于当前的单元$w_i$和标签$l^(i)$：<br>$(1, 1)=(l_2, l_2)$=(当序列是$\underline{l^{(i-1)}=l_2} -&gt; \underline{l^{(i)}=l_1}$时我们可以得到最大得分为0.5, 当序列是$\underline{l^{(i-1)}=l_2} -&gt; \underline{l^{(i)}=l_2}$时我们可以得到最大得分为0.4)</p><p>$l^{(i-1)}$是前一个字符$w_{i-1}$对应的标签</p><hr><p>最后，我们计算$w_0$ –&gt; $w_1$ –&gt; $w_2$:</p><p>$obs=[x_{21}, x_{22}]$</p><p>$previous=[0.5, 0.4]$</p><p>1).扩展$previous$为：</p><p>$$previous=\left(^{previous[0] \quad previous[0]}<em>{previous[1] \quad previous[1]}\right)=\left(^{0.5 \quad 0.5}</em>{0.4 \quad 0.4}\right)$$</p><p>2).扩展$obs$为:</p><p>$$obs=\left(^{obs[0] \quad obs[1]}<em>{obs[0] \quad obs[1]}\right)=\left(^{x</em>{21} \quad x_{22}}_{x_{21} \quad x_{22}}\right)$$</p><p>3).对$previous$，$obs$和transition score进行求和:</p><p>$$scores=\left(^{0.5 \quad 0.5}<em>{0.4 \quad 0.4}\right) +\left(^{x</em>{21} \quad x_{22}}_{x_{21} \quad x_{22}}\right)+ \left(^{t_{11} \quad t_{12}}_{t_{21} \quad t_{22}}\right)$$</p><p>即:</p><p>$$scores=\left(^{0.5+x_{21}+t_{11} \quad 0.5+x_{22}+t_{12}}<em>{0.4+x</em>{21}+t{21} \quad 0.4+x_{22}+t_{22}}\right)$$</p><p>更新$previous$为：</p><p>$$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])]$$</p><p>比如，我们得到的scores为：</p><p>$$scores=\left( ^{0.6 \quad \underline{0.9}}_{\underline{0.8} \quad 0.7}\right)$$</p><p>因此，$previous$将更新为：</p><p>$$previous=[0.8, 0.9]$$</p><p>实际上，$previousp[0]$和$previous[1]$之间的较大的一个是最好的预测结果的得分。与此同时，每个标签的最大得分和索引将被添加到$alpha_0$和$alpha_1$中：</p><p>$alpha_0=[(0.5,0.4),\underline{(scores[10],scores[01])}]$</p><p>$　　　=[(0.5,0.4),\underline{(0.8,0.9)}]$</p><p>$alpha_1=[(1,1),\underline{(1,0)}]$</p><hr><p>第3步：找出得分最高的最佳序列<br>在该步骤中，我们将根据$previousp[0]$和$previous[1]$找到最高的得分。我们将从右到左的方式反推最优序列，即最后一个单元反推到第一个单元。</p><hr><p>$w_1$ –&gt; $w_2$：<br>首先，检查$alpha_0$和$alpha_1$最后一个元素:(0.8, 0.9)和(1, 0)。0.9是最高分数，其对应的位置是1，因此对应的标签是$l_2$。继续从$alpha_1$中对应位置获得$w_1$对应的标签索引， 即(1, 0)[1]=0。索引0表示$w_1$对应的标签是$l_1$。因此我们可以得到$w_1 -&gt; w_2$的最佳序列是$l_1 -&gt; l_2$。</p><p>$w_0$ –&gt; $w_1$：</p><p>接着，我们继续向前移动并获得$alpha_1$的上一个元素：(1, 1)。从上面可知$w_1$的标签是$l_1$(标签对应的索引为0)，因此我们可以得到$w_0$对应的标签索引为(1,1)[0]=1。所以我们可以得到$w_0 -&gt; w_1$的最佳序列是$l_2 -&gt; l_1$。</p><p>最终可以得到$w_0 -&gt; w_1 -&gt; w_2$的最佳标签序列是$l_2 -&gt; l_1  -&gt; l_2$</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360. </p><p><a href="https://arxiv.org/abs/1603.01360" target="_blank" rel="noopener">https://arxiv.org/abs/1603.01360</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前面的部分中，我们学习了BiLSTM-CRF模型的结构和CRF损失函数的细节。您可以通过各种开源框架（Keras，Chainer，TensorFlow等）实现您自己的BiLSTM-CRF模型。最重要的事情之一是在这些框架上自动计算模型的反向传播，因此您不需要自己实现反向传播来训练模型（即计算梯度和更新参数）。此外，一些框架已经实现了CRF层，因此只需添加一行代码就可以非常轻松地将CRF层与您自己的模型相结合。&lt;/p&gt;
&lt;p&gt;在本节中，我们将探讨如何预测句子的标签序列。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的运行原理(二)</title>
    <link href="http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--2/"/>
    <id>http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--2/</id>
    <published>2018-12-31T15:21:08.000Z</published>
    <updated>2018-12-31T07:38:01.034Z</updated>
    
    <content type="html"><![CDATA[<p>在上一节中，我们知道CRF层可以从训练数据集中自动学习到一些约束规则来保证预测标签的合法性。这些约束包括：</p><ul><li>句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”</li><li>标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-PersonI-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列.</li><li>标签序列“O I-label”是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说，有效的标签序列应该是“O B-label”。</li></ul><p>这一节，我们将解释为什么CRF层会自动学习到这些约束规则。</p><a id="more"></a><h2 id="1-CRF层"><a href="#1-CRF层" class="headerlink" title="1.CRF层"></a>1.CRF层</h2><p>在CRF层损失函数中，有两种形式的score。这些scores是CRF层的关键概念。</p><h3 id="1-1-emission-score"><a href="#1-1-emission-score" class="headerlink" title="1.1 emission score"></a>1.1 emission score</h3><p>第一个是emission score，主要来自BiLSTM层的输出，如下图所示，比如，单元$w_0$标记为‘B-Person’的score为1.5：</p><div align="center"><img src=" https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181228000013.png"></div><p>为了方便起见，我们用数字来表示各个实体标签，对应关系如下：</p><table><thead><tr><th>Label</th><th>Index</th></tr></thead><tbody><tr><td>B-Person</td><td>0</td></tr><tr><td>I-Person</td><td>1</td></tr><tr><td>B-Organization</td><td>2</td></tr><tr><td>I-Organization</td><td>3</td></tr><tr><td>O</td><td>4</td></tr></tbody></table><p>另外，我们使用$x_{iy_j}$表示emission score，其中:</p><ul><li>i 表示词的索引</li><li>$y_j$表示标签的索引</li></ul><p>例如，根据上图，我们可以得到$x_{i=1,y_j=2} = x_{w_1,B-Organization} = 0.1$，也就是说$w_1$标记为’B-Organization‘的score为0.1。</p><h3 id="1-2-Transition-score"><a href="#1-2-Transition-score" class="headerlink" title="1.2 Transition score"></a>1.2 Transition score</h3><p>我们定义$t_{y_iy_j}$表示transition score，从上图中可知$t_{B-Person,I-Person} = 0.9$，也就是说“$B-Person \rightarrow I-Person$“的标签转移得分为0.9。因此，对于所有的标签序列组合，我们将得到一个transition score矩阵，包含了所有标签之间的transition score。</p><p>为了使transition score矩阵更具鲁棒性，我们额外增加两个标签——START 和END，START 代表句子的开始位置，而非第一个词，同理，END代表句子的结束位置.</p><p>表1 中包含了START和END标签的transition score矩阵。</p><div align="center"><img src=" https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181228000035.png"></div><p>如表1所示，我们可以发现transition score矩阵可以学习到好多约束规则，比如:</p><ul><li>句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”（例如，从“START” 到 “I-Person or I-Organization”的score都很低 ）。</li><li>标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理的标签序列（例如，从“B-Organization”到’I-Person’的score只有0.0003，比其他都低。）。</li><li>标签序列“O I-label” 是不合理的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”（例如，$t_{O,I-Person}$的score就非常小）。</li></ul><p>你可能会有一个疑问，该transition score矩阵如何计算得到？</p><p>事实上，该矩阵是BiLSTM-CRF模型的一个参数，在训练模型之前，一般会初始化一个矩阵作为transition score矩阵，在训练过程中，该矩阵中的transition score都会不断地自动更新。也就是说CRF层可以自动学习，通过学习得到一个最优的transition score矩阵，而不用我们手工定义一个transition score矩阵。当模型训练不断地持续，该矩阵会越来越合理。</p><h3 id="1-3-CRF损失函数"><a href="#1-3-CRF损失函数" class="headerlink" title="1.3 CRF损失函数"></a>1.3 CRF损失函数</h3><p>CRF损失函数中包含了真实标签序列得分和所有可能标签序列的总得分，正常情况下，真实标签序列得分在所有可能标签序列得分中是最高的。</p><p>比如，假设数据集中的标签如下所示：</p><table><thead><tr><th>Label</th><th>Index</th></tr></thead><tbody><tr><td>B-Person</td><td>0</td></tr><tr><td>I-Person</td><td>1</td></tr><tr><td>B-Organization</td><td>2</td></tr><tr><td>I-Organization</td><td>3</td></tr><tr><td>O</td><td>4</td></tr><tr><td>START</td><td>5</td></tr><tr><td>END</td><td>6</td></tr></tbody></table><p>那么，在第一节中我们假设的句子$x$，所有可能的标签序列组合为:</p><ul><li>(1) START B-Person B-Person B-Person B-Person B-Person END</li><li>(2) START B-Person I-Person B-Person B-Person B-Person END</li><li>…</li><li>(10) <strong>START B-Person I-Person O B-Organization O END </strong></li><li>…</li><li>(N) O O O O O O O</li></ul><p>假设一共有N中可能的标签序列组合，且第$i$个标签序列的得分为$P_i$，那么所有可能标签序列组合的总得分为：</p><p>$$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ </p><p>按照我们之前的假设，第10个是真实的标签序列，那么，我们想要的结果是第10个标签序列得分在所有可能的标签序列得分中是最高的。</p><p>因此，我们可以定义模型的损失函数，在整个模型训练过程中，BiLSTM-CRF模型的参数不断地进行更新，使得真实标签序列得分在所有可能标签序列组合得分中的占比是最高的。因此，模型的损失函数格式如下所示：</p><p>$$ LossFunction = \frac{P_{RealPath}}{P_1+P_2+…+P_N} $$</p><p>那么，问题就来了：</p><ul><li>如何定义一个标签序列的得分？</li><li>如何计算所有可能标签序列组合的总得分？</li><li>在计算总得分中，一定需要计算每一个可能的标签序列的得分吗？</li></ul><p>接下来，我们来解答每一个问题。</p><h3 id="1-4-真实标签序列得分"><a href="#1-4-真实标签序列得分" class="headerlink" title="1.4 真实标签序列得分"></a>1.4 真实标签序列得分</h3><p>前面我们定义了标签序列得分为$P_i$，以及所有可能标签序列的总得分为：</p><p>$$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ </p><p>其中$e^{S_i}$表示第i个标签序列得分。</p><p>显然，在所有可能的标签序列组合必然存在一个序列是真实标签序列，而剩下的标签序列组合都是错误的，比如序列”<strong>START B-Person I-Person O B-Organization O END </strong>“是正确的，而序列‘START B-Person I-Person B-Person B-Person B-Person END’是错误的。</p><p>在整个模型训练过程中，CRF层的损失函数只需要两个得分：</p><ul><li>一个是真实标签序列得分</li><li>一个是所有可能标签序列组合的总得分</li></ul><p>而我们的学习目的是让真实的标签序列得分在总得分中的占比是最高的。</p><p>对于真实标签序列的得分$e^{S_i}$，我们直接计算$S_i$即可。</p><p>我们使用之前的案例，真实的标签序列为“<strong>START B-Person I-Person O B-Organization O END </strong>”，即：</p><ul><li>句子$x$由5个字符组成，$w_1,w_2,w_3,w_4,w_5$</li><li>我们在句子前后增加两个字符，记为$w_0,w_6$</li><li>$S_i$主要由第一节中提到的Emission Score和Transition Score组成，即$S_i = Emission Score + Transition Score$</li></ul><h4 id="1-4-1-Emission-Score"><a href="#1-4-1-Emission-Score" class="headerlink" title="1.4.1 Emission Score"></a>1.4.1 Emission Score</h4><p>Emission Score计算公式如下所示：</p><p>$Emission Score  = $</p><p>$x_{0,START} + x_{1,B-Person} + x_{2,I-Person} + x_{3,O} +$</p><p>$x_{4,B-Organization} + x_{5,O} + x_{6,END}$</p><p>其中：</p><ul><li>$x_{index,label}$表示第index个词被标记为label的得分</li><li>$x_{1,B-Person}, x_{2,I-Person} , x_{3,O}, x_{4,B-Organization},x_{5,O}$ 为BiLSTM层的输出</li><li>一般$x_{0,START}$和$x_{6,END}$为0</li></ul><h4 id="1-4-2-Transition-Score"><a href="#1-4-2-Transition-Score" class="headerlink" title="1.4.2 Transition Score"></a>1.4.2 Transition Score</h4><p>Transition Score计算公式如下所示:</p><p>$Transition Score  =$<br>$t_{START \rightarrow B-Person} + t_{B-Person \rightarrow I-Person} +$<br>$t_{I-Person \rightarrow O} + t_{O \rightarrow B-Organization} + t_{B-Organization \rightarrow O} + t_{O \rightarrow END}$</p><p>其中:</p><ul><li>$t_{label1 \rightarrow label2}$ 表示$label1$到$label2$的transition Score。</li><li>transition Score主要是在CRF层进行计算的，也就是说，transition Score完全是CRF层的参数。</li></ul><p>因此，我们通过计算$s_i$，可以得到第i条标签序列的得分。</p><h3 id="1-5-所有可能标签序列组合的总得分"><a href="#1-5-所有可能标签序列组合的总得分" class="headerlink" title="1.5 所有可能标签序列组合的总得分"></a>1.5 所有可能标签序列组合的总得分</h3><p>前面，我们计算了单条标签序列得分，接下来，我们需要计算所有可能标签序列的总得分。由之前内容可知，总得分的计算公式为;</p><p>$$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ </p><p>很显然，总得分计算方式就是每一条标签序列得分的求和，那么我们能想到的最简单的方法就是先计算每一条的标签序列得分，然后将所有的标签序列得分进行相加得到总得分。虽然计算很简单，但是效率不高，需要很长的训练时间。</p><p>接下来,我们将通过公式推导来认识总得分计算过程。</p><h3 id="1-6-CRF的损失函数"><a href="#1-6-CRF的损失函数" class="headerlink" title="1.6 CRF的损失函数"></a>1.6 CRF的损失函数</h3><p>由前面可知，CRF层的损失函数为:</p><p>$$ Loss Function = \frac{P_{RealPath}}{P_1 + P_2 + … + P_N} $$</p><p>我们对其对数化，即：</p><p>$$ LogLossFunction = \log \frac{P_{RealPath}}{P_1 + P_2 + … + P_N} $$</p><p>一般在模型训练过程中，我们希望损失函数最小化，因此，在损失函数添加一个负号，即:</p><p>$Log Loss Function$<br>$= - \log \frac{P_{RealPath}}{P_1 + P_2 + … + P_N}$<br>$= - \log \frac{e^{S_{RealPath}}}{e^{S_1} + e^{S_2} + … + e^{S_N}}$<br>$= - (\log(e^{S_{RealPath}}) - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$<br>$= - (S_{RealPath} - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$<br>$= - ( \sum_{i=1}^{N} x_{iy_i} + \sum_{i=1}^{N-1} t_{y_iy_{i+1}} - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$</p><p>因此，对于总得分，我们需要一个高效的方法计算:</p><p>$$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$$</p><h4 id="1-6-1-emission-Score和transition-Score"><a href="#1-6-1-emission-Score和transition-Score" class="headerlink" title="1.6.1 emission Score和transition Score"></a>1.6.1 emission Score和transition Score</h4><p>为了简化公式，我们假设句子的长度为3，即:</p><p>$x = (w_0,w_1,w_2)$</p><p>假设数据集中只有两个标签，即：</p><p>$LabelSet = (l_1,l_2)$</p><p>则emission Score矩阵可从BiLSTM层的输出获得，即：</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$w_0$</td><td>$x_{01}$</td><td>$x_{02}$</td></tr><tr><td>$w_1$</td><td>$x_{11}$</td><td>$x_{12}$</td></tr><tr><td>$w_2$</td><td>$x_{21}$</td><td>$x_{22}$</td></tr></tbody></table><p>其中$x_{ij}$为单元$w_i$被标记为$l_j$的得分。</p><p>而且，我们可以从CRF层中得到transition Score矩阵，即:</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$l_1$</td><td>$t_{11}$</td><td>$t_{12}$</td></tr><tr><td>$l_2$</td><td>$t_{21}$</td><td>$t_{22}$</td></tr></tbody></table><p>其中$t_{ij}$为标签$i$到标签$j$的得分。</p><h4 id="1-6-2-公式推导"><a href="#1-6-2-公式推导" class="headerlink" title="1.6.2 公式推导"></a>1.6.2 公式推导</h4><p>记住我们的目标是计算: $\log(e^{S_1} + e^{S_2} + … + e^{S_N})$</p><p>很显然，我们可以使用动态规划思想进行计算（如果你不了解动态规划，没关系，本文将一步一步地进行解释，当然还是建议你学习下动态规划算法）。简而言之，首先，我们计算$w_0$的所有可能序列的总得分。接着，我们使用上一步的总得分计算$w_0 \rightarrow w_1$的总得分。最后，我们同样使用上一步的总得分计算$w_0 \rightarrow w_1 \rightarrow w_2$的总得分。最后的总得分就是我们想要的总得分。</p><p>很明显，我们每一次计算都需要利用到上一步计算得到的结果，因此，接下来，你将看到两个变量:</p><ul><li>obs: 定义当前单元的信息</li><li>previous: 存储上一步计算的最后结果</li></ul><hr><p><strong>备注</strong>：以下内容如果看不懂的话，结合上面的emission Score矩阵和transition Score矩阵一起看就明白了</p><p>首先，我们计算$w_0$:</p><p>$obs = [x_{01},x_{02}]$<br>$previous = None$</p><p>如果我们的句子只有一个词$w_0$，那么存储上一步结果的$previous$为$None$，另外，对于$w_0$而言，$obs = [x_{01},x_{02}]$，其中$x_{01}$和$x_{02}$分别为emission Score（ＢiLSTM层的输出）。</p><p>因此，$w_0$的所有可能标签序列总得分为:</p><p>$TotalScore(w_0)=\log (e^{x_{01}} + e^{x_{02}})$</p><hr><p>接着，我们计算$w_0 \rightarrow w_1$:</p><p>$obs = [x_{11},x_{12}]$<br>$previous = [x_{01},x_{02}]$</p><p>为了计算方便，我们将$previous$转变为:</p><p>$$ previous =<br>\begin{pmatrix}<br>x_{01} &amp; x_{01} \<br>x_{02} &amp; x_{02}<br>\end{pmatrix}<br>$$<br>同样，将$obs$转变为:</p><p>$$ obs =<br>\begin{pmatrix}<br>x_{11} &amp; x_{12} \<br>x_{11} &amp; x_{12}<br>\end{pmatrix}<br>$$<br><strong>备注</strong>：通过矩阵方式计算更高效</p><p>接着，我们将$previous,abs$和transition Score进行相加,即:<br>$$<br>scores =<br>\begin{pmatrix}<br>x_{01}&amp;x_{01}\<br>x_{02}&amp;x_{02}<br>\end{pmatrix}<br>+<br>\begin{pmatrix}<br>x_{11}&amp;x_{12}\<br>x_{11}&amp;x_{12}<br>\end{pmatrix}<br>+<br>\begin{pmatrix}<br>t_{11}&amp;t_{12}\<br>t_{21}&amp;t_{22}<br>\end{pmatrix}<br>$$</p><p>接着，可得到:</p><p>$$<br>scores =<br>\begin{pmatrix}<br>x_{01}+x_{11}+t_{11}&amp;x_{01}+x_{12}+t_{12}\<br>x_{02}+x_{11}+t_{21}&amp;x_{02}+x_{12}+t_{22}<br>\end{pmatrix}<br>$$<br>从而我们可得到当前的$previous$为:</p><p>$$previous=[\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}), \log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})]$$</p><p>实际上，第二步已经算完了，可能还有人还无法理解如何得到$w_0$到$w_1$的所有可能序列组合（$label_1 \rightarrow label_1, label_1 \rightarrow label_2 , label_2 \rightarrow label_1, label_2 \rightarrow label_2$）的总得分，其实你主要按照以下计算方式即可;</p><p>$TotalScore(w_0 → w_1)$</p><p>$=\log (e^{previous[0]} + e^{previous[1]})$</p><p>$=\log (e^{\log(e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})}+<br>e^{\log(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})}<br>)$</p><p>$=\log(e^{x_{01}+x_{11}+t_{11}}+e^{x_{02}+x_{11}+t_{21}}+e^{x_{01}+x_{12}+t_{12}}+e^{x_{02}+x_{12}+t_{22}})$</p><p>很明显，与$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$很相似。</p><p>在上述公式中，我们可以看到:</p><ul><li>$S_1 = x_{01}+x_{11}+t_{11}$ ($label_1$ → $label_1$)<ul><li>$S_2 = x_{02}+x_{11}+t_{21}$ ($label_2$ → $label_1$)</li><li>$S_3 = x_{01}+x_{12}+t_{12}$ ($label_1$ → $label_2$)</li><li>$S_4 = x_{02}+x_{12}+t_{22}$ ($label_2$ → $label_2$)</li></ul></li></ul><hr><p>接着我们计算$w_0$ → $w_1$ → $w_2$:</p><p>如果你理解了上一步的计算过程的话，其实这一步的计算与上一步类似。即：</p><p>$obs = [x_{21}, x_{22}]$</p><p>$previous=[\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}), \log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})]$</p><p>类似于第二步，我们将$previous$转化为:</p><p>$$<br>previous =<br>\begin{pmatrix}<br>\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})&amp;\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})\<br>\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})&amp;\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})<br>\end{pmatrix}<br>$$</p><p>同样，将$obs$转化为:</p><p>$$<br>obs =<br>\begin{pmatrix}<br>x_{21}&amp;x_{22}\<br>x_{21}&amp;x_{22}<br>\end{pmatrix}<br>$$</p><p>将$previous，obs$和transition Score进行相加，即:</p><p>$scores =$<br>$\begin{pmatrix}<br>\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})&amp;\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})\<br>\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})&amp;\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})<br>\end{pmatrix}$<br>$+$<br>$\begin{pmatrix}<br>x_{21}&amp;x_{22}\<br>x_{21}&amp;x_{22}<br>\end{pmatrix}$<br>$+$<br>$\begin{pmatrix}<br>t_{11}&amp;t_{12}\<br>t_{21}&amp;t_{22}<br>\end{pmatrix}$</p><p>更新$previous$为:</p><p>$previous = [\log(<br>e^{\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}) + x_{22} + t_{12}}<br>+<br>e^{\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}}) + x_{22} + t_{22}})]$<br>$=\log( (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{22} + t_{12}} + (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{22} + t_{22}})]$</p><p>当计算到最后一步时，我们使用新的$previous$计算总得分:</p><p>$TotalScore(w_0 → w_1 → w_2)$</p><p>$=\log (e^{previous[0]} + e^{previous[1]})$</p><p>$=\log (e^{\log(<br>(e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{21} + t_{11}}<br>+<br>(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{21} + t_{21}}<br>)}$</p><p>$+e^{\log(<br>(e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{22} + t_{12}}<br>+<br>(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{22} + t_{22}})}<br>)$</p><p>$=\log (e^{x_{01}+x_{11}+t_{11}+x_{21}+t_{11}}+e^{x_{02}+x_{11}+t_{21}+x_{21}+t_{11}}$<br>$+e^{x_{01}+x_{12}+t_{12}+x_{21}+t_{21}}+e^{x_{02}+x_{12}+t_{22}+x_{21}+t_{21}}$<br>$+e^{x_{01}+x_{11}+t_{11}+x_{22}+t_{12}}+e^{x_{02}+x_{11}+t_{21}+x_{22}+t_{12}}$<br>$+e^{x_{01}+x_{12}+t_{12}+x_{22}+t_{22}}+e^{x_{02}+x_{12}+t_{22}+x_{22}+t_{22}})$</p><hr><p>到这里，我们就完成 了$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$的计算过程。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360.<br><a href="https://arxiv.org/abs/1603.01360" target="_blank" rel="noopener">https://arxiv.org/abs/1603.01360</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一节中，我们知道CRF层可以从训练数据集中自动学习到一些约束规则来保证预测标签的合法性。这些约束包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”&lt;/li&gt;
&lt;li&gt;标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-PersonI-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列.&lt;/li&gt;
&lt;li&gt;标签序列“O I-label”是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说，有效的标签序列应该是“O B-label”。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这一节，我们将解释为什么CRF层会自动学习到这些约束规则。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的运行原理(一)</title>
    <link href="http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--1/"/>
    <id>http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--1/</id>
    <published>2018-12-31T15:20:08.000Z</published>
    <updated>2018-12-31T07:37:47.606Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要内容如下:</p><ul><li>介绍: 在命名实体识别任务中，BiLSTM模型中CRF层的通用思想</li><li>实例: 通过实例来一步步展示CRF的工作原理</li><li>实现: CRF层的一步步实现过程</li></ul><p><strong>备注</strong>: 需要有的基础知识：你只需要知道什么是命名实体识别，如果你不懂神经网络，条件随机场（CRF）或者其它相关知识，不必担心，本文将向你展示CRF层是如何工作的。本文将尽可能的讲的通俗易懂。</p><a id="more"></a><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>基于神经网络的方法，在命名实体识别任务中非常流行和普遍。在文献[1]中，作者提出了BiLSTM-CRF模型用于实体识别任务中，在模型中用到了字嵌入和词嵌入。本文将向你展示CRF层是如何工作的。</p><p>如果你不知道BiLSTM和CRF是什么，你只需要记住他们分别是命名实体识别模型中的两个层。</p><h3 id="1-1开始之前"><a href="#1-1开始之前" class="headerlink" title="1.1开始之前"></a>1.1开始之前</h3><p>我们假设我们的数据集中有两类实体——<code>人名</code>和<code>地名</code>，与之相对应在我们的训练数据集中，有五类标签：</p><ul><li>B-Person</li><li>I-Person</li><li>B-Organization</li><li>I-Organization</li><li>O</li></ul><p>假设句子$x$由5个字符组成，即$x = (w_0,w_1,w_2,w_3,w_4)$，其中$[w_0,w_1]$为<code>人名</code>实体，$[w_3]$为<code>组织</code>实体，其他字符的标签为”O”。</p><h3 id="1-2-BiLSTM-CRF-模型"><a href="#1-2-BiLSTM-CRF-模型" class="headerlink" title="1.2 BiLSTM-CRF 模型"></a>1.2 BiLSTM-CRF 模型</h3><p>首先，对该模型进行简单的介绍，具体的模型结构，如下图所示：<br>​    </p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235235.png"></div><p>从上图中，总的来说可以总结为以下两点：</p><ul><li><p>句子$x$中的每一个单元都代表着由字嵌入或词嵌入构成的向量。其中，字嵌入一般是随机初始化的，而词嵌入一般是通过一个预训练好的词向量模型得到的。所有的嵌入在训练过程中都会调整到最优。        </p></li><li><p>这些字或词嵌入作为BiLSTM-CRF模型的输入，输出的是句子$x$中每个单元的标签。</p></li></ul><p>为了更容易了解CRF层的运行原理，我们需要知道BiLSTM的输出层。</p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235328.png"></div><p>如上图所示，BiLSTM层的输出为句子$x$中的每一个单元每一个标签的预测分值，例如，对于单元$w_0$，BiLSTM层输出的是1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) and 0.05 (O). 这些分值将作为CRF层的输入。</p><h3 id="1-3-如果没有CRF层会怎样"><a href="#1-3-如果没有CRF层会怎样" class="headerlink" title="1.3 如果没有CRF层会怎样"></a>1.3 如果没有CRF层会怎样</h3><p>你也许已经发现了，即使没有CRF层，我们也可以训练一个BiLSTM命名实体识别模型，如图下图所示：</p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235450.jpg"></div><p>由于BiLSTM的输出为每个单元的每一个标签预测分值，我们可以挑选分值最高的一个作为该单元的标签。例如，对于单元$w_0$,“B-Person”标签分值最高——1.5，因此我们可以将“B-Person”作为$w_0$的预测标签。同理，我们可以得到$w_1$—“I-Person”，$w_2$— “O” ，$w_3$—“B-Organization”，$w_4$—“O”。</p><p>虽然我们可以得到句子$x$中每个单元的正确标签，但是我们不能保证标签每次都是预测正确的。例如，如下图所示:</p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235531.png"></div><p>很显然，标签序列“I-Organization I-Person” and “B-Organization I-Person”是错误的。</p><h3 id="1-4-CRF层能从训练数据中学到约束规则"><a href="#1-4-CRF层能从训练数据中学到约束规则" class="headerlink" title="1.4 CRF层能从训练数据中学到约束规则"></a>1.4 CRF层能从训练数据中学到约束规则</h3><p>CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合理的。在训练过程中，这些约束可以通过CRF层自动学习到。</p><p>这些约束可以是：</p><ul><li>句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”</li><li>标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列.</li><li>标签序列“O I-label” 是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。有了这些约束，标签序列预测中不合理序列出现的概率将会大大降低。</li></ul><p>下一节，将通过CRF层的损失函数，解释CRF层如何从训练数据集中学习到这些约束。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360.<br><a href="https://arxiv.org/abs/1603.01360" target="_blank" rel="noopener">https://arxiv.org/abs/1603.01360</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要内容如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;介绍: 在命名实体识别任务中，BiLSTM模型中CRF层的通用思想&lt;/li&gt;
&lt;li&gt;实例: 通过实例来一步步展示CRF的工作原理&lt;/li&gt;
&lt;li&gt;实现: CRF层的一步步实现过程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;备注&lt;/strong&gt;: 需要有的基础知识：你只需要知道什么是命名实体识别，如果你不懂神经网络，条件随机场（CRF）或者其它相关知识，不必担心，本文将向你展示CRF层是如何工作的。本文将尽可能的讲的通俗易懂。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>浅析pytorch中的LSTM(GRU)</title>
    <link href="http://state-of-art.top/2018/11/30/%E6%B5%85%E6%9E%90pytorch%E4%B8%AD%E7%9A%84LSTM(GRU)%20/"/>
    <id>http://state-of-art.top/2018/11/30/浅析pytorch中的LSTM(GRU) /</id>
    <published>2018-11-30T15:21:08.000Z</published>
    <updated>2018-11-30T13:59:48.467Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文介绍pytorch RNN网络搭建，主要包括LSTM和GRU的使用。</p><p>最近从tensorflow入坑pytorch，发现两者的RNN模块前向传播有些不同。特别是RNN中的pack_padded_sequence和pad_packed_sequence。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="tensorflow的BILSTM"><a href="#tensorflow的BILSTM" class="headerlink" title="tensorflow的BILSTM"></a>tensorflow的BILSTM</h3><p>以BILSTM为例，tensorflow中我们一般这样操作：</p><ul><li><p>先定义个前向层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size)</span><br></pre></td></tr></table></figure></li><li><p>在定义一个后向层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size)</span><br></pre></td></tr></table></figure></li></ul><p>然后前向传播是这个样子的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, seq_length, training)</span>:</span></span><br><span class="line">       <span class="comment"># 嵌入层</span></span><br><span class="line">       embedded_words = self.embeddings(inputs)</span><br><span class="line">       <span class="comment"># RNN层</span></span><br><span class="line">       outputs, final_state = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">           self.fw_cell,</span><br><span class="line">           self.bw_cell,</span><br><span class="line">           inputs=embedded_words, </span><br><span class="line">           <span class="comment"># 句子原始长度（batch_size，1）</span></span><br><span class="line">           sequence_length=seq_length, </span><br><span class="line">           dtype=tf.float32,</span><br><span class="line">           time_major=<span class="keyword">False</span>)</span><br><span class="line">       <span class="comment"># 合并前后向的结果</span></span><br><span class="line">       <span class="comment"># outputs是一个列表:[（batch_size, cell_size）,(...)]</span></span><br><span class="line">       <span class="comment"># len(output) = time_steps</span></span><br><span class="line">       outputs = tf.concat(outputs, axis=<span class="number">2</span>)</span><br><span class="line">       <span class="comment"># 由于采用的是dynamic_rnn，padding部分自动被截断了</span></span><br><span class="line">       <span class="comment"># 这里取最后一维就好了</span></span><br><span class="line">       final_output = outputs[<span class="number">-1</span>]</span><br><span class="line">       logits = self.Dense(final_output)</span><br><span class="line">       <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>可以看到，tensorflow的<em>dynamic</em> LSTM有两个特点：</p><ul><li>前向传播时，不看词向量维度的话，输入是个二维（batch_size, time_steps)</li><li>padding部分不参与计算，会被自动截断</li></ul><p><em>with this in our mind</em>, <em>let‘s see what’s the bilstm in pytorch。</em></p><h2 id="Pytorch-BILSTM"><a href="#Pytorch-BILSTM" class="headerlink" title="Pytorch BILSTM"></a>Pytorch BILSTM</h2><h3 id="LSTM定义"><a href="#LSTM定义" class="headerlink" title="LSTM定义"></a>LSTM定义</h3><p>一般这样操作：</p><ul><li><p>先定义个LSTM</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension,</span><br><span class="line">                                    hidden_size=hidden_size,</span><br><span class="line">                                    num_layers=num_layer,</span><br><span class="line">                                    batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                    bidirectional=bi_flag)</span><br></pre></td></tr></table></figure><p>双向的怎么办呢, 改个参数就行了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bidirectional=<span class="keyword">True</span></span><br></pre></td></tr></table></figure></li><li><p>多层呢</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_layers=<span class="keyword">True</span></span><br></pre></td></tr></table></figure></li></ul><p>可以看到，pytorch的LSTM把所有的功能整合到一起了，不需要定义前向后向（当然我们也可以搞两个单向的）。此外，我们除了要传递隐藏层的个数cell_size, 还需要将词向量的维度传递进来，这是为什么呢？</p><p>这还不是关键。关键在于pytorch的动态padding机制。因为我们一般是将输入作为batch传进来的，对于变长的文本来说，padding是不可避免的。但是在我们使用LSTM进行计算时，是不希望padding部分参与计算的（他们不是真实的文本，假如纳入计算，会引入不必要的噪声和不必要的计算量）。tensorflow采用dynamic LSTM很好地解决了这一问题。pytorch当然也有他自己的一套，下面来看看。</p><h3 id="pack-padded-sequence"><a href="#pack-padded-sequence" class="headerlink" title="pack_padded_sequence"></a>pack_padded_sequence</h3><p>我们从embedding层拿到的输入维度为（batch_size, time_steps, word_embedding_dimension）, 并不将他直接喂给LSTM，而是要预加工一下，这时候第一个重要的函数pack_padded_sequence就登场了，可以将其看成一个截断函数，作用就是将padding部分截断。不仅如此，阶段后会将输入拉平，变成（batch_size*time_steps, word_embedding_dimension）, 仔细看一下，降了一个维度。来看实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence </span><br><span class="line"><span class="comment"># 手动造个张量, 包含3个序列，长度分别为10,5,3，使用0进行padding</span></span><br><span class="line">x = torch.FloatTensor([[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>],[<span class="number">7</span>],[<span class="number">8</span>],[<span class="number">8</span>],[<span class="number">9</span>]],[[<span class="number">1</span>], [<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]],[[<span class="number">5</span>],[<span class="number">4</span>],[<span class="number">6</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]]])</span><br><span class="line">print(<span class="string">"x shape is："</span>, x.shape)</span><br><span class="line"><span class="comment"># x 的真实长度</span></span><br><span class="line">length = torch.LongTensor([<span class="number">10</span>, <span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">x_packed = pack_padded_sequence(x, length,batch_first=<span class="keyword">True</span>)</span><br><span class="line">print(x_packed)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x shape <span class="keyword">is</span>：（<span class="number">3</span>，<span class="number">10</span>，<span class="number">1</span>）</span><br><span class="line">PackedSequence(data=tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>],</span><br><span class="line">        [<span class="number">8.</span>],</span><br><span class="line">        [<span class="number">8.</span>],</span><br><span class="line">        [<span class="number">9.</span>]]), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这里有几点要说一下：</p><ul><li>输入的张量是已经按照长度排序的！这是必须的，否则pack_padded_sequence函数会报错！一般而言排序有两种做法，一是使用torch.sort函数，第二是在构造batch时使用torchtext进行batch内排序，见我上篇博文</li><li>经过pack，输入发生了三个变化。一是由三维变成了2维(18,1)，二是padding部分的0没有了，三是对序列进行了拼接，这其实是维度降低的结果</li><li>pack完之后的结果是个tuple，tuple[0]是数据，是按列进行拼接的，tuple[1]是batch_size, 跟我们之前的batch_size是不一样的，这是为了我们后面可以还原</li></ul><p>好了，现在pddding问题解决了，我们将它输入进LSTM，前向传播是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">    <span class="string">"""前向传播"""</span></span><br><span class="line">    embeddings = self.embedding(inputs, length)  <span class="comment"># (batch_size, time_steps, embedding_dim)</span></span><br><span class="line">    <span class="comment"># 去除padding元素</span></span><br><span class="line">    <span class="comment"># embeddings_packed: (batch_size*time_steps, embedding_dim)</span></span><br><span class="line">    embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">    output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0))</span><br><span class="line">    <span class="comment"># padded_output: (batch_size, time_steps, hidden_size * bi_num)</span></span><br><span class="line">    <span class="comment"># h_n|c_n: (num_layer*bi_num, batch_size, hidden_size)</span></span><br><span class="line">    padded_output, length = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># 取最后一个有效输出作为最终输出（0为无效输出）</span></span><br><span class="line">    last_output = padded_output[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>这里做下参数说明：</p><ul><li>batch_first    类型：bool， True，输入的维度为（batch_size，time_steps, word_embedding_dimension）；False，输入维度为（time_steps, batch_size, word_embedding_dimension）</li><li>h_0: 初始化隐藏态</li><li>c_0: 初始化细胞态</li><li>output：输出，为一个tuple，后面会用例子说明</li><li>h_n: 最后个隐藏态</li><li>c_n: 最后个细胞态</li></ul><p>还是以上面我们造的那个张量为例，看下输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">birnn = nn.LSTM(input_size=<span class="number">1</span>, hidden_size=<span class="number">8</span>, bidirectional=<span class="keyword">True</span>)</span><br><span class="line">output, (h_n, c_n) = birnn(x_packed)</span><br><span class="line">print(<span class="string">'output:\n'</span>, output)</span><br><span class="line">print(<span class="string">'h_n shape:\n'</span>,h_n.shape)</span><br><span class="line">print(<span class="string">'c_n shape:\n'</span>,c_n.shape)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line"> PackedSequence(data=tensor([[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,  <span class="number">0.0291</span>,</span><br><span class="line">          <span class="number">0.0619</span>,  <span class="number">0.0213</span>,  <span class="number">0.1517</span>,  <span class="number">0.0241</span>,  <span class="number">0.2986</span>, <span class="number">-0.2594</span>, <span class="number">-0.1432</span>, <span class="number">-0.1742</span>],</span><br><span class="line">        [<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,  <span class="number">0.0291</span>,</span><br><span class="line">          <span class="number">0.0486</span>,  <span class="number">0.0195</span>,  <span class="number">0.1454</span>,  <span class="number">0.0138</span>,  <span class="number">0.2596</span>, <span class="number">-0.2467</span>, <span class="number">-0.1491</span>, <span class="number">-0.1596</span>],</span><br><span class="line">        [<span class="number">-0.1019</span>,  <span class="number">0.1606</span>, <span class="number">-0.0482</span>, <span class="number">-0.0519</span>, <span class="number">-0.1932</span>,  <span class="number">0.2632</span>, <span class="number">-0.0423</span>,  <span class="number">0.0774</span>,</span><br><span class="line">          <span class="number">0.0434</span>,  <span class="number">0.0142</span>,  <span class="number">0.0915</span>, <span class="number">-0.0455</span>,  <span class="number">0.2970</span>, <span class="number">-0.6665</span>, <span class="number">-0.0578</span>, <span class="number">-0.0521</span>],</span><br><span class="line">        [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,  <span class="number">0.0918</span>,</span><br><span class="line">          <span class="number">0.0730</span>,  <span class="number">0.0247</span>,  <span class="number">0.1526</span>,  <span class="number">0.0160</span>,  <span class="number">0.3559</span>, <span class="number">-0.4301</span>, <span class="number">-0.1375</span>, <span class="number">-0.1429</span>],</span><br><span class="line">        [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,  <span class="number">0.0918</span>,</span><br><span class="line">          <span class="number">0.0517</span>,  <span class="number">0.0197</span>,  <span class="number">0.1431</span>, <span class="number">-0.0005</span>,  <span class="number">0.2929</span>, <span class="number">-0.4016</span>, <span class="number">-0.1401</span>, <span class="number">-0.1218</span>],</span><br><span class="line">        [<span class="number">-0.1855</span>,  <span class="number">0.2300</span>, <span class="number">-0.0625</span>, <span class="number">-0.0508</span>, <span class="number">-0.2488</span>,  <span class="number">0.2741</span>, <span class="number">-0.0638</span>,  <span class="number">0.1441</span>,</span><br><span class="line">          <span class="number">0.0239</span>,  <span class="number">0.0137</span>,  <span class="number">0.0786</span>, <span class="number">-0.0524</span>,  <span class="number">0.2311</span>, <span class="number">-0.5685</span>, <span class="number">-0.0673</span>, <span class="number">-0.0485</span>],</span><br><span class="line">        [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,  <span class="number">0.1487</span>,</span><br><span class="line">          <span class="number">0.0789</span>,  <span class="number">0.0259</span>,  <span class="number">0.1354</span>,  <span class="number">0.0050</span>,  <span class="number">0.3851</span>, <span class="number">-0.5676</span>, <span class="number">-0.1110</span>, <span class="number">-0.1117</span>],</span><br><span class="line">        [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,  <span class="number">0.1487</span>,</span><br><span class="line">          <span class="number">0.0439</span>,  <span class="number">0.0172</span>,  <span class="number">0.1198</span>, <span class="number">-0.0221</span>,  <span class="number">0.2856</span>, <span class="number">-0.5105</span>, <span class="number">-0.1089</span>, <span class="number">-0.0852</span>],</span><br><span class="line">        [<span class="number">-0.1346</span>,  <span class="number">0.3093</span>, <span class="number">-0.0775</span>, <span class="number">-0.0678</span>, <span class="number">-0.2616</span>,  <span class="number">0.2946</span>, <span class="number">-0.1167</span>,  <span class="number">0.1699</span>,</span><br><span class="line">         <span class="number">-0.0280</span>,  <span class="number">0.0067</span>,  <span class="number">0.0333</span>, <span class="number">-0.0924</span>,  <span class="number">0.1308</span>, <span class="number">-0.5332</span>, <span class="number">-0.0237</span>, <span class="number">-0.0201</span>],</span><br><span class="line">        [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,  <span class="number">0.1887</span>,</span><br><span class="line">          <span class="number">0.0803</span>,  <span class="number">0.0254</span>,  <span class="number">0.1112</span>, <span class="number">-0.0077</span>,  <span class="number">0.3929</span>, <span class="number">-0.6681</span>, <span class="number">-0.0804</span>, <span class="number">-0.0828</span>],</span><br><span class="line">        [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,  <span class="number">0.1887</span>,</span><br><span class="line">          <span class="number">0.0194</span>,  <span class="number">0.0129</span>,  <span class="number">0.0852</span>, <span class="number">-0.0521</span>,  <span class="number">0.2364</span>, <span class="number">-0.5524</span>, <span class="number">-0.0720</span>, <span class="number">-0.0538</span>],</span><br><span class="line">        [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,  <span class="number">0.2141</span>,</span><br><span class="line">          <span class="number">0.0779</span>,  <span class="number">0.0234</span>,  <span class="number">0.0866</span>, <span class="number">-0.0215</span>,  <span class="number">0.3831</span>, <span class="number">-0.7393</span>, <span class="number">-0.0539</span>, <span class="number">-0.0580</span>],</span><br><span class="line">        [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,  <span class="number">0.2141</span>,</span><br><span class="line">         <span class="number">-0.0274</span>,  <span class="number">0.0076</span>,  <span class="number">0.0448</span>, <span class="number">-0.0817</span>,  <span class="number">0.1408</span>, <span class="number">-0.4683</span>, <span class="number">-0.0382</span>, <span class="number">-0.0285</span>],</span><br><span class="line">        [<span class="number">-0.1470</span>,  <span class="number">0.3710</span>, <span class="number">-0.0771</span>, <span class="number">-0.0605</span>, <span class="number">-0.2693</span>,  <span class="number">0.2962</span>, <span class="number">-0.0684</span>,  <span class="number">0.2291</span>,</span><br><span class="line">          <span class="number">0.0724</span>,  <span class="number">0.0204</span>,  <span class="number">0.0651</span>, <span class="number">-0.0356</span>,  <span class="number">0.3568</span>, <span class="number">-0.7885</span>, <span class="number">-0.0341</span>, <span class="number">-0.0385</span>],</span><br><span class="line">        [<span class="number">-0.1142</span>,  <span class="number">0.3956</span>, <span class="number">-0.0744</span>, <span class="number">-0.0727</span>, <span class="number">-0.2434</span>,  <span class="number">0.2935</span>, <span class="number">-0.1226</span>,  <span class="number">0.2366</span>,</span><br><span class="line">          <span class="number">0.0636</span>,  <span class="number">0.0164</span>,  <span class="number">0.0477</span>, <span class="number">-0.0497</span>,  <span class="number">0.3136</span>, <span class="number">-0.8199</span>, <span class="number">-0.0208</span>, <span class="number">-0.0245</span>],</span><br><span class="line">        [<span class="number">-0.0854</span>,  <span class="number">0.4126</span>, <span class="number">-0.0669</span>, <span class="number">-0.0829</span>, <span class="number">-0.2140</span>,  <span class="number">0.2871</span>, <span class="number">-0.1699</span>,  <span class="number">0.2389</span>,</span><br><span class="line">          <span class="number">0.0496</span>,  <span class="number">0.0118</span>,  <span class="number">0.0343</span>, <span class="number">-0.0641</span>,  <span class="number">0.2542</span>, <span class="number">-0.8307</span>, <span class="number">-0.0124</span>, <span class="number">-0.0151</span>],</span><br><span class="line">        [<span class="number">-0.0847</span>,  <span class="number">0.4176</span>, <span class="number">-0.0620</span>, <span class="number">-0.0877</span>, <span class="number">-0.2053</span>,  <span class="number">0.2863</span>, <span class="number">-0.2063</span>,  <span class="number">0.2468</span>,</span><br><span class="line">          <span class="number">0.0281</span>,  <span class="number">0.0090</span>,  <span class="number">0.0254</span>, <span class="number">-0.0784</span>,  <span class="number">0.1827</span>, <span class="number">-0.7871</span>, <span class="number">-0.0100</span>, <span class="number">-0.0116</span>],</span><br><span class="line">        [<span class="number">-0.0620</span>,  <span class="number">0.4284</span>, <span class="number">-0.0550</span>, <span class="number">-0.0931</span>, <span class="number">-0.1827</span>,  <span class="number">0.2785</span>, <span class="number">-0.2409</span>,  <span class="number">0.2438</span>,</span><br><span class="line">         <span class="number">-0.0278</span>,  <span class="number">0.0044</span>,  <span class="number">0.0120</span>, <span class="number">-0.1073</span>,  <span class="number">0.0924</span>, <span class="number">-0.6543</span>, <span class="number">-0.0047</span>, <span class="number">-0.0063</span>]],</span><br><span class="line">       grad_fn=&lt;CatBackward&gt;), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">h_n shape:</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>])</span><br><span class="line">c_n shape:</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><p>output由一个tuple组成，第一个元素就是输出，这里的维度为torch.Size([18, 16])，第二个元素和我们之前pack时的batch_size参数一样。</p><p>h_n和c_n分别为两个三维张量，维度如上，因为是双向，第一维为2.</p><p>到这里是不是就结束了呢，当然不是，pack完了之后经过LSTM输出了结果，看上去很费劲，因为序列连一起了，batch都没了。所以我们还原，这时候就用到pack_padded_sequence的好基友pad_packed_sequence了。</p><h3 id="pad-packed-sequence"><a href="#pad-packed-sequence" class="headerlink" title="pad_packed_sequence"></a>pad_packed_sequence</h3><p>pad_packed_sequence可以看成是解压缩操作。从上面可以看到，h_n, c_n已经是正常维度的张量了，没有pack，当然也用不着pad。我们只需对output做pad。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">padded_output, length = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'padded_output\n'</span>, padded_output)</span><br><span class="line">print(<span class="string">'padded_output shape\n'</span>, padded_output.shape)</span><br><span class="line">print(<span class="string">'length\n'</span>,length)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">padded_output</span><br><span class="line"> tensor([[[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,</span><br><span class="line">           <span class="number">0.0291</span>,  <span class="number">0.0619</span>,  <span class="number">0.0213</span>,  <span class="number">0.1517</span>,  <span class="number">0.0241</span>,  <span class="number">0.2986</span>, <span class="number">-0.2594</span>,</span><br><span class="line">          <span class="number">-0.1432</span>, <span class="number">-0.1742</span>],</span><br><span class="line">         [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,</span><br><span class="line">           <span class="number">0.0918</span>,  <span class="number">0.0730</span>,  <span class="number">0.0247</span>,  <span class="number">0.1526</span>,  <span class="number">0.0160</span>,  <span class="number">0.3559</span>, <span class="number">-0.4301</span>,</span><br><span class="line">          <span class="number">-0.1375</span>, <span class="number">-0.1429</span>],</span><br><span class="line">         [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,</span><br><span class="line">           <span class="number">0.1487</span>,  <span class="number">0.0789</span>,  <span class="number">0.0259</span>,  <span class="number">0.1354</span>,  <span class="number">0.0050</span>,  <span class="number">0.3851</span>, <span class="number">-0.5676</span>,</span><br><span class="line">          <span class="number">-0.1110</span>, <span class="number">-0.1117</span>],</span><br><span class="line">         [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,</span><br><span class="line">           <span class="number">0.1887</span>,  <span class="number">0.0803</span>,  <span class="number">0.0254</span>,  <span class="number">0.1112</span>, <span class="number">-0.0077</span>,  <span class="number">0.3929</span>, <span class="number">-0.6681</span>,</span><br><span class="line">          <span class="number">-0.0804</span>, <span class="number">-0.0828</span>],</span><br><span class="line">         [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,</span><br><span class="line">           <span class="number">0.2141</span>,  <span class="number">0.0779</span>,  <span class="number">0.0234</span>,  <span class="number">0.0866</span>, <span class="number">-0.0215</span>,  <span class="number">0.3831</span>, <span class="number">-0.7393</span>,</span><br><span class="line">          <span class="number">-0.0539</span>, <span class="number">-0.0580</span>],</span><br><span class="line">         [<span class="number">-0.1470</span>,  <span class="number">0.3710</span>, <span class="number">-0.0771</span>, <span class="number">-0.0605</span>, <span class="number">-0.2693</span>,  <span class="number">0.2962</span>, <span class="number">-0.0684</span>,</span><br><span class="line">           <span class="number">0.2291</span>,  <span class="number">0.0724</span>,  <span class="number">0.0204</span>,  <span class="number">0.0651</span>, <span class="number">-0.0356</span>,  <span class="number">0.3568</span>, <span class="number">-0.7885</span>,</span><br><span class="line">          <span class="number">-0.0341</span>, <span class="number">-0.0385</span>],</span><br><span class="line">         [<span class="number">-0.1142</span>,  <span class="number">0.3956</span>, <span class="number">-0.0744</span>, <span class="number">-0.0727</span>, <span class="number">-0.2434</span>,  <span class="number">0.2935</span>, <span class="number">-0.1226</span>,</span><br><span class="line">           <span class="number">0.2366</span>,  <span class="number">0.0636</span>,  <span class="number">0.0164</span>,  <span class="number">0.0477</span>, <span class="number">-0.0497</span>,  <span class="number">0.3136</span>, <span class="number">-0.8199</span>,</span><br><span class="line">          <span class="number">-0.0208</span>, <span class="number">-0.0245</span>],</span><br><span class="line">         [<span class="number">-0.0854</span>,  <span class="number">0.4126</span>, <span class="number">-0.0669</span>, <span class="number">-0.0829</span>, <span class="number">-0.2140</span>,  <span class="number">0.2871</span>, <span class="number">-0.1699</span>,</span><br><span class="line">           <span class="number">0.2389</span>,  <span class="number">0.0496</span>,  <span class="number">0.0118</span>,  <span class="number">0.0343</span>, <span class="number">-0.0641</span>,  <span class="number">0.2542</span>, <span class="number">-0.8307</span>,</span><br><span class="line">          <span class="number">-0.0124</span>, <span class="number">-0.0151</span>],</span><br><span class="line">         [<span class="number">-0.0847</span>,  <span class="number">0.4176</span>, <span class="number">-0.0620</span>, <span class="number">-0.0877</span>, <span class="number">-0.2053</span>,  <span class="number">0.2863</span>, <span class="number">-0.2063</span>,</span><br><span class="line">           <span class="number">0.2468</span>,  <span class="number">0.0281</span>,  <span class="number">0.0090</span>,  <span class="number">0.0254</span>, <span class="number">-0.0784</span>,  <span class="number">0.1827</span>, <span class="number">-0.7871</span>,</span><br><span class="line">          <span class="number">-0.0100</span>, <span class="number">-0.0116</span>],</span><br><span class="line">         [<span class="number">-0.0620</span>,  <span class="number">0.4284</span>, <span class="number">-0.0550</span>, <span class="number">-0.0931</span>, <span class="number">-0.1827</span>,  <span class="number">0.2785</span>, <span class="number">-0.2409</span>,</span><br><span class="line">           <span class="number">0.2438</span>, <span class="number">-0.0278</span>,  <span class="number">0.0044</span>,  <span class="number">0.0120</span>, <span class="number">-0.1073</span>,  <span class="number">0.0924</span>, <span class="number">-0.6543</span>,</span><br><span class="line">          <span class="number">-0.0047</span>, <span class="number">-0.0063</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,</span><br><span class="line">           <span class="number">0.0291</span>,  <span class="number">0.0486</span>,  <span class="number">0.0195</span>,  <span class="number">0.1454</span>,  <span class="number">0.0138</span>,  <span class="number">0.2596</span>, <span class="number">-0.2467</span>,</span><br><span class="line">          <span class="number">-0.1491</span>, <span class="number">-0.1596</span>],</span><br><span class="line">         [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,</span><br><span class="line">           <span class="number">0.0918</span>,  <span class="number">0.0517</span>,  <span class="number">0.0197</span>,  <span class="number">0.1431</span>, <span class="number">-0.0005</span>,  <span class="number">0.2929</span>, <span class="number">-0.4016</span>,</span><br><span class="line">          <span class="number">-0.1401</span>, <span class="number">-0.1218</span>],</span><br><span class="line">         [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,</span><br><span class="line">           <span class="number">0.1487</span>,  <span class="number">0.0439</span>,  <span class="number">0.0172</span>,  <span class="number">0.1198</span>, <span class="number">-0.0221</span>,  <span class="number">0.2856</span>, <span class="number">-0.5105</span>,</span><br><span class="line">          <span class="number">-0.1089</span>, <span class="number">-0.0852</span>],</span><br><span class="line">         [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,</span><br><span class="line">           <span class="number">0.1887</span>,  <span class="number">0.0194</span>,  <span class="number">0.0129</span>,  <span class="number">0.0852</span>, <span class="number">-0.0521</span>,  <span class="number">0.2364</span>, <span class="number">-0.5524</span>,</span><br><span class="line">          <span class="number">-0.0720</span>, <span class="number">-0.0538</span>],</span><br><span class="line">         [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,</span><br><span class="line">           <span class="number">0.2141</span>, <span class="number">-0.0274</span>,  <span class="number">0.0076</span>,  <span class="number">0.0448</span>, <span class="number">-0.0817</span>,  <span class="number">0.1408</span>, <span class="number">-0.4683</span>,</span><br><span class="line">          <span class="number">-0.0382</span>, <span class="number">-0.0285</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.1019</span>,  <span class="number">0.1606</span>, <span class="number">-0.0482</span>, <span class="number">-0.0519</span>, <span class="number">-0.1932</span>,  <span class="number">0.2632</span>, <span class="number">-0.0423</span>,</span><br><span class="line">           <span class="number">0.0774</span>,  <span class="number">0.0434</span>,  <span class="number">0.0142</span>,  <span class="number">0.0915</span>, <span class="number">-0.0455</span>,  <span class="number">0.2970</span>, <span class="number">-0.6665</span>,</span><br><span class="line">          <span class="number">-0.0578</span>, <span class="number">-0.0521</span>],</span><br><span class="line">         [<span class="number">-0.1855</span>,  <span class="number">0.2300</span>, <span class="number">-0.0625</span>, <span class="number">-0.0508</span>, <span class="number">-0.2488</span>,  <span class="number">0.2741</span>, <span class="number">-0.0638</span>,</span><br><span class="line">           <span class="number">0.1441</span>,  <span class="number">0.0239</span>,  <span class="number">0.0137</span>,  <span class="number">0.0786</span>, <span class="number">-0.0524</span>,  <span class="number">0.2311</span>, <span class="number">-0.5685</span>,</span><br><span class="line">          <span class="number">-0.0673</span>, <span class="number">-0.0485</span>],</span><br><span class="line">         [<span class="number">-0.1346</span>,  <span class="number">0.3093</span>, <span class="number">-0.0775</span>, <span class="number">-0.0678</span>, <span class="number">-0.2616</span>,  <span class="number">0.2946</span>, <span class="number">-0.1167</span>,</span><br><span class="line">           <span class="number">0.1699</span>, <span class="number">-0.0280</span>,  <span class="number">0.0067</span>,  <span class="number">0.0333</span>, <span class="number">-0.0924</span>,  <span class="number">0.1308</span>, <span class="number">-0.5332</span>,</span><br><span class="line">          <span class="number">-0.0237</span>, <span class="number">-0.0201</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br><span class="line">padded_output shape</span><br><span class="line"> torch.Size([<span class="number">3</span>, <span class="number">10</span>, <span class="number">16</span>])</span><br><span class="line">length</span><br><span class="line"> tensor([<span class="number">10</span>,  <span class="number">5</span>,  <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>是不是又回来了？16是cell_size*2, 因为是双向。</p><p>可以看到，没参与计算后来被补上来的部分都成为了0！</p><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line">mport os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> util.embedding_util <span class="keyword">import</span> get_embedding</span><br><span class="line"><span class="keyword">from</span> util.plot_util <span class="keyword">import</span> loss_acc_plot</span><br><span class="line"><span class="keyword">from</span> util.lr_util <span class="keyword">import</span> lr_update</span><br><span class="line"><span class="keyword">import</span> config.lstm_config <span class="keyword">as</span> config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BILSTM</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cell_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 checkpoint_dir,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                 model_type,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word2id,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedding_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 keep_prob)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line">        self.history = &#123;&#125;</span><br><span class="line">        self.keep_prob = keep_prob</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding layer</span></span><br><span class="line">        weights = get_embedding(model_type=model_type,</span><br><span class="line">                                word2id=word2id,</span><br><span class="line">                                embedding_dim=embedding_dim)</span><br><span class="line">        <span class="keyword">if</span> model_type == <span class="string">'static'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'non-static'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'rand'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'multichannel'</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'unknown model type'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># BILSTM layer</span></span><br><span class="line">        self.fw_cell = tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">            tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=<span class="number">0.7</span>)</span><br><span class="line">        self.bw_cell = tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">            tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">        self.Dense = tf.layers.Dense(units=num_classes, activation=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, seq_length, training)</span>:</span></span><br><span class="line">        embedded_words = self.embeddings(inputs)</span><br><span class="line">        outputs, final_state = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">            self.fw_cell,</span><br><span class="line">            self.bw_cell,</span><br><span class="line">            inputs=embedded_words,</span><br><span class="line">            sequence_length=seq_length,</span><br><span class="line">            dtype=tf.float32,</span><br><span class="line">            time_major=<span class="keyword">False</span>)</span><br><span class="line">        outputs = tf.concat(outputs, axis=<span class="number">2</span>)</span><br><span class="line">        final_output = outputs[<span class="number">-1</span>]</span><br><span class="line">        logits = self.Dense(final_output)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        preds = self.call(inputs, seq_length, training)</span><br><span class="line">        <span class="comment"># L2正则化</span></span><br><span class="line">        loss_L2 = tf.add_n([tf.nn.l2_loss(v)</span><br><span class="line">                            <span class="keyword">for</span> v <span class="keyword">in</span> self.trainable_variables</span><br><span class="line">                            <span class="keyword">if</span> <span class="string">'bias'</span> <span class="keyword">not</span> <span class="keyword">in</span> v.name]) * <span class="number">0.001</span></span><br><span class="line">        loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)</span><br><span class="line">        loss = loss + loss_L2</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grads_fn</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            loss = self.loss_fn(inputs, target, seq_length, training)</span><br><span class="line">        <span class="keyword">return</span> tape.gradient(loss, self.variables)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self, model)</span>:</span></span><br><span class="line">        <span class="string">""" Function to save trained model.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        checkpoint = tf.train.Checkpoint(model=model)</span><br><span class="line">        checkpoint_prefix = os.path.join(self.checkpoint_dir, <span class="string">'ckpt'</span>)</span><br><span class="line">        checkpoint.save(file_prefix=checkpoint_prefix)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Run the model once to initialize variables</span></span><br><span class="line">        dummy_input = tf.constant(tf.zeros((<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">        dummy_length = tf.constant(<span class="number">1</span>, shape=(<span class="number">1</span>,))</span><br><span class="line">        self(dummy_input, dummy_length, <span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># Restore the variables of the model</span></span><br><span class="line">        saver = tf.contrib.Saver(self.variables)</span><br><span class="line">        saver.restore(tf.train.latest_checkpoint</span><br><span class="line">                      (self.checkpoint_directory))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        y = self.call(inputs, seq_length, training)</span><br><span class="line">        y_pred = tf.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        correct = tf.where(tf.equal(y_pred, target)).numpy().shape[<span class="number">0</span>]</span><br><span class="line">        total = target.numpy().shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> correct/total</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, training_data, eval_data, pbar, num_epochs=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            early_stopping_rounds=<span class="number">5</span>, verbose=<span class="number">1</span>, train_from_scratch=True)</span>:</span></span><br><span class="line">        <span class="string">"""train the model"""</span></span><br><span class="line">        <span class="keyword">if</span> train_from_scratch <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            self.restore_model()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize best loss. This variable will store the lowest loss on the</span></span><br><span class="line">        <span class="comment"># eval dataset.</span></span><br><span class="line">        best_loss = <span class="number">2018</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize classes to update the mean loss of train and eval</span></span><br><span class="line">        train_loss = []</span><br><span class="line">        eval_loss = []</span><br><span class="line">        train_accuracy = []</span><br><span class="line">        eval_accuracy = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize dictionary to store the loss history</span></span><br><span class="line">        self.history[<span class="string">'train_loss'</span>] = []</span><br><span class="line">        self.history[<span class="string">'eval_loss'</span>] = []</span><br><span class="line">        self.history[<span class="string">'train_accuracy'</span>] = []</span><br><span class="line">        self.history[<span class="string">'eval_accuracy'</span>] = []</span><br><span class="line"></span><br><span class="line">        count = early_stopping_rounds</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Begin training</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># 在每个epoch训练之初初始化optimizer，决定是否使用学习率衰减</span></span><br><span class="line">            learning_rate = lr_update(i+<span class="number">1</span>, mode=config.lr_mode)</span><br><span class="line">            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Training with gradient descent</span></span><br><span class="line">            start = time.time()</span><br><span class="line">            <span class="keyword">for</span> index, (sequence, label, seq_length) <span class="keyword">in</span> enumerate(training_data):</span><br><span class="line">                <span class="comment"># cpu需要类型转换，不然会报错：Could not find valid device</span></span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                grads = self.grads_fn(sequence, label, seq_length, training=<span class="keyword">True</span>)</span><br><span class="line">                optimizer.apply_gradients(zip(grads, self.variables))</span><br><span class="line">                pbar.show(index, use_time=time.time()-start)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the loss on the training data after one epoch</span></span><br><span class="line">            <span class="keyword">for</span> sequence, label, seq_length <span class="keyword">in</span> training_data:</span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                train_los = self.loss_fn(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                train_acc = self.get_accuracy(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                train_loss.append(train_los)</span><br><span class="line">                train_accuracy.append(train_acc)</span><br><span class="line">            self.history[<span class="string">'train_loss'</span>].append(np.mean(train_loss))</span><br><span class="line">            self.history[<span class="string">'train_accuracy'</span>].append(np.mean(train_accuracy))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the loss on the eval data after one epoch</span></span><br><span class="line">            <span class="keyword">for</span> sequence, label, seq_length <span class="keyword">in</span> eval_data:</span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                eval_los = self.loss_fn(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                eval_acc = self.get_accuracy(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                eval_loss.append(eval_los)</span><br><span class="line">                eval_accuracy.append(eval_acc)</span><br><span class="line">            self.history[<span class="string">'eval_loss'</span>].append(np.mean(eval_loss))</span><br><span class="line">            self.history[<span class="string">'eval_accuracy'</span>].append(np.mean(eval_accuracy))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print train and eval losses</span></span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span>) | ((i + <span class="number">1</span>) % verbose == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">'Epoch %d - train_loss: %4f - eval_loss: %4f - train_acc:%4f - eval_acc:%4f'</span></span><br><span class="line">                      % (i + <span class="number">1</span>,</span><br><span class="line">                         self.history[<span class="string">'train_loss'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'train_accuracy'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'eval_accuracy'</span>][<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check for early stopping</span></span><br><span class="line">            <span class="keyword">if</span> self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>] &lt; best_loss:</span><br><span class="line">                best_loss = self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>]</span><br><span class="line">                count = early_stopping_rounds</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 画出loss_acc曲线</span></span><br><span class="line">        loss_acc_plot(history=self.history)</span><br></pre></td></tr></table></figure><h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_packed_sequence, pack_padded_sequence</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config.config <span class="keyword">as</span> config</span><br><span class="line"><span class="keyword">from</span> util.embedding_util <span class="keyword">import</span> get_embedding</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">2018</span>)</span><br><span class="line">torch.cuda.manual_seed(<span class="number">2018</span>)</span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">2018</span>)</span><br><span class="line">np.random.seed(<span class="number">2018</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICE"</span>] = <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word_embedding_dimension,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, bi_flag,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layer,</span></span></span><br><span class="line"><span class="function"><span class="params">                 labels,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cell_type,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout,</span></span></span><br><span class="line"><span class="function"><span class="params">                 checkpoint_dir)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.num_label = len(labels)</span><br><span class="line">        self.num_layer = num_layer</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            self.device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, word_embedding_dimension)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            p.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.embedding.weight.data.copy_(torch.from_numpy(get_embedding(vocab_size, word_embedding_dimension)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cell_type == <span class="string">"LSTM"</span>:</span><br><span class="line">            self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension,</span><br><span class="line">                                    hidden_size=hidden_size,</span><br><span class="line">                                    num_layers=num_layer,</span><br><span class="line">                                    batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                    dropout=dropout,</span><br><span class="line">                                    bidirectional=bi_flag)</span><br><span class="line">        <span class="keyword">elif</span> cell_type == <span class="string">"GRU"</span>:</span><br><span class="line">            self.rnn_cell = nn.GRU(input_size=word_embedding_dimension,</span><br><span class="line">                                   hidden_size=hidden_size,</span><br><span class="line">                                   num_layers=num_layer,</span><br><span class="line">                                   batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                   dropout=dropout,</span><br><span class="line">                                   bidirectional=bi_flag)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"RNN: Unknown rnn cell type"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否双向</span></span><br><span class="line">        self.bi_num = <span class="number">2</span> <span class="keyword">if</span> bi_flag <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(hidden_size*self.bi_num, self.num_label)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">        batch_size = inputs.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 初始化态h和C,默认为zeros</span></span><br><span class="line">        h_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float()</span><br><span class="line">        c_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float()</span><br><span class="line"></span><br><span class="line">        embeddings = self.embedding(inputs, length)  <span class="comment"># (batch_size, time_steps, embedding_dim)</span></span><br><span class="line">        <span class="comment"># 去除padding元素</span></span><br><span class="line">        <span class="comment"># embeddings_packed: (batch_size*time_steps, embedding_dim)</span></span><br><span class="line">        embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0))</span><br><span class="line">        <span class="comment"># padded_output: (batch_size, time_steps, hidden_size * bi_num)</span></span><br><span class="line">        <span class="comment"># h_n|c_n: (num_layer*bi_num, batch_size, hidden_size)</span></span><br><span class="line">        padded_output, _ = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 取最后一个有效输出作为最终输出（0为无效输出）</span></span><br><span class="line">        last_output = padded_output[torch.LongTensor(range(batch_size)), length]</span><br><span class="line">        last_output = F.dropout(last_output, p=self.dropout, training=self.training)</span><br><span class="line">        output = self.linear(last_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.load_state_dict(torch.load(self.checkpoint_dir))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self)</span>:</span></span><br><span class="line">        torch.save(self.state_dict(), self.checkpoint_dir)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, y_pred, y_true)</span>:</span></span><br><span class="line">        _, y_pred = torch.max(y_pred.data, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> config.use_cuda:</span><br><span class="line">            y_true = y_true.cpu().numpy()</span><br><span class="line">            y_pred = y_pred.cpu().numpy()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_true = y_true.numpy()</span><br><span class="line">            y_pred = y_pred.numpy()</span><br><span class="line">        f1 = f1_score(y_true, y_pred, labels=self.labels, average=<span class="string">"macro"</span>)</span><br><span class="line">        correct = np.sum((y_true==y_pred).astype(int))</span><br><span class="line">        acc = correct/y_pred.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> (acc, f1)</span><br></pre></td></tr></table></figure><p>各位晚安~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;本文介绍pytorch RNN网络搭建，主要包括LSTM和G
      
    
    </summary>
    
      <category term="pytorch" scheme="http://state-of-art.top/categories/pytorch/"/>
    
    
      <category term="pytorch，lstm，nlp" scheme="http://state-of-art.top/tags/pytorch%EF%BC%8Clstm%EF%BC%8Cnlp/"/>
    
  </entry>
  
  <entry>
    <title>torchtext读取文本数据集</title>
    <link href="http://state-of-art.top/2018/11/28/torchtext%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://state-of-art.top/2018/11/28/torchtext读取文本数据集/</id>
    <published>2018-11-28T15:21:08.000Z</published>
    <updated>2018-11-28T14:15:32.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文主要介绍如何使用Torchtext读取文本数据集。</p><p>Torchtext是非官方的、一种为pytorch提供文本数据处理能力的库， 类似于图像处理库Torchvision。</p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><ol><li>下载地址：<a href="https://github.com/text" target="_blank" rel="noopener">https://github.com/text</a></li><li>安装：pip install text-master.zip</li><li>测试安装是否成功： import torchtext</li></ol><h2 id="How-To-Use"><a href="#How-To-Use" class="headerlink" title="How To Use"></a>How To Use</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/20181128/1.png" alt="image text"></p><p>先上一张图。使用tortext的目的是将文本转换成Batch，方便后面训练模型时使用。过程如下:</p><ul><li>使用Field对象进行文本预处理， 生成example</li><li>使用Dataset类生成数据集dataset</li><li>使用Iterator生成迭代器</li></ul><p>从图中还可以看到，torchtext可以生成词典vocab和词向量embedding，但个人比较喜欢将这两步放在数据预处理和模型里面进行，所以这两个功能不在本文之列。</p><h3 id="常用的类"><a href="#常用的类" class="headerlink" title="常用的类"></a>常用的类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, Example, TabularDataset</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> BucketIterator</span><br></pre></td></tr></table></figure><p>Field：用来定义字段以及文本预处理方法</p><p>Example: 用来表示一个样本，通常为“数据+标签”</p><p>TabularDataset: 用来从文件中读取数据，生成Dataset， Dataset是Example实例的集合</p><p>BucketIterator：迭代器，用来生成batch， 类似的有Iterator，Buckeiterator的功能较强大点，支持排序，动态padding等</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>见我上篇博文&lt;文本预处理&gt;。使用生成的train.tsv和valid.tsv。</p><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><h4 id="创建Field对象"><a href="#创建Field对象" class="headerlink" title="创建Field对象"></a>创建Field对象</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">x_tokenize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 如果加载进来的是已经转成id的文本</span></span><br><span class="line">    <span class="comment"># 此处必须将字符串转换成整型</span></span><br><span class="line">    <span class="comment"># 否则必须将use_vocab设为True</span></span><br><span class="line">    <span class="keyword">return</span> [int(c) <span class="keyword">for</span> c <span class="keyword">in</span> x.split()]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_tokenize</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(y)</span><br><span class="line"></span><br><span class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                     use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                     fix_length=self.fix_length, </span><br><span class="line">                     eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                     include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line">LABEL = Field(sequential=<span class="keyword">False</span>, tokenize=y_tokenize, use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><ul><li>sequential     类型boolean,  作用：是否为序列，一般文本都为True，标签为False</li><li>tokenize    类型: function， 作用: 文本处理，默认为str.split(), 这里对x和y分别自定义了处理函数。</li><li>use_vocab： 类型: boolean， 作用：是否建立词典</li><li>batch_first：类型: boolean， 作用：为True则返回Batch维度为(batch_size， 文本长度), False 则相反</li><li>fix_length：类型: int, 作用：固定文本的长度，长则截断，短则padding，可认为是静态padding；为None则按每个Batch内的最大长度进行动态padding。</li><li>eos_token：类型：str, 作用: 句子结束字符</li><li>init_token：类型：str, 作用: 句子开始字符</li><li>include_lengths：类型: boolean， 作用：是否返回句子的原始长度，一般为True，方便RNN使用。</li><li>pad_token：padding的字符，默认为”<pad>“, 这里因为原始数据已经转成了int类型，所以使用0。注意这里的pad_token要和你的词典vocab里的“<pad>”的Id保持一致，否则会影响后面词向量的读取。</pad></pad></li></ul><h4 id="读取文件生成数据集"><a href="#读取文件生成数据集" class="headerlink" title="读取文件生成数据集"></a>读取文件生成数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fields = [</span><br><span class="line">    (<span class="string">"label"</span>, LABEL), (<span class="string">"text"</span>, TEXT)]</span><br><span class="line"></span><br><span class="line">train, valid = TabularDataset.splits(</span><br><span class="line">    path=config.ROOT_DIR,</span><br><span class="line">    train=self.train_path, validation=self.valid_path,</span><br><span class="line">    format=<span class="string">'tsv'</span>,</span><br><span class="line">    skip_header=<span class="keyword">False</span>,</span><br><span class="line">    fields=fields)</span><br><span class="line"><span class="keyword">return</span> train, valid</span><br></pre></td></tr></table></figure><h4 id="生成迭代器"><a href="#生成迭代器" class="headerlink" title="生成迭代器"></a>生成迭代器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_iter, val_iter = BucketIterator.splits((train, valid),</span><br><span class="line">                                             batch_sizes=(self.batch_size, self.batch_size),</span><br><span class="line">                                             device = torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">                                             sort_key=<span class="keyword">lambda</span> x: len(x.text), <span class="comment"># field sorted by len</span></span><br><span class="line">                                             sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                             repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>这里要注意的是sort_with_batch要设置为True，并指定排序的key为文本长度，方便后面pytorch RNN进行pack和pad。</p><p>我们来看下train_iter和val_iter里放了什么东西。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=<span class="number">1</span>, fix_length=<span class="keyword">None</span>)</span><br><span class="line">train, valid  = bi.create_dataset()</span><br><span class="line">train_iter, valid_iter = bi.get_iterator(train, valid)</span><br><span class="line">batch = next(iter(train_iter))</span><br><span class="line">print(train_iter)</span><br><span class="line">print(<span class="string">'batch:\n'</span>, batch)</span><br><span class="line">print(<span class="string">'batch_text:\n'</span>, batch.text)</span><br><span class="line">print(<span class="string">'batch_label:\n'</span>, batch.label)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;torchtext.data.iterator.BucketIterator object at <span class="number">0x7f04a9d845f8</span>&gt;</span><br><span class="line">batch:</span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">1</span>]</span><br><span class="line">[.label]:[torch.LongTensor of size <span class="number">1</span>]</span><br><span class="line">[.text]:(<span class="string">'[torch.LongTensor of size 1x125]'</span>, <span class="string">'[torch.LongTensor of size 1]'</span>)</span><br><span class="line">batch_text:</span><br><span class="line"> (tensor([[<span class="number">11149</span>,  <span class="number">7772</span>, <span class="number">13752</span>, <span class="number">13743</span>, <span class="number">13773</span>, <span class="number">13793</span>, <span class="number">13791</span>, <span class="number">13591</span>, <span class="number">12478</span>, <span class="number">13759</span>,</span><br><span class="line">         <span class="number">13783</span>, <span class="number">13492</span>, <span class="number">13793</span>, <span class="number">13745</span>, <span class="number">13754</span>, <span class="number">13612</span>,  <span class="number">7452</span>, <span class="number">12185</span>, <span class="number">13789</span>, <span class="number">13784</span>,</span><br><span class="line">         <span class="number">13765</span>, <span class="number">12451</span>, <span class="number">12112</span>, <span class="number">13620</span>, <span class="number">12240</span>, <span class="number">13073</span>, <span class="number">13790</span>, <span class="number">13738</span>, <span class="number">13637</span>, <span class="number">13759</span>,</span><br><span class="line">         <span class="number">13776</span>, <span class="number">13793</span>, <span class="number">13739</span>, <span class="number">13783</span>, <span class="number">13787</span>, <span class="number">13793</span>, <span class="number">12702</span>, <span class="number">13790</span>, <span class="number">13698</span>, <span class="number">13774</span>,</span><br><span class="line">         <span class="number">13792</span>, <span class="number">13768</span>, <span class="number">13715</span>, <span class="number">13641</span>, <span class="number">13761</span>, <span class="number">13713</span>, <span class="number">13682</span>, <span class="number">13712</span>, <span class="number">13786</span>, <span class="number">13749</span>,</span><br><span class="line">         <span class="number">13097</span>, <span class="number">13734</span>, <span class="number">13702</span>, <span class="number">13735</span>, <span class="number">13257</span>, <span class="number">13642</span>, <span class="number">13700</span>, <span class="number">13793</span>, <span class="number">13684</span>, <span class="number">13755</span>,</span><br><span class="line">         <span class="number">13488</span>, <span class="number">13789</span>, <span class="number">13750</span>, <span class="number">13484</span>, <span class="number">13494</span>, <span class="number">13793</span>, <span class="number">13624</span>, <span class="number">13670</span>, <span class="number">13786</span>, <span class="number">13655</span>,</span><br><span class="line">         <span class="number">13768</span>, <span class="number">13687</span>, <span class="number">13774</span>, <span class="number">13792</span>, <span class="number">13791</span>, <span class="number">13591</span>, <span class="number">13546</span>, <span class="number">13777</span>, <span class="number">13658</span>, <span class="number">13740</span>,</span><br><span class="line">         <span class="number">13577</span>, <span class="number">13790</span>, <span class="number">13684</span>, <span class="number">13755</span>, <span class="number">13793</span>, <span class="number">13572</span>, <span class="number">12891</span>, <span class="number">13793</span>, <span class="number">13368</span>, <span class="number">13713</span>,</span><br><span class="line">         <span class="number">13682</span>, <span class="number">13712</span>, <span class="number">13786</span>, <span class="number">13786</span>, <span class="number">13642</span>, <span class="number">13700</span>, <span class="number">13793</span>, <span class="number">13429</span>, <span class="number">13520</span>, <span class="number">13613</span>,</span><br><span class="line">         <span class="number">13792</span>, <span class="number">13368</span>, <span class="number">13790</span>, <span class="number">13750</span>, <span class="number">13699</span>, <span class="number">13764</span>, <span class="number">13590</span>, <span class="number">13675</span>, <span class="number">13742</span>, <span class="number">13691</span>,</span><br><span class="line">         <span class="number">13688</span>, <span class="number">13742</span>, <span class="number">13782</span>, <span class="number">13538</span>, <span class="number">13742</span>, <span class="number">13783</span>, <span class="number">13787</span>, <span class="number">13774</span>, <span class="number">13645</span>, <span class="number">13742</span>,</span><br><span class="line">         <span class="number">13791</span>, <span class="number">13740</span>, <span class="number">13744</span>, <span class="number">13750</span>, <span class="number">13792</span>]]), tensor([<span class="number">125</span>]))</span><br><span class="line">batch_label:</span><br><span class="line"> tensor([<span class="number">11</span>])</span><br></pre></td></tr></table></figure><p>可以看到batch有两个属性，分别为label和text, text是一个元组，第一个元素为文本，第二个元素为文本原始长度（这里因为我们在定义TEXT时使用了include_lengths=True，否则这里只返回文本）， label则是标签。</p><p>这里为了方便展示只使用了一个batch，返回的batch维度为（batch_size * length）, 数据格式为LongTensor。如果想看动态padding的效果，可多取几个batch，会发现他们是按照长度进行排序，并且是以0进行padding的。</p><h4 id="对Batch包装一下，方便调用"><a href="#对Batch包装一下，方便调用" class="headerlink" title="对Batch包装一下，方便调用"></a>对Batch包装一下，方便调用</h4><p>通过以上步骤，我们能够得到一个batch。但是很快就发现有个不太方便的地方。我们只能通过batch的属性，即自定义的字段名称，如text和label，来访问数据。这样的话在训练时我们只能这样操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_iter:</span><br><span class="line">        inputs = batch.text[<span class="number">0</span>]</span><br><span class="line">        label = batch.label</span><br><span class="line">        length = batch.text[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>万一这个字段改了，还要去改训练的代码，很麻烦，关键是显得很LOW，姿势不对。</p><p>怎么办呢？</p><p>我们对获得的iter进行包装一下，就可以避免这个问题了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""对batch做个包装，方便调用，可选择性使用"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                temp = [getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars]</span><br><span class="line">                label = torch.cat(temp, dim=<span class="number">1</span>).long()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'BatchWrapper: invalid label'</span>)</span><br><span class="line">            text = x[<span class="number">0</span>]</span><br><span class="line">            length = x[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> (text, label, length)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br></pre></td></tr></table></figure><p>我们这样使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br></pre></td></tr></table></figure><p>这样你就会发现batch不再有text和label属性了，而是一个三元组（text， label， length），调用时</p><p>就可以采用如下优雅一点的姿势：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> inputs, label, length <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>data_loader.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""将id格式的输入转换成dataset，并做动态padding"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, TabularDataset</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> BucketIterator</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">x_tokenize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 如果加载进来的是已经转成id的文本</span></span><br><span class="line">    <span class="comment"># 此处必须将字符串转换成整型</span></span><br><span class="line">    <span class="keyword">return</span> [int(c) <span class="keyword">for</span> c <span class="keyword">in</span> x.split()]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_tokenize</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(y)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchIterator</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_path, valid_path,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size, fix_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 x_var=<span class="string">"text"</span>, y_var=[<span class="string">"label"</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 format=<span class="string">'tsv'</span>)</span>:</span></span><br><span class="line">        self.train_path = train_path</span><br><span class="line">        self.valid_path = valid_path</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.fix_length = fix_length</span><br><span class="line">        self.format = format</span><br><span class="line">        self.x_var = x_var</span><br><span class="line">        self.y_vars = y_var</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line">        TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                     use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                     fix_length=self.fix_length,   <span class="comment">#  如需静态padding,则设置fix_length, 但要注意要大于文本最大长度</span></span><br><span class="line">                     eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                     include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line">        LABEL = Field(sequential=<span class="keyword">False</span>, tokenize=y_tokenize, use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        fields = [</span><br><span class="line">            (<span class="string">"label"</span>, LABEL), (<span class="string">"text"</span>, TEXT)]</span><br><span class="line"></span><br><span class="line">        train, valid = TabularDataset.splits(</span><br><span class="line">            path=config.ROOT_DIR,</span><br><span class="line">            train=self.train_path, validation=self.valid_path,</span><br><span class="line">            format=<span class="string">'tsv'</span>,</span><br><span class="line">            skip_header=<span class="keyword">False</span>,</span><br><span class="line">            fields=fields)</span><br><span class="line">        <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_iterator</span><span class="params">(self, train, valid)</span>:</span></span><br><span class="line">        train_iter, val_iter = BucketIterator.splits((train, valid),</span><br><span class="line">                                                     batch_sizes=(self.batch_size, self.batch_size),</span><br><span class="line">                                                     device = torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">                                                     sort_key=<span class="keyword">lambda</span> x: len(x.text), <span class="comment"># field sorted by len</span></span><br><span class="line">                                                     sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                                     repeat=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">        train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">        val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">        <span class="comment">### batch = iter(train_iter)</span></span><br><span class="line">        <span class="comment">### batch： ((text, length), y)</span></span><br><span class="line">        <span class="keyword">return</span> train_iter, val_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""对batch做个包装，方便调用，可选择性使用"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                temp = [getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars]</span><br><span class="line">                y = torch.cat(temp, dim=<span class="number">1</span>).long()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'BatchWrapper: invalid label'</span>)</span><br><span class="line">            text = x[<span class="number">0</span>]</span><br><span class="line">            length = x[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> (text, y, length)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=<span class="number">1</span>, fix_length=<span class="keyword">None</span>)</span><br><span class="line">    train, valid  = bi.create_dataset()</span><br><span class="line">    train_iter, valid_iter = bi.get_iterator(train, valid)</span><br><span class="line">    batch = next(iter(train_iter))</span><br><span class="line">    print(train_iter)</span><br><span class="line">    print(<span class="string">'batch:\n'</span>, batch)</span><br><span class="line">    print(<span class="string">'batch_text:\n'</span>, batch.text)</span><br><span class="line">    print(<span class="string">'batch_label:\n'</span>, batch.label)</span><br></pre></td></tr></table></figure><p>config.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_FILE = <span class="string">'outputs/intermediate/train.tsv'</span></span><br><span class="line">VALID_FILE = <span class="string">'outputs/intermediate/valid.tsv'</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;本文主要介绍如何使用Torchtext读取文本数据集。&lt;/p
      
    
    </summary>
    
      <category term="NLP" scheme="http://state-of-art.top/categories/NLP/"/>
    
    
      <category term="torchtext，pytorch" scheme="http://state-of-art.top/tags/torchtext%EF%BC%8Cpytorch/"/>
    
  </entry>
  
  <entry>
    <title>文本预处理</title>
    <link href="http://state-of-art.top/2018/11/28/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://state-of-art.top/2018/11/28/文本预处理/</id>
    <published>2018-11-28T15:21:08.000Z</published>
    <updated>2018-11-28T14:14:22.908Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本文介绍如何利用有限内存进行大规模数据处理，主要包括：</p><ul><li>建立词典</li><li>将单词转换为id</li><li>训练集验证集切分</li></ul><h2 id="How-To-Do-IT"><a href="#How-To-Do-IT" class="headerlink" title="How To Do IT"></a>How To Do IT</h2><h3 id="原始数据集"><a href="#原始数据集" class="headerlink" title="原始数据集"></a>原始数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">11</span>@成都 高新技术 产业 开发区 人民 检察院 指控 ， <span class="number">2015</span>年 <span class="number">3</span> 月 <span class="number">29</span>日 <span class="number">23</span>时 许 ， 被告人 刘某 某 饮 酒后 驾驶川 A ＊ ＊ ＊ <span class="number">84</span> 北京 现代牌 小型 轿车 ， 从 成都市 桐梓林 附近 出发 上 人民 南 路 出 城 ， 当 车 行驶 至 成都 高新区 天府 大道 与 府城 大道 交叉 路口处 时 ，  公诉 机关 认为 ， 被告人 刘 某某 在 道路 上 醉 酒 驾驶 机动车 ， 危害 公共 安全 ， 其 行为 应当 以 ×× 追究 其 刑事 责任 。</span><br><span class="line"></span><br><span class="line"><span class="number">11</span>@黑龙江省 尚志市 人民 检察院 指控 ： ×× <span class="number">2014</span>年 <span class="number">9</span> 月 <span class="number">22</span>日 <span class="number">20</span>时 许 ， 被告人 矫 <span class="number">2</span> 某 在 尚志市 苇河镇 阿里郎歌厅 对面 停放 的 货车 的 副 驾驶 座位 上 ， 将 被害人 李某 甲 的 蓝色 女式 拎 包 盗 走 ， 包 内 有 人民币 <span class="number">57000</span> 元 ， 红色 钱包 一个 ， 农业 银行卡 一 张 ， 身份证 一 张 、 驾驶证 一 本 、 账本 一 册 。 案 发 前 ， 被告人 矫 <span class="number">2</span> 某 将 盗走 的 财物 返还 被害人 。  在 ×× 到 五 年 幅度 内 量刑 ， 并 处 罚金 ； 对 所 犯 的 ×× 在 ×× 到 六 个 月 幅度 内 量刑 ， 并 处 罚金 。 针对 上述 指控 ， 公诉 机关 提供 了 相应 的 证据 。</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这里以<a href="http://cail.cipsc.org.cn/" target="_blank" rel="noopener">法研杯</a>比赛的文本数据集为例。格式为 标签@文本</p><p>其中，文本已经过分词处理，使用空格分隔。</p><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent_label_split</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    句子处理成单词</span></span><br><span class="line"><span class="string">    :param line: 原始行</span></span><br><span class="line"><span class="string">    :return: 单词， 标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    line = line.strip(<span class="string">'\n'</span>).split(<span class="string">'@'</span>)</span><br><span class="line">    label = line[<span class="number">0</span>]</span><br><span class="line">    sent = line[<span class="number">1</span>].split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">return</span> sent, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_to_id</span><span class="params">(word, word2id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单词--&gt;ID</span></span><br><span class="line"><span class="string">    :param word: 单词</span></span><br><span class="line"><span class="string">    :param word2id: word2id @type: dict</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> word2id[word] <span class="keyword">if</span> word <span class="keyword">in</span> word2id <span class="keyword">else</span> word2id[<span class="string">'unk'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_vocab</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_word_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    建立词典</span></span><br><span class="line"><span class="string">    :param vocab_size: 词典大小</span></span><br><span class="line"><span class="string">    :param min_freq: 最小词频限制</span></span><br><span class="line"><span class="string">    :param stop_list: 停用词 @type：file_path</span></span><br><span class="line"><span class="string">    :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试</span></span><br><span class="line"><span class="string">    :return: word2id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    count = Counter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        logger.info(<span class="string">'Building vocab'</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'Build vocab'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            count.update(words)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> stop_word_list:</span><br><span class="line">        stop_list = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fr):</span><br><span class="line">                    word = line.strip(<span class="string">'\n'</span>)</span><br><span class="line">                    <span class="keyword">if</span> stop_list.get(word) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                        stop_list[word] = i</span><br><span class="line">        count = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> count.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> stop_list&#125;</span><br><span class="line">    count = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 词典</span></span><br><span class="line">    vocab = [w[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> count <span class="keyword">if</span> w[<span class="number">1</span>] &gt;= min_freq]</span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; len(vocab):</span><br><span class="line">        vocab = vocab[:vocab_size]</span><br><span class="line">    vocab = config.flag_words + vocab</span><br><span class="line">    logger.info(<span class="string">'vocab_size is %d'</span>%len(vocab))</span><br><span class="line">    <span class="comment"># 词典到编号的映射</span></span><br><span class="line">    word2id = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> zip(vocab, range(<span class="number">0</span>, len(vocab)))&#125;</span><br><span class="line">    <span class="keyword">assert</span> word2id[<span class="string">'&lt;pad&gt;'</span>] == <span class="number">0</span>, <span class="string">"ValueError: '&lt;pad&gt;' id is not 0"</span></span><br><span class="line">    print(word2id)</span><br><span class="line">    <span class="keyword">with</span> open(config.WORD2ID_FILE, <span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        pickle.dump(word2id, fw)</span><br><span class="line">    <span class="keyword">return</span> word2id</span><br></pre></td></tr></table></figure><h3 id="文本映射到Id"><a href="#文本映射到Id" class="headerlink" title="文本映射到Id"></a>文本映射到Id</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text2id</span><span class="params">(word2id, maxlen=None, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集文本转ID</span></span><br><span class="line"><span class="string">    :param valid_size: 验证集大小</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))</span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Text to id file existed'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    logger.info(<span class="string">'Text to id'</span>)</span><br><span class="line">    sentences, labels, lengths = [], [], []</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'text_to_id'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            sent = [word_to_id(word=word, word2id=word2id) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">            <span class="keyword">if</span> maxlen:</span><br><span class="line">                sent = sent[:maxlen]</span><br><span class="line">            length = len(sent)</span><br><span class="line">            sentences.append(sent)</span><br><span class="line">            labels.append(label)</span><br><span class="line">            lengths.append(length)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    train, valid = train_val_split(sentences, labels,</span><br><span class="line">                                   valid_size=valid_size,</span><br><span class="line">                                   random_state=random_state,</span><br><span class="line">                                   shuffle=shuffle)</span><br><span class="line">    <span class="keyword">del</span> sentences, labels, lengths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing train to file done'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.VALID_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing valid to file done'</span>)</span><br></pre></td></tr></table></figure><h3 id="训练集验证集分割"><a href="#训练集验证集分割" class="headerlink" title="训练集验证集分割"></a>训练集验证集分割</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_split</span><span class="params">(X, y, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集验证集分割</span></span><br><span class="line"><span class="string">    :param X: sentences</span></span><br><span class="line"><span class="string">    :param y: labels</span></span><br><span class="line"><span class="string">    :param random_state: 随机种子</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger.info(<span class="string">'train val split'</span>)</span><br><span class="line">    data = [(data_x, data_y) <span class="keyword">for</span> data_x, data_y <span class="keyword">in</span> zip(X, y)]</span><br><span class="line">    N = len(data)</span><br><span class="line">    test_size = int(N * valid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.seed(random_state)</span><br><span class="line">        random.shuffle(data)</span><br><span class="line"></span><br><span class="line">    valid = data[:test_size]</span><br><span class="line">    train = data[test_size:]</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">from</span> Logginger <span class="keyword">import</span> init_logger</span><br><span class="line"></span><br><span class="line">logger = init_logger(<span class="string">"torch"</span>, logging_path=config.LOG_PATH)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent_label_split</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    句子处理成单词</span></span><br><span class="line"><span class="string">    :param line: 原始行</span></span><br><span class="line"><span class="string">    :return: 单词， 标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    line = line.strip(<span class="string">'\n'</span>).split(<span class="string">'@'</span>)</span><br><span class="line">    label = line[<span class="number">0</span>]</span><br><span class="line">    sent = line[<span class="number">1</span>].split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">return</span> sent, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_to_id</span><span class="params">(word, word2id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单词--&gt;ID</span></span><br><span class="line"><span class="string">    :param word: 单词</span></span><br><span class="line"><span class="string">    :param word2id: word2id @type: dict</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> word2id[word] <span class="keyword">if</span> word <span class="keyword">in</span> word2id <span class="keyword">else</span> word2id[<span class="string">'unk'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_vocab</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_word_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    建立词典</span></span><br><span class="line"><span class="string">    :param vocab_size: 词典大小</span></span><br><span class="line"><span class="string">    :param min_freq: 最小词频限制</span></span><br><span class="line"><span class="string">    :param stop_list: 停用词 @type：file_path</span></span><br><span class="line"><span class="string">    :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试</span></span><br><span class="line"><span class="string">    :return: word2id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    count = Counter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        logger.info(<span class="string">'Building vocab'</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'Build vocab'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            count.update(words)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> stop_word_list:</span><br><span class="line">        stop_list = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fr):</span><br><span class="line">                    word = line.strip(<span class="string">'\n'</span>)</span><br><span class="line">                    <span class="keyword">if</span> stop_list.get(word) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                        stop_list[word] = i</span><br><span class="line">        count = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> count.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> stop_list&#125;</span><br><span class="line">    count = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 词典</span></span><br><span class="line">    vocab = [w[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> count <span class="keyword">if</span> w[<span class="number">1</span>] &gt;= min_freq]</span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; len(vocab):</span><br><span class="line">        vocab = vocab[:vocab_size]</span><br><span class="line">    vocab = config.flag_words + vocab</span><br><span class="line">    logger.info(<span class="string">'vocab_size is %d'</span>%len(vocab))</span><br><span class="line">    <span class="comment"># 词典到编号的映射</span></span><br><span class="line">    word2id = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> zip(vocab, range(<span class="number">0</span>, len(vocab)))&#125;</span><br><span class="line">    <span class="keyword">assert</span> word2id[<span class="string">'&lt;pad&gt;'</span>] == <span class="number">0</span>, <span class="string">"ValueError: '&lt;pad&gt;' id is not 0"</span></span><br><span class="line">    print(word2id)</span><br><span class="line">    <span class="keyword">with</span> open(config.WORD2ID_FILE, <span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        pickle.dump(word2id, fw)</span><br><span class="line">    <span class="keyword">return</span> word2id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_split</span><span class="params">(X, y, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集验证集分割</span></span><br><span class="line"><span class="string">    :param X: sentences</span></span><br><span class="line"><span class="string">    :param y: labels</span></span><br><span class="line"><span class="string">    :param random_state: 随机种子</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger.info(<span class="string">'train val split'</span>)</span><br><span class="line">    data = [(data_x, data_y) <span class="keyword">for</span> data_x, data_y <span class="keyword">in</span> zip(X, y)]</span><br><span class="line">    N = len(data)</span><br><span class="line">    test_size = int(N * valid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.seed(random_state)</span><br><span class="line">        random.shuffle(data)</span><br><span class="line"></span><br><span class="line">    valid = data[:test_size]</span><br><span class="line">    train = data[test_size:]</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text2id</span><span class="params">(word2id, maxlen=None, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集文本转ID</span></span><br><span class="line"><span class="string">    :param valid_size: 验证集大小</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))</span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Text to id file existed'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    logger.info(<span class="string">'Text to id'</span>)</span><br><span class="line">    sentences, labels, lengths = [], [], []</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'text_to_id'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            sent = [word_to_id(word=word, word2id=word2id) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">            <span class="keyword">if</span> maxlen:</span><br><span class="line">                sent = sent[:maxlen]</span><br><span class="line">            length = len(sent)</span><br><span class="line">            sentences.append(sent)</span><br><span class="line">            labels.append(label)</span><br><span class="line">            lengths.append(length)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    train, valid = train_val_split(sentences, labels,</span><br><span class="line">                                   valid_size=valid_size,</span><br><span class="line">                                   random_state=random_state,</span><br><span class="line">                                   shuffle=shuffle)</span><br><span class="line">    <span class="keyword">del</span> sentences, labels, lengths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing train to file done'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.VALID_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing valid to file done'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能整合，提供给外部调用的函数接口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_helper</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="comment"># 判断文件是否已存在</span></span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Word to id file existed'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE), <span class="string">'rb'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">            word2id = pickle.load(fr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        word2id = bulid_vocab(vocab_size=vocab_size, min_freq=min_freq, stop_word_list=stop_list,</span><br><span class="line">                is_debug=is_debug)</span><br><span class="line">    text2id(word2id, valid_size=valid_size, random_state=random_state, shuffle=shuffle, is_debug=is_debug)</span><br></pre></td></tr></table></figure><p>config.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------PATH------------</span></span><br><span class="line">ROOT_DIR = <span class="string">'/home/daizelin/pytorch/'</span></span><br><span class="line">RAW_DATA = <span class="string">'data/data_for_test.csv'</span></span><br><span class="line">TRAIN_FILE = <span class="string">'outputs/intermediate/train.tsv'</span></span><br><span class="line">VALID_FILE = <span class="string">'outputs/intermediate/valid.tsv'</span></span><br><span class="line">LOG_PATH = <span class="string">'outputs/logs'</span></span><br><span class="line">is_debug = <span class="keyword">False</span></span><br><span class="line">flag_words = [<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;unk&gt;'</span>]</span><br></pre></td></tr></table></figure><p>Logginger.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> logging <span class="keyword">import</span> Logger</span><br><span class="line"><span class="keyword">from</span> logging.handlers <span class="keyword">import</span> TimedRotatingFileHandler</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">使用方式</span></span><br><span class="line"><span class="string">from you_logging_filename.py import init_logger</span></span><br><span class="line"><span class="string">logger = init_logger("dataset",logging_path='')</span></span><br><span class="line"><span class="string">def you_function():</span></span><br><span class="line"><span class="string">logger.info()</span></span><br><span class="line"><span class="string">logger.error()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">日志模块</span></span><br><span class="line"><span class="string">1. 同时将日志打印到屏幕跟文件中</span></span><br><span class="line"><span class="string">2. 默认值保留近7天日志文件</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_logger</span><span class="params">(logger_name, logging_path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> logger_name <span class="keyword">not</span> <span class="keyword">in</span> Logger.manager.loggerDict:</span><br><span class="line">        logger  = logging.getLogger(logger_name)</span><br><span class="line">        logger.setLevel(logging.DEBUG)</span><br><span class="line">        handler = TimedRotatingFileHandler(filename=logging_path+<span class="string">"/all.log"</span>,when=<span class="string">'D'</span>,backupCount = <span class="number">7</span>)</span><br><span class="line">        datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">        format_str = <span class="string">'[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'</span></span><br><span class="line">        formatter = logging.Formatter(format_str,datefmt)</span><br><span class="line">        handler.setFormatter(formatter)</span><br><span class="line">        handler.setLevel(logging.INFO)</span><br><span class="line">        logger.addHandler(handler)</span><br><span class="line">        console= logging.StreamHandler()</span><br><span class="line">        console.setLevel(logging.INFO)</span><br><span class="line">        console.setFormatter(formatter)</span><br><span class="line">        logger.addHandler(console)</span><br><span class="line"></span><br><span class="line">        handler = TimedRotatingFileHandler(filename=logging_path+<span class="string">"/error.log"</span>,when=<span class="string">'D'</span>,backupCount=<span class="number">7</span>)</span><br><span class="line">        datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">        format_str = <span class="string">'[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'</span></span><br><span class="line">        formatter = logging.Formatter(format_str,datefmt)</span><br><span class="line">        handler.setFormatter(formatter)</span><br><span class="line">        handler.setLevel(logging.ERROR)</span><br><span class="line">        logger.addHandler(handler)</span><br><span class="line">    logger = logging.getLogger(logger_name)</span><br><span class="line">    <span class="keyword">return</span> logger</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本
      
    
    </summary>
    
      <category term="NLP" scheme="http://state-of-art.top/categories/NLP/"/>
    
    
      <category term="文本预处理" scheme="http://state-of-art.top/tags/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>样本不平衡的处理策略及评价指标选择</title>
    <link href="http://state-of-art.top/2018/11/17/%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E5%8F%8A%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E9%80%89%E6%8B%A9/"/>
    <id>http://state-of-art.top/2018/11/17/样本不平衡处理及评价指标选择/</id>
    <published>2018-11-17T15:20:08.000Z</published>
    <updated>2019-07-02T13:07:59.577Z</updated>
    
    <content type="html"><![CDATA[<h1 id="样本不平衡的处理策略及评价指标选择"><a href="#样本不平衡的处理策略及评价指标选择" class="headerlink" title="样本不平衡的处理策略及评价指标选择"></a>样本不平衡的处理策略及评价指标选择</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在对实际问题做有监督的训练时，我们通常会碰到样本类别不平衡的情况，这样的例子不胜枚举，如：厂家生产的灯泡合格与不合格的比例，涉恐人员与正常人员的比例等等。另一方面，机器学习中，除了本身就是分类问题的，如情感分析，垃圾邮件过滤等，还有很多问题的建模都可以转化为分类问题，如下个句子预测，句子匹配可转化成二分类问题，抽取式阅读理解可转化为N（N为句子长度）分类问题，语言模型、命名实体识别都可归为分类问题。不难看出分类问题的重要性，而在这些问题中，数据不平衡问题都普遍存在。因此，研究如何处理类别不平衡和选择合适的评价指标显得尤为重要。</p><h2 id="处理策略"><a href="#处理策略" class="headerlink" title="处理策略"></a>处理策略</h2><h3 id="1-改变数据分布"><a href="#1-改变数据分布" class="headerlink" title="1 改变数据分布"></a>1 改变数据分布</h3><p>常见的有过采样、下采样、综合采样。</p><h4 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h4><p>过采样又叫重采样，这是一种保持多数样本不变的情况下，按一定比率增加少数样本的个数，使类别平衡的方法。</p><ul><li>最简单的过采样方法是随机过采样，即随机地复制少数样本。这种方法能够起到一定的效果，但是由于重复样本出现次数过多，容易发生过拟合。</li><li>SMOTE（Synthetic Minority Oversampling Technique）[1], 合成少数类过采样技术，实际上是一种插值的方法，是随机过采样的改进版本。算法流程如下：</li></ul><p>设训练集的一个少数类的样本数为T， 那么SMOTE算法将为这个少数类合成NT个新样本，N是过采样倍率。考虑少数类的一个样本i， 其特征向量为xi</p><p>A. 首先从少数类的全部T个样本中找到样本xi的k个近邻（例如欧式距离）样本，记为xi_near, near=1, 2, 3, ..,k.</p><p>B. 然后从这k个近邻中随机选择一个样本xi_nn, 在生成一个0到1之间的随机数alpha， 从而合成一个新样本xj：<br>$$<br>x_j = x_i + alpha * (x_inn - x_i)<br>$$<br>C. 将步骤B重复N次， 从而合成N个新样本。</p><p>SMOTE一定程度上缓解了过拟合问题，但是引入了噪声，MSMOTE[2]是其改进版。算法流程和SMOTE基本相同。不同的是，它将少数样本分为3个不同的组：安全样本、边界样本和潜在噪声样本。一张图来说明一下：</p><h4 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h4><ul><li>最简单的是从多数样本中随机抽取，从而减少多数样本的数量</li><li>第二种，ENN，Edited Nearest Neighbor， 这是一种基于近邻的方法。做法是如果某个样本K近邻样本的类别都跟他本身不一样，我们就将他删除，一致重复，直到无法删除为止</li><li>第三种，Tomek Link Removal， 如果有两个不同类别的样本，他们的最近邻都是对方，即A的最近邻是B， B的最近邻是A， 那么就说A和B是Tomek link。我们要做的就是把所有的Tomek link都删除掉。具体怎么删除呢，如果组成Tomek link的两个样本，如果有一个属于多数类样本，就将该多数类样本删除掉。</li></ul><h4 id="综合采样"><a href="#综合采样" class="headerlink" title="综合采样"></a>综合采样</h4><p>即将下采样和过采样相结合。</p><p>上述方法一般都可以在不同程度上缓解数据不平衡的问题。对于图像和文本这类非结构化的数据，基于最近邻的SMOTE和方法似乎不能直接使用。这涉及到文本和图像的数据增强，将在以后单独写篇说明。</p><h3 id="2-代价敏感"><a href="#2-代价敏感" class="headerlink" title="2 代价敏感"></a>2 代价敏感</h3><p>这种方法是通过改变不同类别损失函数的权重，例如，将少数类别的损失给一个较大的权重，如果少数类分错，那么就会得到一个大的损失，使模型更加关注少数类别。其实这种方法和上述的过采样和下采样有异曲同工之妙（类别下Adaboost）。</p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><ul><li>Accuracy, 准确率， 即模型分对的个数占总个数的比例。准确率对于不平衡的数据集不是个好的评价指标。举个简单的例子。假设有100个样本，97个正例3个负例，模型将这100个样本全部判断为正样本，那么这时的准确率为97/100=0.97, 准确率很高，但是模型的效果其实很差，因为一个负例也没有识别出来。一般来说，要计算准确率，需要首先将模型分数或者说概率映射成类别，这时候需要确定一个阈值，默认为0.5，即大于0.5的为正例，小于0.5的为负例。</li><li>Recall, 召回率，每个类别预测对的占该类别总数的比例。还是以上面的例子说明，100个样本，正负样本比例为99:3， 假如还是全部预测称正例，那么正例的召回率为 97/97=1, 负例的召回率为 0/3 = 0， 平均召回率为（1+0）/2 =0.5。可以看出模型的召回并不是很好。</li><li>F1-Measure, 是综合Accuracy和Recall的一个评价指标。能够较好地反映模型的真实水平，就算是不平衡的数据集。但是需要确定类别的阈值。</li><li>AUC，Area under curve, 曲线下的面积，这个曲线说的是ROC曲线，我们后面说。AUC刻画的是模型按照概率把正例排在负例前面的概率。比如说有97个正例，3个负例，我们就能得到 97*3个正负样本对，拿模型来预测，如果正例的得分要高于负例，记为1， 如果正例和负例的得分相同，记为0.5， 如果负例高于正例，记为0， 将所有得分相加，对样本对数求平均，就是AUC了，可见AUC也是一个统计概率值。AUC很适合不平衡数据集的评价，而且不需要确定类别的阈值。现在来说说ROC曲线是怎么回事。首先明确几个概念：（1）TP，true positive，表示一个样本是正例，也被预测为正例，简称正确的肯定数目（2）FN， false negative, 表示一个样本是正例，但被预测称负例，简称漏报（3）FP， false positive， 表示一个样本是负例，但被预测称正例，简称误报（4）TN， true negative， 表示一个样本是负例，也被预测为负例。当然，我们希望TP和TN越大越好，FN和FP越小越好。进一步地，引出两个概念TPR， true positive rate， 真正例率， TP/(TP+FN)，表示预测的正例中实际正例占所有正例的个数，FPR, false positive rate, 负正例率， FP/(FP+TN)表示预测的正例中实际负例占所有负例的个数。我们希望TPR越大越好，FPR越小越好。ROC曲线上的点就是由（FPR， TPR）二维点组成，理想的ROC是一条曲线，目标是图2中（0， 1）点，一般来说，ROC曲线越偏离45度对角线越好。但是实际中，由于阈值的个数总是离散的，所有ROC曲线是一个梯形折线，如图3所示。ROC和AUC有两大好处：（1）不用确定类别映射阈值（2）当数据分布（正负类别比例）发生变化时，Recall曲线和Precision曲线会发生明显变化，而AUC曲线基本能保持稳定。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;样本不平衡的处理策略及评价指标选择&quot;&gt;&lt;a href=&quot;#样本不平衡的处理策略及评价指标选择&quot; class=&quot;headerlink&quot; title=&quot;样本不平衡的处理策略及评价指标选择&quot;&gt;&lt;/a&gt;样本不平衡的处理策略及评价指标选择&lt;/h1&gt;&lt;h2 id=&quot;引言&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://state-of-art.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="样本不平衡" scheme="http://state-of-art.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1/"/>
    
    
      <category term="样本不平衡 - 评价指标" scheme="http://state-of-art.top/tags/%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>周期性学习率(Cyclical Learning Rate)技术</title>
    <link href="http://state-of-art.top/2018/10/28/%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%8A%80%E6%9C%AF/"/>
    <id>http://state-of-art.top/2018/10/28/周期性学习率技术/</id>
    <published>2018-10-28T15:21:08.000Z</published>
    <updated>2018-10-30T14:25:09.279Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍神经网络训练中的周期性学习率技术。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>学习率(learning_rate, LR)是神经网络训练过程中最重要的超参数之一，它对于快速、高效地训练神经网络至关重要。简单来说，LR决定了我们当前的权重参数朝着降低损失的方向上改变多少。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_weight = exsiting_weight - learning_rate * gradient</span><br></pre></td></tr></table></figure><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-neural_network-7.png" alt="image text"></p><p><em><center>Fig.: A simple neural network where the w’s and b’s are to be learnt (Img Credit: Matt Mazur)</center></em></p><p>这看上去很简单。但是正如许多研究显示的那样，单单通过提升这一步就会对我们的训练产生深远的影响，并且尚有很大的优化空间。</p><p>本文介绍了一种叫做周期性学习率（CLR）的技术，它是一种非常新的、简单的想法，用来设置和控制训练过程中LR的大小。该技术在<a href="https://twitter.com/jeremyphoward" target="_blank" rel="noopener">jeremyphoward</a>今年的<a href="http://www.fast.ai/" target="_blank" rel="noopener">fast.ai course</a>课程中提及过。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>神经网络用来完成某项任务需要对大量参数进行训练。参数训练意味着寻找合适的一些参数，使得在每个batch训练完成后损失（loss）达到最小，而参数更新的方式则与LR密切相关。</p><p>通常来说，有两种广泛使用的方法用来设置训练过程中的LR。</p><h3 id="One-LR-for-all-parameters"><a href="#One-LR-for-all-parameters" class="headerlink" title="One LR for all parameters"></a>One LR for all parameters</h3><p>一个典型的例子是<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">SGD</a>， 在训练开始时设置一个LR常量，并且设定一个LR衰减策略（如step，exponential等）。这个单一的LR用来更新所有的参数。在每个epochs中，LR按预先设定随时间逐渐衰减，当我们临近最小损失时， 通过衰减可以减缓更新，以防止我们越过最小值。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-learningrates.jpeg" alt="image text"></p><p><em><center>Fig. Effect of various learning rates on convergence (Img Credit: cs231n)</center></em></p><p>该方法存在如下挑战(<a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">refer</a>)：</p><ol><li>难以选择初始的LR达到想要的效果（如上图所示）；</li><li>LR衰减策略同样难以设定，他们很难自适应动态变化的数据；</li><li>所有的参数使用相同的LR进行更新，而这些参数可能学习速率不完全相同；</li><li>很容易陷入马鞍点不能自拔</li></ol><h3 id="Adaptive-LR-for-each-parameter"><a href="#Adaptive-LR-for-each-parameter" class="headerlink" title="Adaptive LR for each parameter"></a>Adaptive LR for each parameter</h3><p>一些改进的优化器如<em>AdaGrad</em>, <em>AdaDelta</em>, <em>RMSprop</em> and <em>Adam</em> 很大程度上缓解了上述困难，方法是对每个参数采用不同的自适应学习率。比如AdaDelta，它的更新机制甚至不需要我们主动设置默认的学习率。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/%E8%87%AA%E9%80%82%E5%BA%94SGD%E6%96%B9%E6%B3%95.gif" alt="image text"></p><p><em><center>Fig: Animation comparing optimization algorithms (Img Credit: Alec Radford)</center></em></p><h2 id="Cycling-Learning-Rate"><a href="#Cycling-Learning-Rate" class="headerlink" title="Cycling Learning Rate"></a>Cycling Learning Rate</h2><p>CLR是Leslie Smith于2015年提出的。这是一种调节LR的方法，在该方法中，设定一个LR上限和下限，LR的值在上限和下限的区间里周期性地变化。看上去，LCR似乎是自适应LR技术和SGD的竞争者，事实上，CLR技术是可以和上述提到的改进的优化器一起使用来进行参数更新的。</p><p>而在计算上，CLR比上述提到的改进的优化器更容易实现，正如文献[1]所述：</p><p><em>Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. In addition, CLR policies are computationally simpler than adaptive learning rates. CLR is likely most similar to the SGDR method that appeared recently.</em></p><h3 id="Why-it-works"><a href="#Why-it-works" class="headerlink" title="Why it works"></a>Why it works</h3><p>直觉上看，随着训练进度的增加我们应该保持学习率一直减小以便于在某一时刻达到收敛。</p><p>然而，事实恰与直觉相反，使用一个在给定区间里周期性变化的LR可能更有用处。原因是周期性高的学习率能够使模型跳出在训练过程中遇到的局部最低点和马鞍点。事实上，Dauphin等[3]指出相比于局部最低点，马鞍点更加阻碍收敛。如果马鞍点正好发生在一个巧妙的平衡点，小的学习率通常不能产生足够大的梯度改变使其跳过该点（即使跳过，也需要花费很长时间）。这正是周期性高学习率的作用所在，它能够更快地跳过马鞍点。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/%E9%A9%AC%E9%9E%8D%E7%82%B9.gif" alt="image text"></p><p><em><center>Fig.: A saddle point in the error surface (Img Credit: safaribooksonline)</center></em></p><p>另外一个好处是，最优的LR肯定落在最小值和最大值之间。换言之，我们确实在迭代过程中使用了最好的LR。</p><h4 id="Epoch，iterations-cycles-and-stepsize"><a href="#Epoch，iterations-cycles-and-stepsize" class="headerlink" title="Epoch，iterations, cycles and stepsize"></a>Epoch，iterations, cycles and stepsize</h4><p>首先介绍几个术语，理解这些术语可以更好地理解下面描述的算法和公式。</p><p>我们现在考虑一个包含50000个样本的训练集。</p><p>一个epoch是至将整个训练集训练一轮。如果我们将batch_size, 我们在一个epoch里会得到500个batch或者叫iteration。iteration的数目随着epoch的增加不断积累，在第二个epoch，对应着501到1000次iteration，后面的以此类推。</p><p>一个cycle定义为学习率从低到高，然后从高到低走一轮所用的iteration数。而stepsize指的是cycle迭代步数的一半。注意，cycle不一定必须和epoch相同，但实践上通常将cycle和epoch对应相同的iteration。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-triangle.png" alt="image text"></p><p><em><center>Fig: Triangular LR policy. (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在上图中，两条红线分别表示学习率最小值（base lr）和学习率最大值（max lr）。蓝色的线是学习率随着iteration改变的方式。蓝线上下一次表示一个cycle，stepsize则是其一半。</p><h3 id="Calculating-the-LR"><a href="#Calculating-the-LR" class="headerlink" title="Calculating the LR"></a>Calculating the LR</h3><p>综上所述，接下来我们需要参数作为该算法的输入：</p><ul><li><p>stepsize</p></li><li><p>base_lr</p></li><li><p>max_lr</p></li></ul><p>下面是LR更新的一段代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_triangular_lr</span><span class="params">(iteration, stepsize, base_lr, max_lr)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given the inputs, calculates the lr that should be</span></span><br><span class="line"><span class="string">    applicable for this iteration</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cycle = np.floor(<span class="number">1</span> + iteration/(<span class="number">2</span>  * stepsize))</span><br><span class="line">    x = np.abs(iteration/stepsize - <span class="number">2</span> * cycle + <span class="number">1</span>)</span><br><span class="line">    lr = base_lr + (max_lr - base_lr) * np.maximum(<span class="number">0</span>, (<span class="number">1</span>-x))</span><br><span class="line">    <span class="keyword">return</span> lr</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># Demo of how the LR varies with iterations</span></span><br><span class="line">    num_iterations = <span class="number">10000</span></span><br><span class="line">    stepsize = <span class="number">1000</span></span><br><span class="line">    base_lr = <span class="number">0.0001</span></span><br><span class="line">    max_lr = <span class="number">0.001</span></span><br><span class="line">    lr_trend = list()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        lr = get_triangular_lr(iteration, stepsize, base_lr, max_lr)</span><br><span class="line">        <span class="comment"># Update your optimizer to use this learning rate in this iteration</span></span><br><span class="line">        lr_trend.append(lr)</span><br><span class="line">    </span><br><span class="line">    plt.plot(lr_trend)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如下图所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-graph.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration. We are using the triangular profile.</center></em></p><h3 id="Deriving-the-optimal-base-lr-and-max-lr"><a href="#Deriving-the-optimal-base-lr-and-max-lr" class="headerlink" title="Deriving the optimal base lr and max lr"></a>Deriving the optimal base lr and max lr</h3><p>对于给定的数据集，怎么确定合理的base lr 和max lr呢？</p><p>答案是先跑几个epoch，并且让学习率线性增加，观察准确率的变化，从中选出合适的base 和max lr。</p><p>我们让学习率按照上面的斜率进行增长，跑了几轮，结果如下图所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-deciding-baselr-maxlr.png" alt="image text"></p><p><em><center>Fig: Plot of accuracy vs learning rate (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>可以看出，开始的时候，准确率随着学习率的增加而增加，然后进入平缓起期，然后又开始减小，出现震荡。注意图中准确率开始增长的那一点和达到平衡的那一点（图中红色箭头所示）。这两个点可以作为比较好的base lr 和 max lr。当然，你也可以选择平衡点旁边的准确率峰值点作为max lr， 把base lr 设为其1/3 或者1/4。</p><p>好了，三个参数中已经有两个确定了，那么怎么确定stepsize呢？</p><p>已经有论文做过实验，他们将stepsize设成一个epoch包含的iteration数量的2-10倍。拿我们之前举的例子来说，我们一个epoch包含500个iteration，那么stepsize就设成1000-5000。该论文实验表明，stepsize设成2倍或者10倍，两者结果并没有太大的不同。</p><h3 id="Variants"><a href="#Variants" class="headerlink" title="Variants"></a>Variants</h3><p>上面我们实现的算法中，学习率是按照三角的规律周期性变化。除了这种以外，还有其他几种不同的函数形式。</p><p><strong><em>traiangular2：</em></strong>这里max lr 按cycle进行对半衰减。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-triangular2.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration for the triangular2 approach (Img Credit: Brad Kenstler)</center></em></p><p><strong><em>exp_range：</em></strong>这里max lr按iteration进行指数衰减。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-exp_range.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration for the exp-range approach (Img Credit: Brad Kenstler)</center></em></p><p>这些与固定学习率的指数衰减（exponential decay）相比，有论文表明效果都得到了明显的提升。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>如下图所示，在某神经网络上，CLR提供了一个快速的收敛，因此它的确值得一试。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-cifar10.png" alt="image text"></p><p><em><center>Fig. CLR tested on CIFAR 10 (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在上图的试验中，CLR花了25K次迭代达到了81%的准确率，传统的LR更新方法大约需要70K才能达到同样的水平。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-adam.png" alt="image text"></p><p><em><center>Fig. CLR used with Nesterov and Adam. Much faster convergence with Nesterov (Nesterov is an improvement over SGD) (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在另一项试验中，如上图所示，CLR + Nesterov优化器比著名的Adam收敛的还要快。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>CLR带来了一种新的方案来控制学习率的更新，它可以与SGD以及一些更加高级的优化器上一起使用。CLR应该成为每一个深度学习实践者工具箱里的一项技术。</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol><li><a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">Cyclical Learning Rates for Training Neural Networks, Smith</a></li><li><a href="https://arxiv.org/pdf/1609.04747.pdf" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms, Rudder</a></li><li>Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization.</li><li><a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">SGDR: Stochastic Gradient Descent with Warm Restarts, Loshchilov, Hutter</a></li><li><a href="https://github.com/bckenstler/CLR" target="_blank" rel="noopener">https://github.com/bckenstler/CLR</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文介绍神经网络训练中的周期性学习率技术。&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;学
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="超参数" scheme="http://state-of-art.top/tags/%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    
      <category term="学习率" scheme="http://state-of-art.top/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>图像数据-TFrecord在动态图中的使用</title>
    <link href="http://state-of-art.top/2018/10/27/TFrecord%E5%9C%A8%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://state-of-art.top/2018/10/27/TFrecord在动态图中的使用/</id>
    <published>2018-10-27T15:21:08.000Z</published>
    <updated>2018-10-28T08:50:46.251Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍图片数据使用TFrecord和tf.data.dataset进行存储和读取。</p><p>Tensorflow 提供了四种数据读取方式：</p><ol><li><p>Preloaded data: 用一个tf.constant常量将数据集加载进来，主要用于很小的数据集；</p></li><li><p>Feeding: 使用python代码供给数据，将所有数据加载进内存，然后一个batch一个batch地输入到计算图中， 适用于小数据集；</p></li><li>QueueRunner: 基于队列的输入通道，读取TFrecord静态图使用；</li><li>tf.data API: 能够从不同的输入或文件格式中读取、预处理数据，并且对数据应用一些变换（例如，batching、shuffling、mapping function over the dataset），tf.data API 是旧的 feeding、QueueRunner的升级。值得注意的是， Eager模式必须使用该API来构建输入通道， 一般结合TFrecord使用。该API相比于Queue更容易使用。</li></ol><h2 id="What‘s-TFrecord"><a href="#What‘s-TFrecord" class="headerlink" title="What‘s TFrecord"></a>What‘s TFrecord</h2><p>TFrecord是Tensorflow提供的一种二进制存储格式，可将数据和标签统一存储。从上述读取方式中可以看出，TFrecord在QueueRunner和tf.data API读取中均扮演了重要的角色。</p><h2 id="Why-TFrecord"><a href="#Why-TFrecord" class="headerlink" title="Why TFrecord"></a>Why TFrecord</h2><p>与其他方案相比， 使用TFrecord读取的优点在于：</p><ol><li>可处理大规模数据量，而不会造成其他方案所带来的内存不够用的问题；</li><li>在Feeding方案中，batch读取的IO操作势必会阻塞训练，前一个batch加载完成后，神经网络必须等待下一个batch加载完成后才能继续训练，效率较低。</li></ol><h2 id="How-To-Use"><a href="#How-To-Use" class="headerlink" title="How To Use"></a>How To Use</h2><p>TFrecord的使用主要有两块：一是图片数据转TFrecord格式存储，二是解析存储好的TFrecord文件。下面逐一介绍。</p><h3 id="图片转TFrecord"><a href="#图片转TFrecord" class="headerlink" title="图片转TFrecord"></a>图片转TFrecord</h3><p>本文使用的数据集是Kaggle猫狗数据集。</p><p>该数据集包含train和test两个文件夹， 分别为训练集和测试集，下面以train集为例操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls |wc -w</span><br><span class="line"></span><br><span class="line">25000</span><br></pre></td></tr></table></figure><p>训练集包含25000张图片，猫狗各一半。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls </span><br><span class="line"></span><br><span class="line">cat.124.jpg    cat.3750.jpg  cat.6250.jpg  cat.8751.jpg  dog.11250.jpg  dog.2500.jpg   dog.5000.jpg  dog.7501.jpg</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>图片文件以jpg格式存储，以cat， dog作为文件名开头。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_tfrecord_encode</span><span class="params">(classes, tfrecord_filename, data_path, is_training=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    功能：读取图片转换成tfrecord格式的文件</span></span><br><span class="line"><span class="string">    @params: classes: 标签类别  @type：classes: dict</span></span><br><span class="line"><span class="string">    @params: tfrecord_filename: tfrecord文件保存文件</span></span><br><span class="line"><span class="string">    @type：tfrecord_filename: str</span></span><br><span class="line"><span class="string">    @params: data_path: 原始训练集存储路径</span></span><br><span class="line"><span class="string">    @is_training: 是否为训练集，用来区分训练集和测试集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 初始化一个writer</span></span><br><span class="line">    writer = tf.python_io.TFRecordWriter(tfrecord_filename)</span><br><span class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> tqdm(os.listdir(path)):</span><br><span class="line">        name = img_name.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 使用tf.gfile.FastFile读取图片要比PIL.Image读取处理得到的</span></span><br><span class="line">        <span class="comment"># 最终TFrecod文件小得多，在本案例中，IMAGE方式读取得到的TFrecord大小约为3.7G</span></span><br><span class="line">        <span class="comment"># 而tf.gfile.FastFile得到的约为548M</span></span><br><span class="line">        <span class="keyword">with</span> tf.gfile.FastGFile(os.path.join(path, img_name), <span class="string">'rb'</span>) <span class="keyword">as</span> gf:</span><br><span class="line">            img = gf.read()</span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            <span class="comment"># 构造特征</span></span><br><span class="line">            feature = &#123;</span><br><span class="line">                <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[classes[name]])),</span><br><span class="line">                <span class="string">'img_raw'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),</span><br><span class="line">                <span class="string">'file_name'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()]))</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature = &#123;</span><br><span class="line">                <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[<span class="number">-1</span>])),</span><br><span class="line">                <span class="string">'img_raw'</span>:tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),</span><br><span class="line">                <span class="string">'file_name'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()]))</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="comment"># example 对象将label和image特征进行封装</span></span><br><span class="line">        example = tf.train.Example(features=tf.train.Features(feature=feature))  </span><br><span class="line">        writer.write(example.SerializeToString())   <span class="comment"># 序列化为字符串</span></span><br><span class="line">    writer.close()</span><br><span class="line">    print(<span class="string">'tfrecord writen done!'</span>)</span><br></pre></td></tr></table></figure><p>调用上述函数，可得到猫狗训练集的TFrecord格式文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    classes = &#123;<span class="string">'cat'</span>: <span class="number">0</span>, <span class="string">'dog'</span>: <span class="number">1</span>&#125;</span><br><span class="line">    tfrecord_filename = <span class="string">'cat_and_dog.tfrecord'</span></span><br><span class="line">    data_path = <span class="string">'train/'</span></span><br><span class="line">    img_tfrecord_encode(classes, tfrecord_filename, data_path, is_training=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>上述程序运行大约需要2min。</p><h3 id="使用tf-data读取TFrecord"><a href="#使用tf-data读取TFrecord" class="headerlink" title="使用tf.data读取TFrecord"></a>使用tf.data读取TFrecord</h3><p>在动态图（eager）模式下，QueueRunner不可用，必须使用tf.data进行TFrecord的读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_tfrecord_parse</span><span class="params">(tfrecord_filename, epochs, batch_size, shape,</span></span></span><br><span class="line"><span class="function"><span class="params">                       padded_shapes=None, shuffle=True, buffer_size=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param: tfrecord_filename:tfrecord文件列表   @type:list</span></span><br><span class="line"><span class="string">    @param: epoch:训练轮数（repeating次数）       @type:int</span></span><br><span class="line"><span class="string">    @param：batch_size:批数据大小                @type:int</span></span><br><span class="line"><span class="string">    @param: shape:图片维度                      @type:tuple</span></span><br><span class="line"><span class="string">    @param: padded_shapes:不定长padding        @type:tuple</span></span><br><span class="line"><span class="string">    @param: shuffle:是否打乱                   @type:boolean</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"><span class="comment"># 解析单个example，特征与encode一一对应。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_example</span><span class="params">(serialized_example)</span>:</span></span><br><span class="line">        features = tf.parse_single_example(serialized_example,</span><br><span class="line">                                           features=&#123;</span><br><span class="line">                                               <span class="string">'label'</span>: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">                                               <span class="string">'img_raw'</span>: tf.FixedLenFeature([], tf.string),</span><br><span class="line">                                               <span class="string">'file_name'</span>: tf.FixedLenFeature([], tf.string)</span><br><span class="line">                                           &#125;)</span><br><span class="line">        <span class="comment"># 解码</span></span><br><span class="line">        image = tf.image.decode_jpeg(features[<span class="string">'img_raw'</span>])</span><br><span class="line">        <span class="comment"># 设置shape</span></span><br><span class="line">        image = tf.image.resize_images(image, shape, method=<span class="number">1</span>)</span><br><span class="line">        label = tf.cast(features[<span class="string">'label'</span>], tf.int64)</span><br><span class="line">        file_name = tf.cast(features[<span class="string">'file_name'</span>], tf.string)</span><br><span class="line">        <span class="keyword">return</span> image, label, file_name</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 解析TFrecord</span></span><br><span class="line">    dataset = tf.data.TFRecordDataset(tfrecord_filename).map(parse_example)</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="keyword">if</span> padded_shapes:</span><br><span class="line">            dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).padded_batch(batch_size, padded_shapes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).batch(batch_size)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> padded_shapes:</span><br><span class="line">            dataset = dataset.repeat(epochs).padded_batch(batch_size, padded_shapes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = dataset.repeat(epochs).batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><p>调用上述函数，解析TFrecord得到dataset。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>():</span><br><span class="line">    tfrecord_filename = <span class="string">'cat_and_dog.tfrecord'</span></span><br><span class="line">    epochs = <span class="number">100</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    shape = (<span class="number">227</span>, <span class="number">227</span>)</span><br><span class="line">    dataset = img_tfrecord_parse(tfrecord_filename=tfrecord_filename,</span><br><span class="line">                                epochs=epochs,</span><br><span class="line">                                batch_size=batch_size,</span><br><span class="line">                                shape=shape)</span><br><span class="line">    <span class="comment"># 查看dataset</span></span><br><span class="line">    iterator = dataset.make_one_hot_iterator()</span><br><span class="line">    image, label, file_name = iterator.get_next()</span><br><span class="line">    print(image[<span class="number">0</span>])</span><br><span class="line">    print(label[<span class="number">0</span>])</span><br><span class="line">    print(file_name[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文介绍图片数据使用TFrecord和tf.data.dataset进行存储和读取。&lt;/p&gt;
&lt;p&gt;Tensorflow 提供了四种数据读取方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Preloaded data: 用一个tf.constant常量将数据集加载进来，主要用于很小
      
    
    </summary>
    
      <category term="Tensorflow" scheme="http://state-of-art.top/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://state-of-art.top/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Word2vec 数学原理详解</title>
    <link href="http://state-of-art.top/2018/07/17/Word2vec%20%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/"/>
    <id>http://state-of-art.top/2018/07/17/Word2vec 数学原理详解/</id>
    <published>2018-07-17T15:20:08.000Z</published>
    <updated>2019-07-02T13:45:52.413Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Word2vec-数学原理详解"><a href="#Word2vec-数学原理详解" class="headerlink" title="Word2vec 数学原理详解"></a>Word2vec 数学原理详解</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>词向量可看做基于深度学习的自然语言处理的一个里程碑。从2003年最初的神经语言模型的副产物，到后面的Word2vec, glove, fastext, 及其近两年流行的ELMO， GPT， BERT， 可以说，预训练词向量是伴随着自然语言处理一起发展的。</p><p>这篇博客主要想介绍下Word2vec的相关知识，特别是word2vec的数学原理部分，已达到对其有个知其然且知其所以然的目的。Word2vec 最初由Mikolov2013年在《Efficient Estimation of Word Representations in Vector Space》中提出，但文中对其细节并未涉及太多。本文主要基于2016年Xin Rong的一篇论文，《word2vec Parameter Learning Explained》。</p><h2 id="1-神经语言模型"><a href="#1-神经语言模型" class="headerlink" title="1 神经语言模型"></a>1 神经语言模型</h2><p>神经语言模型是对一个句子出现的概率进行建模。直白点的说法就是，什么样的句子更像是人话。假设一个句子有m个单词，$ [w_1， w_2, …, w_m] $，那么这个句子出现的概率就是这m个单词的共现概率：</p><p>$$<br>P(sent=[w_1, w_2, …, w_m])=P(w_1, w_2, …, w_m)=P(w_1)P(w_2|w_1)P(w_3|w_2, w_1)…P(w_m|w_{m-1}…w_1)<br>$$<br>我们希望概率$P$能取到最大值，从而得到一句比较靠谱的话，其实就是希望上式中的条件概率$P(w_m|w_{m-1}…w_1)$最大，即在给定$[w_1, w_2, …, w_{m-1}]$的情况下， 下个词是$w_m$的概率最大。实际情况中，往往假设下个词出现的概率只与距离他比较近的N个词相关（一般N&lt;5），这就是N-gram语言模型了，它是语言模型的一个近似。这时， 上式退化成：</p><p>$$<br>P(sent=[w_1, w_2, …, w_m])=P(w_1, w_2, …, w_m)=P(w_1)P(w_2|w_1)P(w_3|w_2w_1)…P(w_m|w_{m-1}…w_{w-n})<br>$$<br>2003年，Bengio在《A neural probabilistic language model》中提出了非常经典的神经语言模型，word2vec就是由此简化而来。Bengio利用一个简单的三层(实际上是四层，第一层是查表)神经网络来计算条件概率$P(w_m|w_{m-1…w_{m-n}} )$,如下图所示(这里并没有采用论文原图，而是用了一个更加详细的前馈神经网络，标注更加详细)：</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/word2vec/1.jpeg?q-sign-algorithm=sha1&amp;q-ak=AKIDcdpXUgopQxEsen6K2BOhvxibRJoij0kt&amp;q-sign-time=1562074927;1562078527&amp;q-key-time=1562074927;1562078527&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=c05e311021fc4b57efe4be255302537c34d256ee&amp;x-cos-security-token=336115f0378aec8fa6905cab4bac02eccdaf088510001" alt=""></p><p>从下往上看，首先， C是一个维度（V, D) 的矩阵， 存储着所有词的词向量，其中D为词向量的维度， V为词典大小， C一般是随机初始化的。用前n个词的索引到C中去取对应的词向量， 拼接到一起，得到一个（1， ND）的向量，然后经过线性运算W（维度为(ND, H）映射到隐藏层（维度为1， H），在与$W’$（维度为（H， V））线性运算，经过激活函数，输出一个（1， V）的矩阵。由于我们希望遍历所有词汇找到概率最大的那个词， 因此，最终的输出概率应该是所有词的概率， 为（1， V）维。</p><p>这个模型训练起来会非常耗时。原因将在下面说明。</p><h2 id="2-One-word-model"><a href="#2-One-word-model" class="headerlink" title="2 One word model"></a>2 One word model</h2><p>下面结合图2来说明类CBOW的前向和反向传播过程。为简单起见，我们只考虑上文为1的情况，即输入是一个词， N=1。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/word2vec/3.png?q-sign-algorithm=sha1&amp;q-ak=AKIDuWjYJgly5GRfho0DIoBFprJeV3dxrkxy&amp;q-sign-time=1562075039;1562078639&amp;q-key-time=1562075039;1562078639&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=3142bc630110d6061ff5d314830bed89f38796f9&amp;x-cos-security-token=efe6d2fe30e802e08a75375a04db2a3558dcfe0310001" alt=""></p><p>CBOW相比于神经语言模型，在网络结构上有个两个主要的改变：</p><ul><li>没有激活函数</li><li>只有三层，上面我们提到过，神经语言模型实际上有四层，第一层是查表操作。CBOW实际上是省略了神经语言模型中的隐藏层。</li></ul><p>在输入层，单词$w_I$是用one-hot矩阵来表示的，维度为（1， V）， 即除了$x_k$为1， 其他全部为0, k为该单词在词汇表中的下标。该向量与词向量矩阵（维度为（V，D））相乘，实际上是做行的选取， 取除了第k个单词对应的词向量矩阵的第k行，得到一个（1， D）的向量, 记为$X$，事实上这就是这个单词的词向量表示。</p><p>先来看下前向传播，从第一层到第二层：</p><p>$$h=W_T \cdot X=v^T_{w_I}$$</p><p>第二层到第三层（输出层）：</p><p>$$u=W’^T \cdot h$$</p><p>各层维度变化依次为：<br>$$<br>(1, V)<em>(V, D)\rightarrow(1, D)</em>(D, V)\rightarrow(1, V)<br>$$<br>最后用softmax将输出层做一个归一化，得到每个候选词$w_j$的概率:</p><p>$$P(w_j|w_I)=y_j=\frac{exp(u_j)}{\Sigma_{k\in V}exp(u_k)}$$</p><p>这里面$W$和$W’$是要学习的参数， 其中W就是词向量。这里其实出现一个问题了，为什么用W而不用W’作为词向量的表示呢？下面从反向传播出发给出解释。</p><p>首先，一个训练样本为$（w_I, w_o）$, 输入词是一个one-hot表示的维度为V的向量X，真实值为$w_O$， 也可以将其表示为one-hot编码，而我们模型的输出是维度为V的概率，因此可以用交叉熵作为损失函数。</p><p>上面我们得到了每个候选词的概率表示，那么，利用最大对数似然函数，可以得到：</p><p>$$max P(w_O|w_I) = max \frac{exp(u_j)}{\Sigma_{k\in V}exp(u_k)}=max(u_j-\Sigma^V_{k=1}exp(u_k))=-E$$</p><p>取对数，可得到：</p><p>$$max Obj=u_{j^*}-\Sigma^V_{k=1}exp(u_k))$$</p><p>结合以上两式，可得：</p><p>$$E=u_{j^*}-\Sigma^V_{k=1}exp(u_k))$$</p><p>接下来就可以利用E进行反向传播了。</p><p>首先第三层到第二层的向量矩阵W’的梯度：</p><p>$$\frac{\partial E}{\partial W’_{ij}}=\frac{\partial E}{\partial u_j}\frac{\partial u_j}{\partial W’_{ij}}=(y_j-t_j)h_i$$</p><p>这样就能得到W‘更新方式了：</p><p>$$<br>W’^{(new)}<em>{ij}=W’^{(old)}</em>{ij}-\eta \cdot(y_j-t_j)h_i<br>$$<br>上式是W’一个元素的更新方式，我们将他整合成向量的形式：</p><p>$$v’^{(new)}_{w_j}=v’^{(old)}_{w_j}-\eta \cdot(y_j-t_j) \cdot h$$                $$for j=1, 2, …, V$$</p><p>其中，$\eta$是学习率。我们具体看下更新过程。我们的目标$t=[0, 0, 0,…,1,0, 0, 0]_{1<em>V}$。只有一个词为1， 剩下的都为0。而模型的预测为$y=[x_1, x_2, …, x_V]$. 更新的目的就是为了是y逼近t。这就是说，对于每个j，W‘的第j列向量都要如上式的方式更新。W’的更新需要对所有的V列个向量进行更新，即D</em>V个参数。</p><p>接着看第二层到第一层的更新方式，即参数矩阵<strong>W</strong>的更新，其实就是<strong>h</strong>的更新，因为h是W的一行。</p><p>$$\frac {\partial E}{\partial W}=\frac {\partial E}{\partial h} \frac{\partial h}{\partial W}$$</p><p>由于h是（1， D）为向量，且$u_j = w_{ij} \cdot h_i$，我们先考察每个元素$h_i$的更新方式：</p><p>$$\frac {\partial E}{\partial h_i}=\frac {\partial E}{\partial u} \frac {\partial u}{\partial h_i}= \Sigma^V_{j=1}(y_j-t_j)w’_{ij}=W’_i \bigodot P$$</p><p>这里求和的原因与上面类似。这样就能得到：</p><p>$$\frac {\partial E}{\partial h}=W’ \bigodot P$$</p><p>这里其实就是$W_i$的更新了。</p><p>由于$x$中仅一个元素不为0， 所以W只需要更新第i行就行了。</p><h2 id="CBOW-model"><a href="#CBOW-model" class="headerlink" title="CBOW model"></a>CBOW model</h2><p>模型如下图所示：</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/word2vec/5.png?q-sign-algorithm=sha1&amp;q-ak=AKIDchzX2CPKps8QUKRLw5lZKGPgBDqg7udV&amp;q-sign-time=1562075136;1562078736&amp;q-key-time=1562075136;1562078736&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=6ba554129d20e4b59e3e18788678200953375227&amp;x-cos-security-token=b3de0ad72e34c345336a287eeda9c1135b8531b910001" alt=""></p><p>跟one word model唯一不同的是，输入不再是一个单词，而是C个单词，每个单词都采用one hot编码。这时候第二层的计算与之前就有些不同了。之前是直接取出W的第i行作为h的值，现在则是取出W中所有C个单词的词向量，共C行，然后直接取平均：</p><p>$$h=\frac {1}{C}W^T(x_1+x_2+…+x_C)=\frac {1}{C}(v_{w_1}+v_{w_2}+…+v_{w_C})T$$</p><p>后面的前向传播就和one-word-model 没什么不同了。</p><p>再来看反向传播。第三层到第二层，与one word model 完全相同。$\frac {\partial E}{\partial h}$也不变， 再写一遍：</p><p>$$\frac {\partial E}{\partial h}=W’ \bigodot P$$</p><p>但是这时候W的更新稍有不同了，因为h是由多个行做平均的，这里采取的策略是将h的梯度平均分配到每个单词上，每次反向传播时会更新W中的c行。</p><p>$$v^{T(new)}_{w_{I,c}}=v^{T(old)}_{w_{I,c}}-\frac {1}{C} \eta W’\bigodot P$$                                   $c=1, 2, …, C$</p><h2 id="SkipGram-Model"><a href="#SkipGram-Model" class="headerlink" title="SkipGram Model"></a>SkipGram Model</h2><p>skipGram是通过单词预测上下文，这个模型与one word model不同之处在于，SG的输出有多个词，而one word model的输出只有一个词。此时，输出层就不是一个多项式分布了，而是C个多项分布，模型如下图所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/word2vec/2.png?q-sign-algorithm=sha1&amp;q-ak=AKIDrUE63jFnGUAObNGIaOTEvS4w6iXNUwQ3&amp;q-sign-time=1562074980;1562078580&amp;q-key-time=1562074980;1562078580&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=d81ed94eeea25d5af05fbb56ef94130c54c6293c&amp;x-cos-security-token=2b42e94cc9e2f7c56840d9b057541531dacdc40810001" alt=""></p><p>先来看下前向传播，第一层到第二层与one word model相同：</p><p>$$h=W^T \cdot X=v^T_{w_I}$$</p><p>在输出层，有C个词，意味着有C个独立的多项式分布：$y_1, y_2, …y_C$, 对于第c个词，输出层预测的概率与one word model相同：</p><p>$$P(w_{c, j}|w_I)=y_{c, j}=\frac {exp(u_{c,j})}{\Sigma^V_{k=1}exp(u_{c, k})}$$</p><p>至于从第二层到第三层，从图中可以看出，连接第二层与c个输出层的参数矩阵W‘是共享的，即$u_{c,j}=u_j=v’^T_{wj} \cdot h$, 这里$v’^T_{wj}$与one word model中一样，表示W’的第J列，同时也是词汇表中第j个单词的一种词向量表示。</p><p>对于SG模型，损失函数和前面两个模型稍有不同，这里需要最大化的是C个词的概率，而这C个词是独立的，因此他们的联合概率只需将每个词的概率相乘即可，其对数最大似然函数如下：</p><p>$$E=-\Sigma^C_{c=1}u_{j^*c}+Clog\Sigma^V_{k=1}exp(u_k)$$</p><p>这里：</p><p>$$\frac {\partial E}{\partial u_{c,j}}=y_{c, j}-t_{c, j}$$</p><p>下面来看下W‘的梯度。由于W’是C个单词共享的，因此其梯度更新受C个单词影响，实际上也是平摊的思想，只不过反过来了。先来看下W‘中一个元素的梯度：</p><p>$$<br>\frac {\partial E}{\partial w’<em>{ij}}=\Sigma^C</em>{c=1}\frac {\partial E}{\partial u_{c,j}}\frac{\partial u_{c,j}}{\partial w’<em>{ij}}=\Sigma^C</em>{c=1}(y_{c,j}-t_{c,j})h_i=Q_jh_i<br>$$<br>与one word model相比，这里就多了一个求和项而已。W’的更新也类似。</p><p>再来看第二层到第一层，首先考虑对$h_i$的梯度：</p><p>$$<br>\frac {\partial E}{\partial h_i}=\Sigma^C_{c=1}\Sigma^V_{j=1}\frac{\partial E}{\partial u_{c,j}}\frac{\partial u_{c,j}}{\partial h_i}=\Sigma^C_{c=1}\Sigma^V_{j=1}(y_{c,j}-t_{c,j})w’<em>{ij}=\Sigma^V</em>{j=1}Q_jw’_{ij}=W’_i \bigodot Q<br>$$<br>向量形式为：</p><p>$$\frac {\partial E}{\partial h}=W’ \bigodot Q$$</p><p>h同样是W的一行。</p><p>至此，CBOW和SKIP的前向和反向传播就介绍完了。这两个都可以看做one word model的变种，CBOW是第一层到第二层变了，而SG是第二层到第三层变了，所以先理解one word model很重要。</p><p>从反向传播的推导可以看出，对于CBOW和SKIP模型，W‘每次需要更行所有的参数，而W每次只需更行部分行。W’的参数个数为（D，V），其中，D为词向量维度，一般在50-300之间，V为词典大小，对于大规模语料，可能达到十万百万量级，因此，这一层的参数量在千万亿量级，规模巨大。因此，人们提出一些策略来优化，word2vec采用了层次化softmax和负采样两种方法来优化，这两种方法的目的是一致的，就是在训练时，<strong>一次反向传播的只需更新W‘的部分参数</strong>，也就是说W’并不是实际中存在的，这也是为什么不用它作为词向量表示的原因。上述过程训练过程复杂度高的原因在于，每次都要更新V列个列向量，这两种方法都是用来减小这个数目。</p><h2 id="Hierarchical-softmax-层次化softmax"><a href="#Hierarchical-softmax-层次化softmax" class="headerlink" title="Hierarchical softmax (层次化softmax)"></a>Hierarchical softmax (层次化softmax)</h2><p>回想下上面三个模型的输出层，我们的目的是对给定的输入，找到概率最大的候选词作为输出。HS主要基于哈弗曼树，他将计算量大的部分变成了二分类问题。下图所示是一颗哈弗曼树（二叉树），其叶子节点为所有的V个词。根节点与第二层的h相连。从根节点到每个叶子节点，都有一条唯一的路径。比如说，$w_2$是真实值，那我们的目的就是使得加粗的这条路径的概率最大。从根节点到叶子节点，经过每个内部节点时，都有两种选择，即向左还是向右，不同的选择对应不同的路径。这里用到类似逻辑回归的思想。用$n(w,j)$表示到叶子节点$w$的路径上第j个非叶子节点，我们假设每个非叶子节点都对应一个向量$v’_{n(w,j)}$, 维度与h相同，为（1， D）。可以证明，一共有V-1个这样的内部节点。在每个内部节点，我们将该节点的向量与h做内积，然后做sigmoid变换，得到一个概率值，通过概率来指示向左还是向右。第j个节点向左和向右的概率定义为：</p><p>$$P(j, left)=\sigma(v’^T_w \cdot h)$$</p><p>$$P(j, right)=1-\sigma(v’^T_w \cdot h)=\sigma(-v’^T_w \cdot h)$$</p><p>这样，就可以定义整条路径上，输入输出的概率了：</p><p>$$P(w=w_O|w_I)=\Pi^{L_{(w)-1}}_{j=1}P(I(n(w,j+1==left)v’^T_w \cdot h))$$</p><p>其中，$I()$为指示函数，当下个节点是left，为1， 否则为-1， $L_{(w)}$表示整条路径的长度。</p><p>下面给出对数似然函数：</p><p>$$-E=-logP(w=w_O|w_I)=-\Sigma^{L_{(w)-1}}_{j=1}log(\sigma([I]v’^T_j \cdot h))$$</p><p>接着就可以求梯度了(这里省略了log，具体推导可见论文)：</p><p>$$\frac {\partial E}{\partial v’^T_j}=\frac {\partial E}{\partial v’^T_j \cdot h}\frac{\partial v’^T_j \cdot h}{\partial v’^T_j}=(\sigma(v’^T_jh)-t_j)h$$</p><p>$v’^T_j$的更新方式为：</p><p>$$v’^{(new)}_j=v’^{(old)}_j-\eta((\sigma(v’^T_jh)-t_j)h$$             $j=1, 2, 3,…,L_{w}-1$</p><p>这就是说，对于一个样本，只需要更新$L_w-1$个向量就行了，而未优化版本需要更新V个，相当于时间复杂度从$O(V)$变成了$O(log(V))$, 而向量的个数没变，都是V个向量，即空间复杂度相同。而且，一般构建哈弗曼树时，高频词会离根节点越近，也就是说高频词的路径更短, 复杂度低于$O(log(V))$</p><p>隐藏层的梯度也很容易求得：</p><p>$$\frac {\partial E}{\partial h}=\frac {\partial E}{\partial v’^T_j \cdot h}\frac{\partial v’^T_j \cdot h}{\partial h}=\Sigma^{L_{(w)-1}}_{j=1}(\sigma(v’^T_jh)-t_j)v’_j$$</p><h2 id="Negative-Sampling-负采样"><a href="#Negative-Sampling-负采样" class="headerlink" title="Negative Sampling 负采样"></a>Negative Sampling 负采样</h2><p>负采样比层次化softmax更简单粗暴了，每次训练，我们这样构造样本，首先正确的候选词只有一个，为正例，负例呢，在剩余的词中按照一定分布采样N个， N&lt;&lt;V。如何采样呢?在word2vec中，作者直接基于词频的权重分布：</p><p>$$len(w)=\frac {count(w)^{3/4}}{\Sigma_{u\in vocab} count(u)^{3/4}}$$</p><p>采样前，我们将长度为1的线段划分成M等份，M&gt;&gt;N, 这M份中的每一份都对应某个词对应的线段上。采样的时候只需要从M个位置中采样出neg位置就可以了，每个位置所属的词就是负例词。</p><p>这样，每次优化时，使正例的概率越大越好，负例的概率越小越好，这实际上是二元逻辑回归来解决多分类问题。正例和负例的概率分别为：</p><p>$$P_+=\sigma(v’^T_w \cdot h)$$</p><p>$$P_-=1-\sigma(v’^T_w \cdot h)=\sigma(-v’^T_w \cdot h)$$</p><p>那么，正例和负例的最大似然函数为：</p><p>$$maxObj=maxP_+ \cdot \Pi^N_1P_-$$</p><p>对数似然函数为：</p><p>$$E = -log(P_+)-\Sigma^N_1log(P_-)$$</p><p>其中，N为负采样的个数。</p><p>剩下部分的推导就和HS一样了，这里就不再赘述。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>Word2vec 是神经语言模型的优化版本，CBOW是用周围的词预测中心词；SKIP-Gram中心词预测上下文</li><li>层级softmax和负采样技术是两种优化技术，其核心思想是每次训练时，不用更新第二层到输出层矩阵的所有参数，只需更新部分参数即可，大大降低了计算复杂度。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Word2vec-数学原理详解&quot;&gt;&lt;a href=&quot;#Word2vec-数学原理详解&quot; class=&quot;headerlink&quot; title=&quot;Word2vec 数学原理详解&quot;&gt;&lt;/a&gt;Word2vec 数学原理详解&lt;/h1&gt;&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
      <category term="词向量" scheme="http://state-of-art.top/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="数学原理 - word2vec" scheme="http://state-of-art.top/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-word2vec/"/>
    
  </entry>
  
  <entry>
    <title>Byte Pair Coding Usage</title>
    <link href="http://state-of-art.top/2018/06/17/Byte%20Pair%20Coding%20Usage/"/>
    <id>http://state-of-art.top/2018/06/17/Byte Pair Coding Usage/</id>
    <published>2018-06-17T15:20:08.000Z</published>
    <updated>2019-07-02T13:04:28.478Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Byte-Pair-Coding-Usage"><a href="#Byte-Pair-Coding-Usage" class="headerlink" title="Byte Pair Coding Usage"></a>Byte Pair Coding Usage</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Traditonally, Byte Pair Coding(BPE) is a text compress algorithm.  Now, it has been developed to be a popular word-segment method to address the rare word and out-of-vocabulary(OOV) problem[1], and significantly decrease the computer complexity, which has been widely in NLP task, such as machine translation, BERT.</p><p>In this post, I want to show you the basic usage of the subword-nmt package.</p><p>Recently, I found a user-friendly package called fastBPE having the same functional as subword-nmt, which can be download at: <a href="https://github.com/glample/fastBPE" target="_blank" rel="noopener">https://github.com/glample/fastBPE</a></p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><ul><li>Install from pip</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install subword-nmt</span><br></pre></td></tr></table></figure><ul><li>Install from source</li></ul><p>First, download the source file from <a href="https://github.com/rsennrich/subword-nmt" target="_blank" rel="noopener">https://github.com/rsennrich/subword-nmt</a>, then unzip it and come in the main directiory, run:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><h2 id="learn-ape"><a href="#learn-ape" class="headerlink" title="learn-ape"></a>learn-ape</h2><ul><li><p>Funcitonal</p><p>learn the subword pieces encoding using ape pair alorgrithm</p></li><li><p>Usage</p><p>For example, we use the subword-cut/tests/data/cropus.en to show the usage of subword-nmt.</p><ul><li><p>Command</p><p>subword-nmt learn-ape -s {num_operations} &lt; {train_file} &gt; {codes_file}</p></li></ul></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt learn-ape &lt; data/corpus.en &gt; output/learn.ape</span><br></pre></td></tr></table></figure><h2 id="get-vocab"><a href="#get-vocab" class="headerlink" title="get-vocab"></a>get-vocab</h2><ul><li><p>Functional</p><p>Build a vocab according to the train_file</p></li><li><p>Command</p><p>subword-nmt get-vocab –input {train_file} –vocab-file {vocab_file}</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt get-vocab --input data/corpus.en --vocab-file output/en.vocab</span><br></pre></td></tr></table></figure><h2 id="segment-char-ngrams"><a href="#segment-char-ngrams" class="headerlink" title="segment_char_ngrams"></a>segment_char_ngrams</h2><p>This has been deprecated in the current version.</p><h2 id="learn-joint-bpe-and-vocab"><a href="#learn-joint-bpe-and-vocab" class="headerlink" title="learn_joint_bpe_and_vocab"></a>learn_joint_bpe_and_vocab</h2><p>For convenient, you can accompanish the learn-ape and get-vocab in a single command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt learn-joint-bpe-and-vocab --input data/cropus.en --output output/joint_learn.ape --write-vocabulary output/joint.vocab</span><br></pre></td></tr></table></figure><p> Note that the joint.vocab is the union vocab of word and sub_word, which is different from that  is got from the command “subword-nmt get-vocab”</p><h2 id="apply-ape"><a href="#apply-ape" class="headerlink" title="apply_ape"></a>apply_ape</h2><p>Final, we need re-segment the train_file with the newest vocabulary, namely joint.vocab, which can be accompanished by apply_ape.</p><p>Command is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt apply-bpe -c output/joint_learn.ape &lt; data/corpus.en &gt; output_apply.bpe</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Neural Machine Translation of Rare Words with Subword Units</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Byte-Pair-Coding-Usage&quot;&gt;&lt;a href=&quot;#Byte-Pair-Coding-Usage&quot; class=&quot;headerlink&quot; title=&quot;Byte Pair Coding Usage&quot;&gt;&lt;/a&gt;Byte Pair Coding Usa
      
    
    </summary>
    
      <category term="机器翻译" scheme="http://state-of-art.top/categories/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="BPE" scheme="http://state-of-art.top/tags/BPE/"/>
    
  </entry>
  
</feed>
