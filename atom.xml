<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>circlepi&#39;s blog</title>
  
  <subtitle>Even with an intractable probelm, one can find a way to do the right thing.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://state-of-art.top/"/>
  <updated>2019-01-06T13:06:21.638Z</updated>
  <id>http://state-of-art.top/</id>
  
  <author>
    <name>circlepi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BERT解读(一) self——attention</title>
    <link href="http://state-of-art.top/2019/01/06/BERT%E8%A7%A3%E8%AF%BB(%E4%B8%80)self-Attention/"/>
    <id>http://state-of-art.top/2019/01/06/BERT解读(一)self-Attention/</id>
    <published>2019-01-06T15:30:08.000Z</published>
    <updated>2019-01-06T13:06:21.638Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BERT解读（一）self-Attention"><a href="#BERT解读（一）self-Attention" class="headerlink" title="BERT解读（一）self-Attention"></a>BERT解读（一）self-Attention</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本系列文章希望对google BERT模型做一点解读。打算采取一种由点到线到面的方式，从基本的元素讲起，逐渐展开。</p><p>讲到BERT就不能不提到Transformer，而self-attention则是Transformer的精髓所在。简单来说，可以将Transformer看成和RNN类似的特征提取器，而其有别于RNN、CNN这些传统特征提取器的是，它另辟蹊径，采用的是attention机制对文本序列进行特征提取。</p><p>所以我们从self-Attention出发。</p><p>文章内容参考<a href="https://jalammar.github.io/illustrated-transformer/，jalammar的博客十分通俗易懂，且切中要害，本系列内容不乏很多翻译自jalammar的博客。" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/，jalammar的博客十分通俗易懂，且切中要害，本系列内容不乏很多翻译自jalammar的博客。</a></p><h2 id="Attention-is-all-your-need"><a href="#Attention-is-all-your-need" class="headerlink" title="Attention is all your need"></a>Attention is all your need</h2><p>尽管attention机制由来已久，但真正令其声名大噪的是google 2017年的这篇名为《attention is all your need》的论文。</p><p>让我们从一个简单的例子看起：</p><p>假设我们想用机器翻译的手段将下面这句话翻译成中文：</p><p>“The animal didn’t cross the street because it was too tired”</p><p>当机器读到“it”时，“it”代表“animal”还是“street”呢？对于人类来讲，这是一个极其简单的问题，但是对于机器或者说算法来讲却十分不容易。</p><p>self-Attention则是处理此类问题的一个解决方案，也是目前看起来一个比较好的方案。当模型处理到“it”时，self-Attention可以将“it”和“animal‘联系到一起。</p><p>它是怎么做到的呢？</p><p>通俗地讲，当模型处理一句话中某一个位置的单词时，self-Attention允许它看一看这句话中其他位置的单词，看是否能够找到能够一些线索，有助于更好地表示（或者说编码）这个单词。</p><p>如果你对RNN比较熟悉的话，我们不妨做一个比较。RNN通过保存一个隐藏态，将前面词的信息编码后依次往后面传递，达到利用前面词的信息来编码当前词的目的。而self-Attention仿佛有个上帝之眼，纵观全局，看看上下文中每个词对当前词的贡献。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/1.png" alt=""></center><p>下面来看下具体是怎么实现的。</p><h2 id="Self-Attention-in-Detail"><a href="#Self-Attention-in-Detail" class="headerlink" title="Self-Attention in Detail"></a>Self-Attention in Detail</h2><p>首先，来看下怎样使用向量来计算self-attention，紧接着看如何用矩阵来计算self-attention。</p><h3 id="使用向量"><a href="#使用向量" class="headerlink" title="使用向量"></a>使用向量</h3><p>如下图所示，一般而言，输入的句子进入模型的第一步是对单词进行embedding，每个单词对应一个embedding。对于每个embedding，我们创建三个向量，Query、Key和Value向量。我们如何来创建三个向量呢？如图，我们假设embedding的维度为4，我们希望得到一个维度为3的Query、Key和Value向量，只需将每个embedding乘上一个维度为4*3的矩阵即可。这些矩阵就是训练过程中要学习的。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/2.png" alt=""><br></center><p>那么，Query、Key和Value向量代表什么呢？他们在attention的计算中发挥了什么样的作用呢？</p><p>我们用一个例子来说明：</p><p>首先要明确一点，self-attention其实是在计算每个单词对当前单词的贡献，也就是对每个单词对当前单词的贡献进行打分score。假设我们现在要计算下图中所有单词对第一个单词”Thinking”的打分。那么分分数如何计算呢，只需要将该单词的Query向量和待打分单词的Key向量做点乘即可。比如，第一个单词对第一个单词的分数为q1× k1，第二个单词对第一个单词的分数为q1×k2。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/3.png" alt=""><br></center><p>我们现在得到了两个单词对第一个单词的打分（分数是个数字了），然后将其进行softmax归一化。需要注意的是，在BERT模型中，作者在softmax之前将分数除以了Key的维度的平方根（据说可以保持梯度稳定）。softmax得到的是每个单词在Thinking这个单词上的贡献的权重。显然，当前单词对其自身的贡献肯定是最大的。</p><p>接着就是Value向量登场的地方了。将上面的分数分别和Value向量相乘，注意这里是对应位置相乘。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/4.png" alt=""><br></center><p>最后，将相乘的结果求和，这就得到了self-attention层对当前位置单词的输出。对每个单词进行如上操作，就能得到整个句子的attention输出了。在实际使用过程中，一般采用矩阵计算使整个过程更加高效。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/5.png" alt=""><br></center><p>在开始矩阵计算之前，先回顾总结一下上面的步骤：</p><ul><li>创建Query、Key、Value向量</li><li>计算每个单词在当前单词的分数</li><li>将分数归一化后与Value相乘</li><li>求和</li></ul><p>值得注意的是，上面阐述的过程实际上是Attention机制的计算流程，对于self-Attention，Query=Value。</p><h3 id="Matrix-Calculation-of-Self-Attention"><a href="#Matrix-Calculation-of-Self-Attention" class="headerlink" title="Matrix Calculation of Self-Attention"></a>Matrix Calculation of Self-Attention</h3><p>其实矩阵计算就是将上面的向量放在一起，同时参与计算。</p><p>首先，将embedding向量pack成一个矩阵X。假设我们有一句话有长度为10，embedding维度为4，那么X的维度为（10 × 4）.</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/6.png" alt=""><br></center><p>假设我们设定Q、K、V的维度为3.第二步我们构造一个维度为（4×3）的权值矩阵。将其与X做矩阵乘法，得到一个10×3的矩阵，这就能得到Query了。依样画葫芦，同样可以得到Key和Value。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/7.png" alt=""><br></center><p>最后，将Query和Key相乘，得到打分，然后经过softmax，接着乘上V的到最终的输出。</p><center><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/BERT_1/8.png" alt=""><br></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;BERT解读（一）self-Attention&quot;&gt;&lt;a href=&quot;#BERT解读（一）self-Attention&quot; class=&quot;headerlink&quot; title=&quot;BERT解读（一）self-Attention&quot;&gt;&lt;/a&gt;BERT解读（一）self-Att
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="BERT" scheme="http://state-of-art.top/tags/BERT/"/>
    
      <category term="Attention" scheme="http://state-of-art.top/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>Torchtext读取JSON数据</title>
    <link href="http://state-of-art.top/2018/12/31/TORCHTEXT%E8%AF%BB%E5%8F%96JSON%E6%95%B0%E6%8D%AE/"/>
    <id>http://state-of-art.top/2018/12/31/TORCHTEXT读取JSON数据/</id>
    <published>2018-12-31T15:50:08.000Z</published>
    <updated>2018-12-31T07:38:16.994Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在文本预处理一节，介绍了如何利用torchtext读取tsv格式的文本数据。对于分类问题，这是足够的。但是在处理如NER和机器翻译等问题时，我们构造的输入通常就不是（类别，序列）这样的结构了，而是（序列，序列）。另一方面，在搭建混合网络时，有时我们希望能够给模型多个输入（例如cnn-bilstm-crf中，既需要字符又需要单词输入），这超过了tsv所能。因此要另辟蹊径。</p><p>尽管Torchtext封装了一个SequenceTaggingDataset类用于构造NER数据，但是在实际使用中发现，十分不方便生成batch。</p><p>json格式是采取字典的方式存储数据，这带来了很大的灵活性。但是关于其完整使用的相关文章非常有限，官方文档也未给出详细案例（不得不吐槽一下torchtext的说明文档）。</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>下面以NER任务为例， 进行json使用说明，以抛砖引玉。事实上，这种方法可以无缝地拓展到其他的任务上去，如文本分类，机器翻译等。</p><p>对于NER任务，标签(target)和输入(source)都是同样长度的序列。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">source: </span><br><span class="line">人 民 网 <span class="number">1</span> 月 <span class="number">1</span> 日 讯 据 《 纽 约 时 报 》 报 道 , 美 国 华 尔 街 股 市 在 <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">3</span> 年 的 最 后 一 天 继 续 上 涨 , 和 全 球 股 市 一 样 , 都 以 最 高 纪 录 或 接 近 最 高 纪 录 结 束 本 年 的 交 易 。</span><br><span class="line">target:</span><br><span class="line">O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O O O O B_LOC I_LOC I_LOC I_LOC I_LOC O O O B_T I_T I_T I_T I_T O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O</span><br></pre></td></tr></table></figure><h3 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h3><p>一般来说，为了节省内存，会先将其做个字符-ID映射(这里只是举例说明，和上面不是同一段文本)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">source:</span><br><span class="line"><span class="number">5627</span> <span class="number">5580</span> <span class="number">5550</span> <span class="number">5636</span> <span class="number">4509</span> <span class="number">5192</span> <span class="number">5466</span> <span class="number">5463</span> <span class="number">5624</span> <span class="number">5520</span> <span class="number">4871</span> <span class="number">5637</span> <span class="number">5607</span> <span class="number">5411</span> <span class="number">5313</span> <span class="number">5251</span> <span class="number">5528</span> <span class="number">5628</span> <span class="number">5580</span> <span class="number">5612</span> <span class="number">5292</span> <span class="number">5636</span> <span class="number">5626</span> <span class="number">5637</span> <span class="number">4810</span></span><br><span class="line">target:</span><br><span class="line"><span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">9</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="构造Json格式的文件"><a href="#构造Json格式的文件" class="headerlink" title="构造Json格式的文件"></a>构造Json格式的文件</h3><p>因为我们最终是通过torchtext来生成batch，所以首先要把数据存储为torchtext能够读取的json格式。值得注意的是，torchtext能够读取的json文件和我们一般意义上的json文件格式是不同的（这也是比较坑的地方），我们需要把上面的数据处理成如下格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br><span class="line">&#123;<span class="string">"source"</span>: <span class="string">"10 111 2 3"</span>, <span class="string">"target"</span>: <span class="string">"1 1 2 2"</span>&#125;</span><br></pre></td></tr></table></figure><p>可以看到，里面的内容和通常的Json并无区别，每个字段采用字典的格式存储。不同的是，多个json序列中间是以换行符隔开的，而且最外面没有列表。</p><p>那么怎么构造这样的数据呢？</p><p>我们知道python中的json模块是用来处理json文件的，但是对所有序列进行json.dump()后的结果并非我们想要的（序列之间不是以换行符隔开的），几经尝试，找到如下的处理方式，仅做参考，可能还有更好的处理方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">    <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">        sent = <span class="string">' '</span>.join([str(w) <span class="keyword">for</span> w <span class="keyword">in</span> sent])</span><br><span class="line">        label = <span class="string">' '</span>.join([str(l) <span class="keyword">for</span> l <span class="keyword">in</span> label])</span><br><span class="line">        df = &#123;<span class="string">"source"</span>: sent, <span class="string">"target"</span>: label&#125;</span><br><span class="line">        encode_json = json.dumps(df)</span><br><span class="line">        <span class="comment"># 一行一行写入，并且采用print到文件的方式</span></span><br><span class="line">        print(encode_json, file=fw)</span><br></pre></td></tr></table></figure><p>这里采用的是一行一行进行dumps， 然后print到文件，就能得到我们想要的格式了。</p><p>接下来就是使用torchtext读取了，这个和之前处理tsv文件并无太大差异。</p><h3 id="Torchtext读取"><a href="#Torchtext读取" class="headerlink" title="Torchtext读取"></a>Torchtext读取</h3><p>这里和之前处理tsv类似，不多赘述，只是将几个不同的点提出来说一下。</p><p>（1）Field的定义，这里source和target都是序列，因此两个字段的定义方式基本相同</p><p>（2）传入TabularDataset的fields和tsv的定义有所不同，这里定义成字典-元组格式</p><p>（3） TabularDataset的format要指定成json格式</p><p>（4）pad_token根据需要，一般来说source使用0作为padding， target使用-1进行padding</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line">    SOURCE = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                 use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                 fix_length=self.fix_length,   <span class="comment">#  如需静态padding,则设置fix_length, 但要注意要大于文本最大长度</span></span><br><span class="line">                 eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                 include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    TARGET = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                 use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                 fix_length=self.fix_length,   <span class="comment">#  如需静态padding,则设置fix_length, 但要注意要大于文本最大长度</span></span><br><span class="line">                 eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                 include_lengths=<span class="keyword">False</span>, pad_token=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    fields = &#123;<span class="string">'source'</span>: (<span class="string">'source'</span>, SOURCE), <span class="string">'target'</span>: (<span class="string">'target'</span>, TARGET)&#125;</span><br><span class="line"></span><br><span class="line">    train, valid = TabularDataset.splits(</span><br><span class="line">        path=config.ROOT_DIR,</span><br><span class="line">        train=self.train_path, validation=self.valid_path,</span><br><span class="line">        format=<span class="string">"json"</span>,</span><br><span class="line">        skip_header=<span class="keyword">False</span>,</span><br><span class="line">        fields=fields)</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_iterator</span><span class="params">(self, train, valid)</span>:</span></span><br><span class="line">    train_iter = BucketIterator(train,</span><br><span class="line">                                batch_size=self.batch_size,</span><br><span class="line">                                device = torch.device(<span class="string">"cpu"</span>),  <span class="comment"># cpu by -1, gpu by 0</span></span><br><span class="line">                                sort_key=<span class="keyword">lambda</span> x: len(x.source), <span class="comment"># field sorted by len</span></span><br><span class="line">                                sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                repeat=<span class="keyword">False</span>)</span><br><span class="line">    val_iter = BucketIterator(valid,</span><br><span class="line">                                batch_size=self.batch_size,</span><br><span class="line">                                device=torch.device(<span class="string">"cpu"</span>),  <span class="comment"># cpu by -1, gpu by 0</span></span><br><span class="line">                                sort_key=<span class="keyword">lambda</span> x: len(x.source),  <span class="comment"># field sorted by len</span></span><br><span class="line">                                sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                repeat=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;在文本预处理一节，介绍了如何利用torchtext读取tsv格式的文本数据。对于分类问题，这是足够的。但是在处理如NER和机器翻译等问题时，
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="torchtext" scheme="http://state-of-art.top/tags/torchtext/"/>
    
      <category term="json" scheme="http://state-of-art.top/tags/json/"/>
    
      <category term="batch" scheme="http://state-of-art.top/tags/batch/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的代码实现（四）</title>
    <link href="http://state-of-art.top/2018/12/31/BiLSTM%E6%A8%A1%E5%9E%8B%E4%B8%ADCRF%E5%B1%82%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%EF%BC%88%E5%9B%9B%EF%BC%89%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>http://state-of-art.top/2018/12/31/BiLSTM模型中CRF层的运行原理（四） 代码实现/</id>
    <published>2018-12-31T15:30:08.000Z</published>
    <updated>2018-12-31T07:37:02.217Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现"><a href="#CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现" class="headerlink" title="CRF ON THE TOP OF BILSTM（四） 代码实现"></a>CRF ON THE TOP OF BILSTM（四） 代码实现</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>前面我们重点介绍了CRF的原理，损失函数以及分数的计算。本节将结合前面的相关内容，介绍BILSTM+CRF代码实现（pytorch 0.4.0）及一些需要注意的细节。</p><h2 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h2><p>BILSTM+CRF模型由BILSTM， CRF， 损失函数， 预测函数几部分组成。BILSTM的输出作为CRF的输入，损失函数定义在CRF中， 损失函数使用前向算法，预测函数使用Viterbi算法，下面逐一介绍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BISLTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word_embedding_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word2id,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, bi_flag,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layer, input_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cell_type, dropout,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_tag, checkpoint_dir)</span>:</span></span><br><span class="line">        super(BISLTM_CRF, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, word_embedding_dim)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            p.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.embedding.weight.data.copy_(torch.from_numpy(get_embedding(vocab_size,</span><br><span class="line">                                                                        word_embedding_dim,</span><br><span class="line">                                                                        word2id)))</span><br><span class="line"></span><br><span class="line">        self.rnn = RNN(hidden_size, bi_flag,</span><br><span class="line">                       num_layer, input_size,</span><br><span class="line">                       cell_type, dropout, num_tag)</span><br><span class="line"></span><br><span class="line">        self.crf = CRF(num_tag=num_tag)</span><br><span class="line"></span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        rnn_output = self.rnn(embeddings, length)     <span class="comment"># (batch_size, time_steps, num_tag+2)</span></span><br><span class="line">        <span class="keyword">return</span> rnn_output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(self, rnn_output, labels, length)</span>:</span></span><br><span class="line">        loss = self.crf.negative_log_loss(inputs=rnn_output, length=length, tags=labels)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, rnn_output, length)</span>:</span></span><br><span class="line">        best_path = self.crf.get_batch_best_path(rnn_output, length)</span><br><span class="line">        <span class="keyword">return</span> best_path</span><br></pre></td></tr></table></figure><h2 id="BILSTM"><a href="#BILSTM" class="headerlink" title="BILSTM"></a>BILSTM</h2><p>BILSTM模型这里采用的是双向GRU。GRU的输出（维度为 batch_size <em> time_steps </em> hidden_size）经过线性层（hidden_size <em> num_tag）变为(batch_size </em> time_steps <em> num_tag), 这是最终的输出结果。其中，hidden_size是GRU隐藏层个数</em>2(因为是双向)， num_tag 为需要标注的标签个数。</p><h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>对于一个句子，损失函数为这个句子的所有可能的路径分数之和减去真实路径分数。值得注意的是，这里的“路径”指的句子中的有小部分，不包括padding部分。</p><p>对于一个batch，我们将所有句子的损失函数相加，然后除以总的句子长度，将损失分配到每个字上，作为训练的损失函数。</p><p>这里的重点在于计算真实路径分数和所有路径分数之和，下面来看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">negative_log_loss</span><span class="params">(self, inputs, length, tags)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    features:(batch_size, time_step, num_tag)</span></span><br><span class="line"><span class="string">    target_function = P_real_path_score/P_all_possible_path_score</span></span><br><span class="line"><span class="string">                    = exp(S_real_path_score)/ sum(exp(certain_path_score))</span></span><br><span class="line"><span class="string">    我们希望P_real_path_score的概率越高越好，即target_function的值越大越好</span></span><br><span class="line"><span class="string">    因此，loss_function取其相反数，越小越好</span></span><br><span class="line"><span class="string">    loss_function = -log(target_function)</span></span><br><span class="line"><span class="string">                  = -S_real_path_score + log(exp(S_1 + exp(S_2) + exp(S_3) + ...))</span></span><br><span class="line"><span class="string">                  = -S_real_path_score + log(all_possible_path_score)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.use_cuda:</span><br><span class="line">        inputs = inputs.cpu()</span><br><span class="line">        length = length.cpu()</span><br><span class="line">        tags = tags.cpu()</span><br><span class="line"></span><br><span class="line">    loss = Variable(torch.tensor(<span class="number">0.</span>), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    num_chars = torch.sum(length.data).float()      <span class="comment"># 所有字的个数</span></span><br><span class="line">    <span class="keyword">for</span> ix, (features, tag) <span class="keyword">in</span> enumerate(zip(inputs, tags)):</span><br><span class="line">        features = features[:length[ix]]</span><br><span class="line">        tag = tag[:length[ix]]</span><br><span class="line">        real_score = self.real_path_score(features, tag)      <span class="comment"># 真实路径分数</span></span><br><span class="line">        total_score = self.all_possible_path_score(features)  <span class="comment"># 所有可能的路径分数</span></span><br><span class="line">        cost = total_score - real_score</span><br><span class="line">        loss  = loss + cost</span><br><span class="line">    <span class="keyword">return</span> loss/num_chars             <span class="comment"># 分配到每个字的损失</span></span><br></pre></td></tr></table></figure><h3 id="转移矩阵"><a href="#转移矩阵" class="headerlink" title="转移矩阵"></a>转移矩阵</h3><p>在介绍分数计算之前，先来看下CRF中的一个重要的训练参数，转移矩阵。转移矩阵的定义决定了后面前向算法和维特比算法的实现，因此十分重要。</p><p>我们的标签序列id映射定义为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tag_to_ix = &#123;</span><br><span class="line">    <span class="string">"B_PER"</span>: <span class="number">0</span>,   <span class="comment"># 人名</span></span><br><span class="line">    <span class="string">"I_PER"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"B_LOC"</span>: <span class="number">2</span>,   <span class="comment"># 地点</span></span><br><span class="line">    <span class="string">"I_LOC"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"B_ORG"</span>: <span class="number">4</span>,   <span class="comment"># 机构</span></span><br><span class="line">    <span class="string">"I_ORG"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"B_T"</span>: <span class="number">6</span>,     <span class="comment"># 时间</span></span><br><span class="line">    <span class="string">"I_T"</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">"O"</span>: <span class="number">8</span>,       <span class="comment"># 其他</span></span><br><span class="line">    <span class="string">"SOS"</span>: <span class="number">9</span>,     <span class="comment"># 起始符</span></span><br><span class="line">    <span class="string">"EOS"</span>:<span class="number">10</span>      <span class="comment"># 结束符</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>转移矩阵的大小为：(num_tag +2, num_tag +2)。+2是因为包含了起始符号和结束符号。</p><p>并且，P_jk 表示从tag_j到tag_k的分数，这样就有：</p><p>（1）P_j* 表示所有从tag_j出发的边</p><p>（2） P_*k 表示所有到tag_k的边</p><p>此外，在均匀初始化时，设定起始符号和结束符号对应的转移分数，使得：</p><p>（1） 从EOS-&gt;其他标签为不可能事件, 如果发生，则产生一个极大的损失</p><p>（2） 从其他标签-&gt;SOS为不可能事件，如果发生，则产生一个极大的损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tag, use_cuda=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> num_tag &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Invalid value of num_tag: %d"</span> % num_tag)</span><br><span class="line">    super(CRF, self).__init__()</span><br><span class="line">    self.num_tag = num_tag</span><br><span class="line">    self.start_tag = num_tag</span><br><span class="line">    self.end_tag = num_tag + <span class="number">1</span></span><br><span class="line">    self.use_cuda = use_cuda</span><br><span class="line">    <span class="comment"># 转移矩阵transitions：P_jk 表示从tag_j到tag_k的分数</span></span><br><span class="line">    <span class="comment"># P_j* 表示所有从tag_j出发的边</span></span><br><span class="line">    <span class="comment"># P_*k 表示所有到tag_k的边</span></span><br><span class="line">    self.transitions = nn.Parameter(torch.Tensor(num_tag + <span class="number">2</span>, num_tag + <span class="number">2</span>))</span><br><span class="line">    nn.init.uniform_(self.transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">    self.transitions.data[self.end_tag, :] = <span class="number">-10000</span>   <span class="comment"># 表示从EOS-&gt;其他标签为不可能事件, 如果发生，则产生一个极大的损失</span></span><br><span class="line">    self.transitions.data[:, self.start_tag] = <span class="number">-10000</span>   <span class="comment"># 表示从其他标签-&gt;SOS为不可能事件, 同上</span></span><br></pre></td></tr></table></figure><h3 id="真实路径分数"><a href="#真实路径分数" class="headerlink" title="真实路径分数"></a>真实路径分数</h3><p>真实路径分数由发射分数和转移分数组成。其中，转移分数是CRF需要训练的参数，我们随机初始化；发射分数是BISTLM的输出矩阵（对于每个句子，其维度为 time_steps * num_tag）， 对于每个tag， 该矩阵都给出一个分数，我们知道了真实的标签序列， 拿这个标签去索引该矩阵，即可得到真实路径的发射分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">real_path_score</span><span class="params">(self, features, tags)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    features: (time_steps, num_tag)</span></span><br><span class="line"><span class="string">    real_path_score表示真实路径分数</span></span><br><span class="line"><span class="string">    它由Emission score和Transition score两部分相加组成</span></span><br><span class="line"><span class="string">    Emission score由LSTM输出结合真实的tag决定，表示我们希望由输出得到真实的标签</span></span><br><span class="line"><span class="string">    Transition score则是crf层需要进行训练的参数，它是随机初始化的，表示标签序列前后间的约束关系（转移概率）</span></span><br><span class="line"><span class="string">    Transition矩阵存储的是标签序列相互间的约束关系</span></span><br><span class="line"><span class="string">    在训练的过程中，希望real_path_score最高，因为这是所有路径中最可能的路径</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    r = torch.LongTensor(range(features.size(<span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">if</span> self.use_cuda:</span><br><span class="line">        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.start_tag]), tags])</span><br><span class="line">        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.end_tag])])</span><br><span class="line">        r = r.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pad_start_tags = torch.cat([torch.LongTensor([self.start_tag]), tags])</span><br><span class="line">        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.end_tag])])</span><br><span class="line">    <span class="comment"># Transition score + Emission score</span></span><br><span class="line">    score = torch.sum(self.transitions[pad_start_tags, pad_stop_tags]).cpu() + torch.sum(features[r, tags])</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><h3 id="所有路径分数之和"><a href="#所有路径分数之和" class="headerlink" title="所有路径分数之和"></a>所有路径分数之和</h3><p>所有路径分数之和采用前向算法计算，算法复杂度为O(N <em> N </em> T)， 其中N为标签个数，T为序列长度。</p><p>这里， 将start_tag的发射分数初始化为0，从start_tag开始，沿着序列一个字一个字地计算前向分数。</p><p>最后，加上end_tag的转移分数，即为所有START_TAG -&gt; 1st word -&gt; 2nd word -&gt;…-&gt;END_TAG的分数。</p><p>事实上，前向算法的精髓在于，用指数将每个字的分数用三个数存起来，而这三个数后面又是可以拆分的(指数的计算特性，乘法拆成加法。）ps：如果对前向算法不是很明白或者大致明白但不是特别清晰，建议画个图看下就一目了然了。</p><p>需要注意的是，根据前向算法，需要计算指数和的log， log_sum_exp， 由于前向方法是一个不断累积的过程， 会导致exp之和趋于无穷大，超过计算机的浮点数最大值限制，出现“上溢”。避免这种做法的手段是，找到整个矩阵的最大值，将矩阵减去该最大值进行指数求和，最终结果再加上该最大值即可。减去最大值后求指数和尽管也会出现无穷大的情况，但是这时是在分母上，该项可以计算为0，这样我们取log后仍可以得到一个合理的数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_possible_path_score</span><span class="params">(self, features)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算所有可能的路径分数的log和：前向算法</span></span><br><span class="line"><span class="string">    step1: 将forward列expand成9*9</span></span><br><span class="line"><span class="string">    step2: 将下个单词的emission行expand成9*9</span></span><br><span class="line"><span class="string">    step3: 将1和2和对应位置的转移矩阵相加</span></span><br><span class="line"><span class="string">    step4: 更新forward，合并行</span></span><br><span class="line"><span class="string">    step5: 取forward指数的对数计算total</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    time_steps = features.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    forward = Variable(torch.zeros(self.num_tag))     <span class="comment"># 初始化START_TAG的发射分数为0</span></span><br><span class="line">    <span class="keyword">if</span> self.use_cuda:</span><br><span class="line">        forward = forward.cuda()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, time_steps):  <span class="comment"># START_TAG -&gt; 1st word -&gt; 2nd word -&gt;...-&gt;END_TAG</span></span><br><span class="line">        emission_start = forward.expand(self.num_tag, self.num_tag).t()</span><br><span class="line">        emission_end = features[i,:].expand(self.num_tag, self.num_tag)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            trans_score = self.transitions[self.start_tag, :self.start_tag].cpu()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_score = self.transitions[:self.start_tag, :self.start_tag].cpu()</span><br><span class="line">        sum = emission_start + emission_end + trans_score</span><br><span class="line">        forward = log_sum(sum, dim=<span class="number">0</span>)</span><br><span class="line">    forward = forward + self.transitions[:self.start_tag, self.end_tag].cpu()  <span class="comment"># END_TAG</span></span><br><span class="line">    total_score = log_sum(forward, dim=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> total_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum</span><span class="params">(matrix, dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向算法是不断累积之前的结果，这样就会有个缺点</span></span><br><span class="line"><span class="string">    指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf</span></span><br><span class="line"><span class="string">    为了避免这种情况，我们做了改动：</span></span><br><span class="line"><span class="string">    1. 用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算</span></span><br><span class="line"><span class="string">    SUM = log(exp(s1)+exp(s2)+...+exp(s100))</span></span><br><span class="line"><span class="string">        = log&#123;exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]&#125;</span></span><br><span class="line"><span class="string">        = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]</span></span><br><span class="line"><span class="string">    where clip=max</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    clip_value = torch.max(matrix)                 <span class="comment"># 极大值</span></span><br><span class="line">    clip_value = int(clip_value.data.tolist())</span><br><span class="line">    log_sum_value = clip_value + torch.log(torch.sum(torch.exp(matrix-clip_value), dim=dim))</span><br><span class="line">    <span class="keyword">return</span> log_sum_value</span><br></pre></td></tr></table></figure><p>整个训练部分至此就介绍完了。下面介绍使用维特比算法进行预测。</p><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>预测算法迭代过程和前向算法类似。不同的是，forward不再是到该点的路径分数之和，而是到该点的所有路径中最大分数， 这样我们就有了到该点的最大路径。同时，在迭代过程中 ，我们将最大路径中到当前节点（具有最大分数）的前一个节点的索引存储下来，作为最佳路径点。</p><p>中间的迭代过程较容易理解，比较绕的是</p><p>（1） 如何处理起始符号和结束符号</p><p>（2） 最后一个节点怎么找</p><p>我们只需要把握一个点，上面的问题就能迎刃而解：</p><p>预测和回溯的时候，我们要从START_TAG -&gt; 1st word -&gt; 2nd word -&gt;…-&gt;END_TAG，一个都不能少！</p><p>最后奉上一份PPT<url><a href="https://baidupan.com，" target="_blank" rel="noopener">https://baidupan.com，</a> 里面用图形化的方式对CRF原理进行了详细的描述。</url></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        time_steps = features.size(<span class="number">0</span>)</span><br><span class="line">        forward = Variable(torch.zeros(self.num_tag))  <span class="comment"># START_TAG</span></span><br><span class="line">        <span class="keyword">if</span> self.use_cuda:</span><br><span class="line">            forward = forward.cuda()</span><br><span class="line">        <span class="comment"># back_points 到该点的最大分数  last_points 前一个点的索引</span></span><br><span class="line">        back_points, index_points = [self.transitions[self.start_tag, :self.start_tag].cpu()], [torch.LongTensor([<span class="number">-1</span>]).expand_as(forward)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, time_steps):  <span class="comment"># START_TAG -&gt; 1st word -&gt; 2nd word -&gt;...-&gt;END_TAG</span></span><br><span class="line">            emission_start = forward.expand(self.num_tag, self.num_tag).t()</span><br><span class="line">            emission_end = features[i,:].expand(self.num_tag, self.num_tag)</span><br><span class="line">            trans_score = self.transitions[:self.start_tag, :self.start_tag].cpu()</span><br><span class="line">            sum = emission_start + emission_end + trans_score</span><br><span class="line">            forward, index = torch.max(sum.detach(), dim=<span class="number">0</span>)</span><br><span class="line">            back_points.append(forward)</span><br><span class="line">            index_points.append(index)</span><br><span class="line">        back_points.append(forward + self.transitions[:self.start_tag, self.end_tag].cpu())  <span class="comment"># END_TAG</span></span><br><span class="line">        <span class="keyword">return</span> back_points, index_points</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_best_path</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        back_points, index_points = self.viterbi(features)</span><br><span class="line">        <span class="comment"># 找到线头</span></span><br><span class="line">        best_last_point = argmax(back_points[<span class="number">-1</span>])</span><br><span class="line">        index_points = torch.stack(index_points)   <span class="comment"># 堆成矩阵</span></span><br><span class="line">        m = index_points.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 初始化矩阵</span></span><br><span class="line">        best_path = [best_last_point]</span><br><span class="line">        <span class="comment"># 循着线头找到其对应的最佳路径</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            best_index_point = index_points[i][best_last_point]</span><br><span class="line">            best_path.append(best_index_point)</span><br><span class="line">            best_last_point = best_index_point</span><br><span class="line">        best_path.reverse()</span><br><span class="line">        <span class="keyword">return</span> best_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch_best_path</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.use_cuda:</span><br><span class="line">            inputs = inputs.cpu()</span><br><span class="line">            length = length.cpu()</span><br><span class="line">        max_len = inputs.size(<span class="number">1</span>)</span><br><span class="line">        batch_best_path = []</span><br><span class="line">        <span class="keyword">for</span> ix, features <span class="keyword">in</span> enumerate(inputs):</span><br><span class="line">            features = features[:length[ix]]</span><br><span class="line">            best_path = self.get_best_path(features)</span><br><span class="line">            best_path = torch.Tensor(best_path).long()</span><br><span class="line">            best_path = padding(best_path, max_len)</span><br><span class="line">            batch_best_path.append(best_path)</span><br><span class="line">        batch_best_path = torch.stack(batch_best_path, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> batch_best_path</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(matrix, dim=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""(0.5, 0.4, 0.3) —&gt; 0"""</span></span><br><span class="line">    _, index = torch.max(matrix, dim=dim)</span><br><span class="line">    <span class="keyword">return</span> index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding</span><span class="params">(vec, max_len, pad_token=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    new_vec = torch.zeros(max_len).long()</span><br><span class="line">    new_vec[:vec.size(<span class="number">0</span>)] = vec</span><br><span class="line">    new_vec[vec.size(<span class="number">0</span>):] = pad_token</span><br><span class="line">    <span class="keyword">return</span> new_vec</span><br></pre></td></tr></table></figure><p>完整代码请戳 <a href="https://github.com/circlePi/NER/BILSTM_CRF" target="_blank" rel="noopener">https://github.com/circlePi/NER/BILSTM_CRF</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现&quot;&gt;&lt;a href=&quot;#CRF-ON-THE-TOP-OF-BILSTM（四）-代码实现&quot; class=&quot;headerlink&quot; title=&quot;CRF ON THE TOP OF BILSTM（四） 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的运行原理(三)</title>
    <link href="http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--3/"/>
    <id>http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--3/</id>
    <published>2018-12-31T15:27:08.000Z</published>
    <updated>2018-12-31T07:22:24.487Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的部分中，我们学习了BiLSTM-CRF模型的结构和CRF损失函数的细节。您可以通过各种开源框架（Keras，Chainer，TensorFlow等）实现您自己的BiLSTM-CRF模型。最重要的事情之一是在这些框架上自动计算模型的反向传播，因此您不需要自己实现反向传播来训练模型（即计算梯度和更新参数）。此外，一些框架已经实现了CRF层，因此只需添加一行代码就可以非常轻松地将CRF层与您自己的模型相结合。</p><p>在本节中，我们将探讨如何预测句子的标签序列。</p><a id="more"></a><h2 id="1-预测句子的标签序列"><a href="#1-预测句子的标签序列" class="headerlink" title="1. 预测句子的标签序列"></a>1. 预测句子的标签序列</h2><p>第1步：BiLSTM-CRF模型的emission score和transition score<br>假设句子$x$包含３个字符：$x=[w_0, w_1, w_2]$。此外，假设我们已经获得了BiLSTM模型的emission score和CRF层的transition score：</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$w_0$</td><td>$w_{01}$</td><td>$w_{02}$</td></tr><tr><td>$w_1$</td><td>$w_{11}$</td><td>$w_{12}$</td></tr><tr><td>$w_2$</td><td>$w_{21}$</td><td>$w_{22}$</td></tr></tbody></table><p>$x_{ij}$表示$w_i$被标记为$l_j$的得分。</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$l_1$</td><td>$t_{11}$</td><td>$t_{12}$</td></tr><tr><td>$l_2$</td><td>$t_{21}$</td><td>$t_{22}$</td></tr></tbody></table><p>$t_{ij}$表示标签$i$转移到标签$j$的分数</p><p>第2步：预测<br>如果您熟悉Viterbi算法，这部分对您来说很容易。如果你不太了解的话，请不要担心，我将逐步解释算法。对于一个句子，我们将按从左到右的方式进行预测，即:</p><p>$w_0$<br>$w_0$–&gt;$w_1$<br>$w_0$–&gt;$w_1$–&gt;$w_2$</p><p>你将看到两个变量：$obs$和$previous$。$previous$存储前面步骤的最终结果。$obs$表示来自当前单元的信息。</p><hr><p>首先，我们来看看$w_0$，即:</p><p>$obs=[x_{01}, x_{02}]$</p><p>$previous=None$</p><p>对于第一个字符$w_0$。$w_0$的最优标签很简单。例如，如果$obs=[x_{01}=0.2,   x_{02}=0.8]$，显然，$w_0$的最优标签是$l_2$，由于只有一个字符从而无标签到标签之间的transition，因此不用计算transition scores。</p><hr><p>接着，对于$w_0$ –&gt; $w_1$：</p><p>$obs=[x_{11,  x_{12}}]$</p><p>$previous=[x_{01}, x_{02}]$</p><p>1).扩展$previous$为：</p><p>$$previous=\left(^{previous[0] \quad previous[0]}<em>{previous[1] \quad previous[1]}\right)=\left(^{x</em>{01} \quad x_{01}}_{x_{02} \quad x_{02}}\right)$$</p><p>2).扩展$obs$为：</p><p>$$obs=\left(^{obs[0] \quad obs[1]}<em>{obs[0] \quad obs[1]}\right)=\left(^{x</em>{11} \quad x_{12}}_{x_{11} \quad x_{12}}\right)$$</p><p>3).将$previous$，$obs$和transition score进行求和:</p><p>$$scores=\left(^{x_{01} \quad x_{01}}_{x_{02} \quad x_{02}}\right) + \left(^{x_{11} \quad x_{12}}_{x_{11} \quad x_{12}}\right) + \left(^{t_{11} \quad t_{12}}_{t_{21} \quad t_{22}}\right)$$</p><p>即:</p><p>$$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}_{x_{02}+x_{11}+t{21} \quad x_{02}+x_{12}+t_{22}}\right)$$</p><p>当我们计算所有可能标签序列组合的总得分时，您可能想知道与前一部分没有有区别。接下来我们将看到其中的差异性。</p><p>更新$previous$值：</p><p>$$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])]$$</p><p>假设，我们得到的score为：</p><p>$$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}_{x_{02}+x_{11}+t{21} \quad x_{02}+x_{12}+t_{22}}\right)=\left( ^{0.2 \quad 0.3}_{0.5 \quad 0.4}\right)$$</p><p>则$previous$将更新为:</p><p>$$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])] = [0.5, 0.4]$$</p><p>$previous$是什么意思？$previous$列表存储当前字符对每个标签的最大得分。</p><h3 id="1-1-案例"><a href="#1-1-案例" class="headerlink" title="1.1 案例"></a>1.1 案例</h3><p>假设，在语料库中，我们总共有2个标签，$label1(l_1)$和$label2(l_2)$，这两个标签的索引分别为0和1。</p><p>$previous[0]$是以第0个标签$l_1$结束的序列的最大得分，类似的$previous[1]$是以$l_2$结束的序列的最大得分。在每次迭代过程中，我们仅仅保留每个标签对应的最优序列的信息($previous=[\max(scores[00], scores[10]),\max( scores[01], scores[11])]$)。分数较少的序列信息将被丢弃。</p><p>回到我们的主要任务：</p><p>同时，我们还有两个变量来存储历史信息（分数和索引），即$alpha_0$和$alpha_1$。<br>每次迭代，我们都将最好的得分追加到$alpha_0$。为方便起见，每个标签的最高分都有下划线。</p><p>$$scores=\left(^{x_{01}+x_{11}+t_{11} \quad x_{01}+x_{12}+t_{12}}<em>{\underline{x</em>{02}+x_{11}+t{21}} \quad \underline{x_{02}+x_{12}+t_{22}}}\right)=\left( ^{0.2 \quad 0.3}_{\underline{0.5} \quad \underline{0.4}}\right)$$</p><p>$$alpha_0=[(scores[10],scores[11])]=[(0.5,0.4)]$$</p><p>另外，相应的列的索引被保存在$alpha_1$：</p><p>$$alpha_1=[(ColumnIndex(scores[10]),ColumnIndex(scores[11]))]=[(1,1)]$$</p><p>其中，$l_1$的索引是0，$l_2$的索引是1，所以$(1, 1)=(l_2, l_2)$，这意味着对于当前的单元$w_i$和标签$l^(i)$：<br>$(1, 1)=(l_2, l_2)$=(当序列是$\underline{l^{(i-1)}=l_2} -&gt; \underline{l^{(i)}=l_1}$时我们可以得到最大得分为0.5, 当序列是$\underline{l^{(i-1)}=l_2} -&gt; \underline{l^{(i)}=l_2}$时我们可以得到最大得分为0.4)</p><p>$l^{(i-1)}$是前一个字符$w_{i-1}$对应的标签</p><hr><p>最后，我们计算$w_0$ –&gt; $w_1$ –&gt; $w_2$:</p><p>$obs=[x_{21}, x_{22}]$</p><p>$previous=[0.5, 0.4]$</p><p>1).扩展$previous$为：</p><p>$$previous=\left(^{previous[0] \quad previous[0]}<em>{previous[1] \quad previous[1]}\right)=\left(^{0.5 \quad 0.5}</em>{0.4 \quad 0.4}\right)$$</p><p>2).扩展$obs$为:</p><p>$$obs=\left(^{obs[0] \quad obs[1]}<em>{obs[0] \quad obs[1]}\right)=\left(^{x</em>{21} \quad x_{22}}_{x_{21} \quad x_{22}}\right)$$</p><p>3).对$previous$，$obs$和transition score进行求和:</p><p>$$scores=\left(^{0.5 \quad 0.5}<em>{0.4 \quad 0.4}\right) +\left(^{x</em>{21} \quad x_{22}}_{x_{21} \quad x_{22}}\right)+ \left(^{t_{11} \quad t_{12}}_{t_{21} \quad t_{22}}\right)$$</p><p>即:</p><p>$$scores=\left(^{0.5+x_{21}+t_{11} \quad 0.5+x_{22}+t_{12}}<em>{0.4+x</em>{21}+t{21} \quad 0.4+x_{22}+t_{22}}\right)$$</p><p>更新$previous$为：</p><p>$$previous=[\max (scores[00], scores[10]),\max (scores[01],scores[11])]$$</p><p>比如，我们得到的scores为：</p><p>$$scores=\left( ^{0.6 \quad \underline{0.9}}_{\underline{0.8} \quad 0.7}\right)$$</p><p>因此，$previous$将更新为：</p><p>$$previous=[0.8, 0.9]$$</p><p>实际上，$previousp[0]$和$previous[1]$之间的较大的一个是最好的预测结果的得分。与此同时，每个标签的最大得分和索引将被添加到$alpha_0$和$alpha_1$中：</p><p>$alpha_0=[(0.5,0.4),\underline{(scores[10],scores[01])}]$</p><p>$　　　=[(0.5,0.4),\underline{(0.8,0.9)}]$</p><p>$alpha_1=[(1,1),\underline{(1,0)}]$</p><hr><p>第3步：找出得分最高的最佳序列<br>在该步骤中，我们将根据$previousp[0]$和$previous[1]$找到最高的得分。我们将从右到左的方式反推最优序列，即最后一个单元反推到第一个单元。</p><hr><p>$w_1$ –&gt; $w_2$：<br>首先，检查$alpha_0$和$alpha_1$最后一个元素:(0.8, 0.9)和(1, 0)。0.9是最高分数，其对应的位置是1，因此对应的标签是$l_2$。继续从$alpha_1$中对应位置获得$w_1$对应的标签索引， 即(1, 0)[1]=0。索引0表示$w_1$对应的标签是$l_1$。因此我们可以得到$w_1 -&gt; w_2$的最佳序列是$l_1 -&gt; l_2$。</p><p>$w_0$ –&gt; $w_1$：</p><p>接着，我们继续向前移动并获得$alpha_1$的上一个元素：(1, 1)。从上面可知$w_1$的标签是$l_1$(标签对应的索引为0)，因此我们可以得到$w_0$对应的标签索引为(1,1)[0]=1。所以我们可以得到$w_0 -&gt; w_1$的最佳序列是$l_2 -&gt; l_1$。</p><p>最终可以得到$w_0 -&gt; w_1 -&gt; w_2$的最佳标签序列是$l_2 -&gt; l_1  -&gt; l_2$</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360. </p><p><a href="https://arxiv.org/abs/1603.01360" target="_blank" rel="noopener">https://arxiv.org/abs/1603.01360</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前面的部分中，我们学习了BiLSTM-CRF模型的结构和CRF损失函数的细节。您可以通过各种开源框架（Keras，Chainer，TensorFlow等）实现您自己的BiLSTM-CRF模型。最重要的事情之一是在这些框架上自动计算模型的反向传播，因此您不需要自己实现反向传播来训练模型（即计算梯度和更新参数）。此外，一些框架已经实现了CRF层，因此只需添加一行代码就可以非常轻松地将CRF层与您自己的模型相结合。&lt;/p&gt;
&lt;p&gt;在本节中，我们将探讨如何预测句子的标签序列。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的运行原理(二)</title>
    <link href="http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--2/"/>
    <id>http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--2/</id>
    <published>2018-12-31T15:21:08.000Z</published>
    <updated>2018-12-31T07:38:01.034Z</updated>
    
    <content type="html"><![CDATA[<p>在上一节中，我们知道CRF层可以从训练数据集中自动学习到一些约束规则来保证预测标签的合法性。这些约束包括：</p><ul><li>句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”</li><li>标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-PersonI-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列.</li><li>标签序列“O I-label”是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说，有效的标签序列应该是“O B-label”。</li></ul><p>这一节，我们将解释为什么CRF层会自动学习到这些约束规则。</p><a id="more"></a><h2 id="1-CRF层"><a href="#1-CRF层" class="headerlink" title="1.CRF层"></a>1.CRF层</h2><p>在CRF层损失函数中，有两种形式的score。这些scores是CRF层的关键概念。</p><h3 id="1-1-emission-score"><a href="#1-1-emission-score" class="headerlink" title="1.1 emission score"></a>1.1 emission score</h3><p>第一个是emission score，主要来自BiLSTM层的输出，如下图所示，比如，单元$w_0$标记为‘B-Person’的score为1.5：</p><div align="center"><img src=" https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181228000013.png"></div><p>为了方便起见，我们用数字来表示各个实体标签，对应关系如下：</p><table><thead><tr><th>Label</th><th>Index</th></tr></thead><tbody><tr><td>B-Person</td><td>0</td></tr><tr><td>I-Person</td><td>1</td></tr><tr><td>B-Organization</td><td>2</td></tr><tr><td>I-Organization</td><td>3</td></tr><tr><td>O</td><td>4</td></tr></tbody></table><p>另外，我们使用$x_{iy_j}$表示emission score，其中:</p><ul><li>i 表示词的索引</li><li>$y_j$表示标签的索引</li></ul><p>例如，根据上图，我们可以得到$x_{i=1,y_j=2} = x_{w_1,B-Organization} = 0.1$，也就是说$w_1$标记为’B-Organization‘的score为0.1。</p><h3 id="1-2-Transition-score"><a href="#1-2-Transition-score" class="headerlink" title="1.2 Transition score"></a>1.2 Transition score</h3><p>我们定义$t_{y_iy_j}$表示transition score，从上图中可知$t_{B-Person,I-Person} = 0.9$，也就是说“$B-Person \rightarrow I-Person$“的标签转移得分为0.9。因此，对于所有的标签序列组合，我们将得到一个transition score矩阵，包含了所有标签之间的transition score。</p><p>为了使transition score矩阵更具鲁棒性，我们额外增加两个标签——START 和END，START 代表句子的开始位置，而非第一个词，同理，END代表句子的结束位置.</p><p>表1 中包含了START和END标签的transition score矩阵。</p><div align="center"><img src=" https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181228000035.png"></div><p>如表1所示，我们可以发现transition score矩阵可以学习到好多约束规则，比如:</p><ul><li>句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”（例如，从“START” 到 “I-Person or I-Organization”的score都很低 ）。</li><li>标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理的标签序列（例如，从“B-Organization”到’I-Person’的score只有0.0003，比其他都低。）。</li><li>标签序列“O I-label” 是不合理的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”（例如，$t_{O,I-Person}$的score就非常小）。</li></ul><p>你可能会有一个疑问，该transition score矩阵如何计算得到？</p><p>事实上，该矩阵是BiLSTM-CRF模型的一个参数，在训练模型之前，一般会初始化一个矩阵作为transition score矩阵，在训练过程中，该矩阵中的transition score都会不断地自动更新。也就是说CRF层可以自动学习，通过学习得到一个最优的transition score矩阵，而不用我们手工定义一个transition score矩阵。当模型训练不断地持续，该矩阵会越来越合理。</p><h3 id="1-3-CRF损失函数"><a href="#1-3-CRF损失函数" class="headerlink" title="1.3 CRF损失函数"></a>1.3 CRF损失函数</h3><p>CRF损失函数中包含了真实标签序列得分和所有可能标签序列的总得分，正常情况下，真实标签序列得分在所有可能标签序列得分中是最高的。</p><p>比如，假设数据集中的标签如下所示：</p><table><thead><tr><th>Label</th><th>Index</th></tr></thead><tbody><tr><td>B-Person</td><td>0</td></tr><tr><td>I-Person</td><td>1</td></tr><tr><td>B-Organization</td><td>2</td></tr><tr><td>I-Organization</td><td>3</td></tr><tr><td>O</td><td>4</td></tr><tr><td>START</td><td>5</td></tr><tr><td>END</td><td>6</td></tr></tbody></table><p>那么，在第一节中我们假设的句子$x$，所有可能的标签序列组合为:</p><ul><li>(1) START B-Person B-Person B-Person B-Person B-Person END</li><li>(2) START B-Person I-Person B-Person B-Person B-Person END</li><li>…</li><li>(10) <strong>START B-Person I-Person O B-Organization O END </strong></li><li>…</li><li>(N) O O O O O O O</li></ul><p>假设一共有N中可能的标签序列组合，且第$i$个标签序列的得分为$P_i$，那么所有可能标签序列组合的总得分为：</p><p>$$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ </p><p>按照我们之前的假设，第10个是真实的标签序列，那么，我们想要的结果是第10个标签序列得分在所有可能的标签序列得分中是最高的。</p><p>因此，我们可以定义模型的损失函数，在整个模型训练过程中，BiLSTM-CRF模型的参数不断地进行更新，使得真实标签序列得分在所有可能标签序列组合得分中的占比是最高的。因此，模型的损失函数格式如下所示：</p><p>$$ LossFunction = \frac{P_{RealPath}}{P_1+P_2+…+P_N} $$</p><p>那么，问题就来了：</p><ul><li>如何定义一个标签序列的得分？</li><li>如何计算所有可能标签序列组合的总得分？</li><li>在计算总得分中，一定需要计算每一个可能的标签序列的得分吗？</li></ul><p>接下来，我们来解答每一个问题。</p><h3 id="1-4-真实标签序列得分"><a href="#1-4-真实标签序列得分" class="headerlink" title="1.4 真实标签序列得分"></a>1.4 真实标签序列得分</h3><p>前面我们定义了标签序列得分为$P_i$，以及所有可能标签序列的总得分为：</p><p>$$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ </p><p>其中$e^{S_i}$表示第i个标签序列得分。</p><p>显然，在所有可能的标签序列组合必然存在一个序列是真实标签序列，而剩下的标签序列组合都是错误的，比如序列”<strong>START B-Person I-Person O B-Organization O END </strong>“是正确的，而序列‘START B-Person I-Person B-Person B-Person B-Person END’是错误的。</p><p>在整个模型训练过程中，CRF层的损失函数只需要两个得分：</p><ul><li>一个是真实标签序列得分</li><li>一个是所有可能标签序列组合的总得分</li></ul><p>而我们的学习目的是让真实的标签序列得分在总得分中的占比是最高的。</p><p>对于真实标签序列的得分$e^{S_i}$，我们直接计算$S_i$即可。</p><p>我们使用之前的案例，真实的标签序列为“<strong>START B-Person I-Person O B-Organization O END </strong>”，即：</p><ul><li>句子$x$由5个字符组成，$w_1,w_2,w_3,w_4,w_5$</li><li>我们在句子前后增加两个字符，记为$w_0,w_6$</li><li>$S_i$主要由第一节中提到的Emission Score和Transition Score组成，即$S_i = Emission Score + Transition Score$</li></ul><h4 id="1-4-1-Emission-Score"><a href="#1-4-1-Emission-Score" class="headerlink" title="1.4.1 Emission Score"></a>1.4.1 Emission Score</h4><p>Emission Score计算公式如下所示：</p><p>$Emission Score  = $</p><p>$x_{0,START} + x_{1,B-Person} + x_{2,I-Person} + x_{3,O} +$</p><p>$x_{4,B-Organization} + x_{5,O} + x_{6,END}$</p><p>其中：</p><ul><li>$x_{index,label}$表示第index个词被标记为label的得分</li><li>$x_{1,B-Person}, x_{2,I-Person} , x_{3,O}, x_{4,B-Organization},x_{5,O}$ 为BiLSTM层的输出</li><li>一般$x_{0,START}$和$x_{6,END}$为0</li></ul><h4 id="1-4-2-Transition-Score"><a href="#1-4-2-Transition-Score" class="headerlink" title="1.4.2 Transition Score"></a>1.4.2 Transition Score</h4><p>Transition Score计算公式如下所示:</p><p>$Transition Score  =$<br>$t_{START \rightarrow B-Person} + t_{B-Person \rightarrow I-Person} +$<br>$t_{I-Person \rightarrow O} + t_{O \rightarrow B-Organization} + t_{B-Organization \rightarrow O} + t_{O \rightarrow END}$</p><p>其中:</p><ul><li>$t_{label1 \rightarrow label2}$ 表示$label1$到$label2$的transition Score。</li><li>transition Score主要是在CRF层进行计算的，也就是说，transition Score完全是CRF层的参数。</li></ul><p>因此，我们通过计算$s_i$，可以得到第i条标签序列的得分。</p><h3 id="1-5-所有可能标签序列组合的总得分"><a href="#1-5-所有可能标签序列组合的总得分" class="headerlink" title="1.5 所有可能标签序列组合的总得分"></a>1.5 所有可能标签序列组合的总得分</h3><p>前面，我们计算了单条标签序列得分，接下来，我们需要计算所有可能标签序列的总得分。由之前内容可知，总得分的计算公式为;</p><p>$$ P_{total} = P_1 + P_2 + … + P_N = e^{S_1} + e^{S_2} + … + e^{S_N} $$ </p><p>很显然，总得分计算方式就是每一条标签序列得分的求和，那么我们能想到的最简单的方法就是先计算每一条的标签序列得分，然后将所有的标签序列得分进行相加得到总得分。虽然计算很简单，但是效率不高，需要很长的训练时间。</p><p>接下来,我们将通过公式推导来认识总得分计算过程。</p><h3 id="1-6-CRF的损失函数"><a href="#1-6-CRF的损失函数" class="headerlink" title="1.6 CRF的损失函数"></a>1.6 CRF的损失函数</h3><p>由前面可知，CRF层的损失函数为:</p><p>$$ Loss Function = \frac{P_{RealPath}}{P_1 + P_2 + … + P_N} $$</p><p>我们对其对数化，即：</p><p>$$ LogLossFunction = \log \frac{P_{RealPath}}{P_1 + P_2 + … + P_N} $$</p><p>一般在模型训练过程中，我们希望损失函数最小化，因此，在损失函数添加一个负号，即:</p><p>$Log Loss Function$<br>$= - \log \frac{P_{RealPath}}{P_1 + P_2 + … + P_N}$<br>$= - \log \frac{e^{S_{RealPath}}}{e^{S_1} + e^{S_2} + … + e^{S_N}}$<br>$= - (\log(e^{S_{RealPath}}) - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$<br>$= - (S_{RealPath} - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$<br>$= - ( \sum_{i=1}^{N} x_{iy_i} + \sum_{i=1}^{N-1} t_{y_iy_{i+1}} - \log(e^{S_1} + e^{S_2} + … + e^{S_N}))$</p><p>因此，对于总得分，我们需要一个高效的方法计算:</p><p>$$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$$</p><h4 id="1-6-1-emission-Score和transition-Score"><a href="#1-6-1-emission-Score和transition-Score" class="headerlink" title="1.6.1 emission Score和transition Score"></a>1.6.1 emission Score和transition Score</h4><p>为了简化公式，我们假设句子的长度为3，即:</p><p>$x = (w_0,w_1,w_2)$</p><p>假设数据集中只有两个标签，即：</p><p>$LabelSet = (l_1,l_2)$</p><p>则emission Score矩阵可从BiLSTM层的输出获得，即：</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$w_0$</td><td>$x_{01}$</td><td>$x_{02}$</td></tr><tr><td>$w_1$</td><td>$x_{11}$</td><td>$x_{12}$</td></tr><tr><td>$w_2$</td><td>$x_{21}$</td><td>$x_{22}$</td></tr></tbody></table><p>其中$x_{ij}$为单元$w_i$被标记为$l_j$的得分。</p><p>而且，我们可以从CRF层中得到transition Score矩阵，即:</p><table><thead><tr><th></th><th>$l_1$</th><th>$l_2$</th></tr></thead><tbody><tr><td>$l_1$</td><td>$t_{11}$</td><td>$t_{12}$</td></tr><tr><td>$l_2$</td><td>$t_{21}$</td><td>$t_{22}$</td></tr></tbody></table><p>其中$t_{ij}$为标签$i$到标签$j$的得分。</p><h4 id="1-6-2-公式推导"><a href="#1-6-2-公式推导" class="headerlink" title="1.6.2 公式推导"></a>1.6.2 公式推导</h4><p>记住我们的目标是计算: $\log(e^{S_1} + e^{S_2} + … + e^{S_N})$</p><p>很显然，我们可以使用动态规划思想进行计算（如果你不了解动态规划，没关系，本文将一步一步地进行解释，当然还是建议你学习下动态规划算法）。简而言之，首先，我们计算$w_0$的所有可能序列的总得分。接着，我们使用上一步的总得分计算$w_0 \rightarrow w_1$的总得分。最后，我们同样使用上一步的总得分计算$w_0 \rightarrow w_1 \rightarrow w_2$的总得分。最后的总得分就是我们想要的总得分。</p><p>很明显，我们每一次计算都需要利用到上一步计算得到的结果，因此，接下来，你将看到两个变量:</p><ul><li>obs: 定义当前单元的信息</li><li>previous: 存储上一步计算的最后结果</li></ul><hr><p><strong>备注</strong>：以下内容如果看不懂的话，结合上面的emission Score矩阵和transition Score矩阵一起看就明白了</p><p>首先，我们计算$w_0$:</p><p>$obs = [x_{01},x_{02}]$<br>$previous = None$</p><p>如果我们的句子只有一个词$w_0$，那么存储上一步结果的$previous$为$None$，另外，对于$w_0$而言，$obs = [x_{01},x_{02}]$，其中$x_{01}$和$x_{02}$分别为emission Score（ＢiLSTM层的输出）。</p><p>因此，$w_0$的所有可能标签序列总得分为:</p><p>$TotalScore(w_0)=\log (e^{x_{01}} + e^{x_{02}})$</p><hr><p>接着，我们计算$w_0 \rightarrow w_1$:</p><p>$obs = [x_{11},x_{12}]$<br>$previous = [x_{01},x_{02}]$</p><p>为了计算方便，我们将$previous$转变为:</p><p>$$ previous =<br>\begin{pmatrix}<br>x_{01} &amp; x_{01} \<br>x_{02} &amp; x_{02}<br>\end{pmatrix}<br>$$<br>同样，将$obs$转变为:</p><p>$$ obs =<br>\begin{pmatrix}<br>x_{11} &amp; x_{12} \<br>x_{11} &amp; x_{12}<br>\end{pmatrix}<br>$$<br><strong>备注</strong>：通过矩阵方式计算更高效</p><p>接着，我们将$previous,abs$和transition Score进行相加,即:<br>$$<br>scores =<br>\begin{pmatrix}<br>x_{01}&amp;x_{01}\<br>x_{02}&amp;x_{02}<br>\end{pmatrix}<br>+<br>\begin{pmatrix}<br>x_{11}&amp;x_{12}\<br>x_{11}&amp;x_{12}<br>\end{pmatrix}<br>+<br>\begin{pmatrix}<br>t_{11}&amp;t_{12}\<br>t_{21}&amp;t_{22}<br>\end{pmatrix}<br>$$</p><p>接着，可得到:</p><p>$$<br>scores =<br>\begin{pmatrix}<br>x_{01}+x_{11}+t_{11}&amp;x_{01}+x_{12}+t_{12}\<br>x_{02}+x_{11}+t_{21}&amp;x_{02}+x_{12}+t_{22}<br>\end{pmatrix}<br>$$<br>从而我们可得到当前的$previous$为:</p><p>$$previous=[\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}), \log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})]$$</p><p>实际上，第二步已经算完了，可能还有人还无法理解如何得到$w_0$到$w_1$的所有可能序列组合（$label_1 \rightarrow label_1, label_1 \rightarrow label_2 , label_2 \rightarrow label_1, label_2 \rightarrow label_2$）的总得分，其实你主要按照以下计算方式即可;</p><p>$TotalScore(w_0 → w_1)$</p><p>$=\log (e^{previous[0]} + e^{previous[1]})$</p><p>$=\log (e^{\log(e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})}+<br>e^{\log(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})}<br>)$</p><p>$=\log(e^{x_{01}+x_{11}+t_{11}}+e^{x_{02}+x_{11}+t_{21}}+e^{x_{01}+x_{12}+t_{12}}+e^{x_{02}+x_{12}+t_{22}})$</p><p>很明显，与$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$很相似。</p><p>在上述公式中，我们可以看到:</p><ul><li>$S_1 = x_{01}+x_{11}+t_{11}$ ($label_1$ → $label_1$)<ul><li>$S_2 = x_{02}+x_{11}+t_{21}$ ($label_2$ → $label_1$)</li><li>$S_3 = x_{01}+x_{12}+t_{12}$ ($label_1$ → $label_2$)</li><li>$S_4 = x_{02}+x_{12}+t_{22}$ ($label_2$ → $label_2$)</li></ul></li></ul><hr><p>接着我们计算$w_0$ → $w_1$ → $w_2$:</p><p>如果你理解了上一步的计算过程的话，其实这一步的计算与上一步类似。即：</p><p>$obs = [x_{21}, x_{22}]$</p><p>$previous=[\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}), \log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})]$</p><p>类似于第二步，我们将$previous$转化为:</p><p>$$<br>previous =<br>\begin{pmatrix}<br>\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})&amp;\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})\<br>\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})&amp;\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})<br>\end{pmatrix}<br>$$</p><p>同样，将$obs$转化为:</p><p>$$<br>obs =<br>\begin{pmatrix}<br>x_{21}&amp;x_{22}\<br>x_{21}&amp;x_{22}<br>\end{pmatrix}<br>$$</p><p>将$previous，obs$和transition Score进行相加，即:</p><p>$scores =$<br>$\begin{pmatrix}<br>\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})&amp;\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})\<br>\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})&amp;\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})<br>\end{pmatrix}$<br>$+$<br>$\begin{pmatrix}<br>x_{21}&amp;x_{22}\<br>x_{21}&amp;x_{22}<br>\end{pmatrix}$<br>$+$<br>$\begin{pmatrix}<br>t_{11}&amp;t_{12}\<br>t_{21}&amp;t_{22}<br>\end{pmatrix}$</p><p>更新$previous$为:</p><p>$previous = [\log(<br>e^{\log (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}}) + x_{22} + t_{12}}<br>+<br>e^{\log (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}}) + x_{22} + t_{22}})]$<br>$=\log( (e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{22} + t_{12}} + (e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{22} + t_{22}})]$</p><p>当计算到最后一步时，我们使用新的$previous$计算总得分:</p><p>$TotalScore(w_0 → w_1 → w_2)$</p><p>$=\log (e^{previous[0]} + e^{previous[1]})$</p><p>$=\log (e^{\log(<br>(e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{21} + t_{11}}<br>+<br>(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{21} + t_{21}}<br>)}$</p><p>$+e^{\log(<br>(e^{x_{01}+x_{11}+t_{11}} + e^{x_{02}+x_{11}+t_{21}})e^{x_{22} + t_{12}}<br>+<br>(e^{x_{01}+x_{12}+t_{12}} + e^{x_{02}+x_{12}+t_{22}})e^{x_{22} + t_{22}})}<br>)$</p><p>$=\log (e^{x_{01}+x_{11}+t_{11}+x_{21}+t_{11}}+e^{x_{02}+x_{11}+t_{21}+x_{21}+t_{11}}$<br>$+e^{x_{01}+x_{12}+t_{12}+x_{21}+t_{21}}+e^{x_{02}+x_{12}+t_{22}+x_{21}+t_{21}}$<br>$+e^{x_{01}+x_{11}+t_{11}+x_{22}+t_{12}}+e^{x_{02}+x_{11}+t_{21}+x_{22}+t_{12}}$<br>$+e^{x_{01}+x_{12}+t_{12}+x_{22}+t_{22}}+e^{x_{02}+x_{12}+t_{22}+x_{22}+t_{22}})$</p><hr><p>到这里，我们就完成 了$\log(e^{S_1} + e^{S_2} + … + e^{S_N})$的计算过程。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360.<br><a href="https://arxiv.org/abs/1603.01360" target="_blank" rel="noopener">https://arxiv.org/abs/1603.01360</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一节中，我们知道CRF层可以从训练数据集中自动学习到一些约束规则来保证预测标签的合法性。这些约束包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”&lt;/li&gt;
&lt;li&gt;标签“B-label1I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-PersonI-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列.&lt;/li&gt;
&lt;li&gt;标签序列“O I-label”是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说，有效的标签序列应该是“O B-label”。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这一节，我们将解释为什么CRF层会自动学习到这些约束规则。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>BiLSTM模型中CRF层的运行原理(一)</title>
    <link href="http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--1/"/>
    <id>http://state-of-art.top/2018/12/31/CRF-Layer-on-the-Top-of-BiLSTM+--1/</id>
    <published>2018-12-31T15:20:08.000Z</published>
    <updated>2018-12-31T07:37:47.606Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要内容如下:</p><ul><li>介绍: 在命名实体识别任务中，BiLSTM模型中CRF层的通用思想</li><li>实例: 通过实例来一步步展示CRF的工作原理</li><li>实现: CRF层的一步步实现过程</li></ul><p><strong>备注</strong>: 需要有的基础知识：你只需要知道什么是命名实体识别，如果你不懂神经网络，条件随机场（CRF）或者其它相关知识，不必担心，本文将向你展示CRF层是如何工作的。本文将尽可能的讲的通俗易懂。</p><a id="more"></a><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>基于神经网络的方法，在命名实体识别任务中非常流行和普遍。在文献[1]中，作者提出了BiLSTM-CRF模型用于实体识别任务中，在模型中用到了字嵌入和词嵌入。本文将向你展示CRF层是如何工作的。</p><p>如果你不知道BiLSTM和CRF是什么，你只需要记住他们分别是命名实体识别模型中的两个层。</p><h3 id="1-1开始之前"><a href="#1-1开始之前" class="headerlink" title="1.1开始之前"></a>1.1开始之前</h3><p>我们假设我们的数据集中有两类实体——<code>人名</code>和<code>地名</code>，与之相对应在我们的训练数据集中，有五类标签：</p><ul><li>B-Person</li><li>I-Person</li><li>B-Organization</li><li>I-Organization</li><li>O</li></ul><p>假设句子$x$由5个字符组成，即$x = (w_0,w_1,w_2,w_3,w_4)$，其中$[w_0,w_1]$为<code>人名</code>实体，$[w_3]$为<code>组织</code>实体，其他字符的标签为”O”。</p><h3 id="1-2-BiLSTM-CRF-模型"><a href="#1-2-BiLSTM-CRF-模型" class="headerlink" title="1.2 BiLSTM-CRF 模型"></a>1.2 BiLSTM-CRF 模型</h3><p>首先，对该模型进行简单的介绍，具体的模型结构，如下图所示：<br>​    </p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235235.png"></div><p>从上图中，总的来说可以总结为以下两点：</p><ul><li><p>句子$x$中的每一个单元都代表着由字嵌入或词嵌入构成的向量。其中，字嵌入一般是随机初始化的，而词嵌入一般是通过一个预训练好的词向量模型得到的。所有的嵌入在训练过程中都会调整到最优。        </p></li><li><p>这些字或词嵌入作为BiLSTM-CRF模型的输入，输出的是句子$x$中每个单元的标签。</p></li></ul><p>为了更容易了解CRF层的运行原理，我们需要知道BiLSTM的输出层。</p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235328.png"></div><p>如上图所示，BiLSTM层的输出为句子$x$中的每一个单元每一个标签的预测分值，例如，对于单元$w_0$，BiLSTM层输出的是1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) and 0.05 (O). 这些分值将作为CRF层的输入。</p><h3 id="1-3-如果没有CRF层会怎样"><a href="#1-3-如果没有CRF层会怎样" class="headerlink" title="1.3 如果没有CRF层会怎样"></a>1.3 如果没有CRF层会怎样</h3><p>你也许已经发现了，即使没有CRF层，我们也可以训练一个BiLSTM命名实体识别模型，如图下图所示：</p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235450.jpg"></div><p>由于BiLSTM的输出为每个单元的每一个标签预测分值，我们可以挑选分值最高的一个作为该单元的标签。例如，对于单元$w_0$,“B-Person”标签分值最高——1.5，因此我们可以将“B-Person”作为$w_0$的预测标签。同理，我们可以得到$w_1$—“I-Person”，$w_2$— “O” ，$w_3$—“B-Organization”，$w_4$—“O”。</p><p>虽然我们可以得到句子$x$中每个单元的正确标签，但是我们不能保证标签每次都是预测正确的。例如，如下图所示:</p><div align="center"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20181227235531.png"></div><p>很显然，标签序列“I-Organization I-Person” and “B-Organization I-Person”是错误的。</p><h3 id="1-4-CRF层能从训练数据中学到约束规则"><a href="#1-4-CRF层能从训练数据中学到约束规则" class="headerlink" title="1.4 CRF层能从训练数据中学到约束规则"></a>1.4 CRF层能从训练数据中学到约束规则</h3><p>CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合理的。在训练过程中，这些约束可以通过CRF层自动学习到。</p><p>这些约束可以是：</p><ul><li>句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”</li><li>标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合理的序列, 但是“B-Person I-Organization” 是不合理标签序列.</li><li>标签序列“O I-label” 是不合理的，实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。有了这些约束，标签序列预测中不合理序列出现的概率将会大大降低。</li></ul><p>下一节，将通过CRF层的损失函数，解释CRF层如何从训练数据集中学习到这些约束。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C., 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360.<br><a href="https://arxiv.org/abs/1603.01360" target="_blank" rel="noopener">https://arxiv.org/abs/1603.01360</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要内容如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;介绍: 在命名实体识别任务中，BiLSTM模型中CRF层的通用思想&lt;/li&gt;
&lt;li&gt;实例: 通过实例来一步步展示CRF的工作原理&lt;/li&gt;
&lt;li&gt;实现: CRF层的一步步实现过程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;备注&lt;/strong&gt;: 需要有的基础知识：你只需要知道什么是命名实体识别，如果你不懂神经网络，条件随机场（CRF）或者其它相关知识，不必担心，本文将向你展示CRF层是如何工作的。本文将尽可能的讲的通俗易懂。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NER" scheme="http://state-of-art.top/tags/NER/"/>
    
      <category term="深度学习" scheme="http://state-of-art.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CRF" scheme="http://state-of-art.top/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>浅析pytorch中的LSTM(GRU)</title>
    <link href="http://state-of-art.top/2018/11/30/%E6%B5%85%E6%9E%90pytorch%E4%B8%AD%E7%9A%84LSTM(GRU)%20/"/>
    <id>http://state-of-art.top/2018/11/30/浅析pytorch中的LSTM(GRU) /</id>
    <published>2018-11-30T15:21:08.000Z</published>
    <updated>2018-11-30T13:59:48.467Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文介绍pytorch RNN网络搭建，主要包括LSTM和GRU的使用。</p><p>最近从tensorflow入坑pytorch，发现两者的RNN模块前向传播有些不同。特别是RNN中的pack_padded_sequence和pad_packed_sequence。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="tensorflow的BILSTM"><a href="#tensorflow的BILSTM" class="headerlink" title="tensorflow的BILSTM"></a>tensorflow的BILSTM</h3><p>以BILSTM为例，tensorflow中我们一般这样操作：</p><ul><li><p>先定义个前向层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size)</span><br></pre></td></tr></table></figure></li><li><p>在定义一个后向层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=cell_size)</span><br></pre></td></tr></table></figure></li></ul><p>然后前向传播是这个样子的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, seq_length, training)</span>:</span></span><br><span class="line">       <span class="comment"># 嵌入层</span></span><br><span class="line">       embedded_words = self.embeddings(inputs)</span><br><span class="line">       <span class="comment"># RNN层</span></span><br><span class="line">       outputs, final_state = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">           self.fw_cell,</span><br><span class="line">           self.bw_cell,</span><br><span class="line">           inputs=embedded_words, </span><br><span class="line">           <span class="comment"># 句子原始长度（batch_size，1）</span></span><br><span class="line">           sequence_length=seq_length, </span><br><span class="line">           dtype=tf.float32,</span><br><span class="line">           time_major=<span class="keyword">False</span>)</span><br><span class="line">       <span class="comment"># 合并前后向的结果</span></span><br><span class="line">       <span class="comment"># outputs是一个列表:[（batch_size, cell_size）,(...)]</span></span><br><span class="line">       <span class="comment"># len(output) = time_steps</span></span><br><span class="line">       outputs = tf.concat(outputs, axis=<span class="number">2</span>)</span><br><span class="line">       <span class="comment"># 由于采用的是dynamic_rnn，padding部分自动被截断了</span></span><br><span class="line">       <span class="comment"># 这里取最后一维就好了</span></span><br><span class="line">       final_output = outputs[<span class="number">-1</span>]</span><br><span class="line">       logits = self.Dense(final_output)</span><br><span class="line">       <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>可以看到，tensorflow的<em>dynamic</em> LSTM有两个特点：</p><ul><li>前向传播时，不看词向量维度的话，输入是个二维（batch_size, time_steps)</li><li>padding部分不参与计算，会被自动截断</li></ul><p><em>with this in our mind</em>, <em>let‘s see what’s the bilstm in pytorch。</em></p><h2 id="Pytorch-BILSTM"><a href="#Pytorch-BILSTM" class="headerlink" title="Pytorch BILSTM"></a>Pytorch BILSTM</h2><h3 id="LSTM定义"><a href="#LSTM定义" class="headerlink" title="LSTM定义"></a>LSTM定义</h3><p>一般这样操作：</p><ul><li><p>先定义个LSTM</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension,</span><br><span class="line">                                    hidden_size=hidden_size,</span><br><span class="line">                                    num_layers=num_layer,</span><br><span class="line">                                    batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                    bidirectional=bi_flag)</span><br></pre></td></tr></table></figure><p>双向的怎么办呢, 改个参数就行了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bidirectional=<span class="keyword">True</span></span><br></pre></td></tr></table></figure></li><li><p>多层呢</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_layers=<span class="keyword">True</span></span><br></pre></td></tr></table></figure></li></ul><p>可以看到，pytorch的LSTM把所有的功能整合到一起了，不需要定义前向后向（当然我们也可以搞两个单向的）。此外，我们除了要传递隐藏层的个数cell_size, 还需要将词向量的维度传递进来，这是为什么呢？</p><p>这还不是关键。关键在于pytorch的动态padding机制。因为我们一般是将输入作为batch传进来的，对于变长的文本来说，padding是不可避免的。但是在我们使用LSTM进行计算时，是不希望padding部分参与计算的（他们不是真实的文本，假如纳入计算，会引入不必要的噪声和不必要的计算量）。tensorflow采用dynamic LSTM很好地解决了这一问题。pytorch当然也有他自己的一套，下面来看看。</p><h3 id="pack-padded-sequence"><a href="#pack-padded-sequence" class="headerlink" title="pack_padded_sequence"></a>pack_padded_sequence</h3><p>我们从embedding层拿到的输入维度为（batch_size, time_steps, word_embedding_dimension）, 并不将他直接喂给LSTM，而是要预加工一下，这时候第一个重要的函数pack_padded_sequence就登场了，可以将其看成一个截断函数，作用就是将padding部分截断。不仅如此，阶段后会将输入拉平，变成（batch_size*time_steps, word_embedding_dimension）, 仔细看一下，降了一个维度。来看实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence </span><br><span class="line"><span class="comment"># 手动造个张量, 包含3个序列，长度分别为10,5,3，使用0进行padding</span></span><br><span class="line">x = torch.FloatTensor([[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>],[<span class="number">7</span>],[<span class="number">8</span>],[<span class="number">8</span>],[<span class="number">9</span>]],[[<span class="number">1</span>], [<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]],[[<span class="number">5</span>],[<span class="number">4</span>],[<span class="number">6</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]]])</span><br><span class="line">print(<span class="string">"x shape is："</span>, x.shape)</span><br><span class="line"><span class="comment"># x 的真实长度</span></span><br><span class="line">length = torch.LongTensor([<span class="number">10</span>, <span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">x_packed = pack_padded_sequence(x, length,batch_first=<span class="keyword">True</span>)</span><br><span class="line">print(x_packed)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x shape <span class="keyword">is</span>：（<span class="number">3</span>，<span class="number">10</span>，<span class="number">1</span>）</span><br><span class="line">PackedSequence(data=tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>],</span><br><span class="line">        [<span class="number">8.</span>],</span><br><span class="line">        [<span class="number">8.</span>],</span><br><span class="line">        [<span class="number">9.</span>]]), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这里有几点要说一下：</p><ul><li>输入的张量是已经按照长度排序的！这是必须的，否则pack_padded_sequence函数会报错！一般而言排序有两种做法，一是使用torch.sort函数，第二是在构造batch时使用torchtext进行batch内排序，见我上篇博文</li><li>经过pack，输入发生了三个变化。一是由三维变成了2维(18,1)，二是padding部分的0没有了，三是对序列进行了拼接，这其实是维度降低的结果</li><li>pack完之后的结果是个tuple，tuple[0]是数据，是按列进行拼接的，tuple[1]是batch_size, 跟我们之前的batch_size是不一样的，这是为了我们后面可以还原</li></ul><p>好了，现在pddding问题解决了，我们将它输入进LSTM，前向传播是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">    <span class="string">"""前向传播"""</span></span><br><span class="line">    embeddings = self.embedding(inputs, length)  <span class="comment"># (batch_size, time_steps, embedding_dim)</span></span><br><span class="line">    <span class="comment"># 去除padding元素</span></span><br><span class="line">    <span class="comment"># embeddings_packed: (batch_size*time_steps, embedding_dim)</span></span><br><span class="line">    embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">    output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0))</span><br><span class="line">    <span class="comment"># padded_output: (batch_size, time_steps, hidden_size * bi_num)</span></span><br><span class="line">    <span class="comment"># h_n|c_n: (num_layer*bi_num, batch_size, hidden_size)</span></span><br><span class="line">    padded_output, length = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># 取最后一个有效输出作为最终输出（0为无效输出）</span></span><br><span class="line">    last_output = padded_output[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>这里做下参数说明：</p><ul><li>batch_first    类型：bool， True，输入的维度为（batch_size，time_steps, word_embedding_dimension）；False，输入维度为（time_steps, batch_size, word_embedding_dimension）</li><li>h_0: 初始化隐藏态</li><li>c_0: 初始化细胞态</li><li>output：输出，为一个tuple，后面会用例子说明</li><li>h_n: 最后个隐藏态</li><li>c_n: 最后个细胞态</li></ul><p>还是以上面我们造的那个张量为例，看下输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">birnn = nn.LSTM(input_size=<span class="number">1</span>, hidden_size=<span class="number">8</span>, bidirectional=<span class="keyword">True</span>)</span><br><span class="line">output, (h_n, c_n) = birnn(x_packed)</span><br><span class="line">print(<span class="string">'output:\n'</span>, output)</span><br><span class="line">print(<span class="string">'h_n shape:\n'</span>,h_n.shape)</span><br><span class="line">print(<span class="string">'c_n shape:\n'</span>,c_n.shape)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line"> PackedSequence(data=tensor([[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,  <span class="number">0.0291</span>,</span><br><span class="line">          <span class="number">0.0619</span>,  <span class="number">0.0213</span>,  <span class="number">0.1517</span>,  <span class="number">0.0241</span>,  <span class="number">0.2986</span>, <span class="number">-0.2594</span>, <span class="number">-0.1432</span>, <span class="number">-0.1742</span>],</span><br><span class="line">        [<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,  <span class="number">0.0291</span>,</span><br><span class="line">          <span class="number">0.0486</span>,  <span class="number">0.0195</span>,  <span class="number">0.1454</span>,  <span class="number">0.0138</span>,  <span class="number">0.2596</span>, <span class="number">-0.2467</span>, <span class="number">-0.1491</span>, <span class="number">-0.1596</span>],</span><br><span class="line">        [<span class="number">-0.1019</span>,  <span class="number">0.1606</span>, <span class="number">-0.0482</span>, <span class="number">-0.0519</span>, <span class="number">-0.1932</span>,  <span class="number">0.2632</span>, <span class="number">-0.0423</span>,  <span class="number">0.0774</span>,</span><br><span class="line">          <span class="number">0.0434</span>,  <span class="number">0.0142</span>,  <span class="number">0.0915</span>, <span class="number">-0.0455</span>,  <span class="number">0.2970</span>, <span class="number">-0.6665</span>, <span class="number">-0.0578</span>, <span class="number">-0.0521</span>],</span><br><span class="line">        [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,  <span class="number">0.0918</span>,</span><br><span class="line">          <span class="number">0.0730</span>,  <span class="number">0.0247</span>,  <span class="number">0.1526</span>,  <span class="number">0.0160</span>,  <span class="number">0.3559</span>, <span class="number">-0.4301</span>, <span class="number">-0.1375</span>, <span class="number">-0.1429</span>],</span><br><span class="line">        [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,  <span class="number">0.0918</span>,</span><br><span class="line">          <span class="number">0.0517</span>,  <span class="number">0.0197</span>,  <span class="number">0.1431</span>, <span class="number">-0.0005</span>,  <span class="number">0.2929</span>, <span class="number">-0.4016</span>, <span class="number">-0.1401</span>, <span class="number">-0.1218</span>],</span><br><span class="line">        [<span class="number">-0.1855</span>,  <span class="number">0.2300</span>, <span class="number">-0.0625</span>, <span class="number">-0.0508</span>, <span class="number">-0.2488</span>,  <span class="number">0.2741</span>, <span class="number">-0.0638</span>,  <span class="number">0.1441</span>,</span><br><span class="line">          <span class="number">0.0239</span>,  <span class="number">0.0137</span>,  <span class="number">0.0786</span>, <span class="number">-0.0524</span>,  <span class="number">0.2311</span>, <span class="number">-0.5685</span>, <span class="number">-0.0673</span>, <span class="number">-0.0485</span>],</span><br><span class="line">        [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,  <span class="number">0.1487</span>,</span><br><span class="line">          <span class="number">0.0789</span>,  <span class="number">0.0259</span>,  <span class="number">0.1354</span>,  <span class="number">0.0050</span>,  <span class="number">0.3851</span>, <span class="number">-0.5676</span>, <span class="number">-0.1110</span>, <span class="number">-0.1117</span>],</span><br><span class="line">        [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,  <span class="number">0.1487</span>,</span><br><span class="line">          <span class="number">0.0439</span>,  <span class="number">0.0172</span>,  <span class="number">0.1198</span>, <span class="number">-0.0221</span>,  <span class="number">0.2856</span>, <span class="number">-0.5105</span>, <span class="number">-0.1089</span>, <span class="number">-0.0852</span>],</span><br><span class="line">        [<span class="number">-0.1346</span>,  <span class="number">0.3093</span>, <span class="number">-0.0775</span>, <span class="number">-0.0678</span>, <span class="number">-0.2616</span>,  <span class="number">0.2946</span>, <span class="number">-0.1167</span>,  <span class="number">0.1699</span>,</span><br><span class="line">         <span class="number">-0.0280</span>,  <span class="number">0.0067</span>,  <span class="number">0.0333</span>, <span class="number">-0.0924</span>,  <span class="number">0.1308</span>, <span class="number">-0.5332</span>, <span class="number">-0.0237</span>, <span class="number">-0.0201</span>],</span><br><span class="line">        [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,  <span class="number">0.1887</span>,</span><br><span class="line">          <span class="number">0.0803</span>,  <span class="number">0.0254</span>,  <span class="number">0.1112</span>, <span class="number">-0.0077</span>,  <span class="number">0.3929</span>, <span class="number">-0.6681</span>, <span class="number">-0.0804</span>, <span class="number">-0.0828</span>],</span><br><span class="line">        [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,  <span class="number">0.1887</span>,</span><br><span class="line">          <span class="number">0.0194</span>,  <span class="number">0.0129</span>,  <span class="number">0.0852</span>, <span class="number">-0.0521</span>,  <span class="number">0.2364</span>, <span class="number">-0.5524</span>, <span class="number">-0.0720</span>, <span class="number">-0.0538</span>],</span><br><span class="line">        [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,  <span class="number">0.2141</span>,</span><br><span class="line">          <span class="number">0.0779</span>,  <span class="number">0.0234</span>,  <span class="number">0.0866</span>, <span class="number">-0.0215</span>,  <span class="number">0.3831</span>, <span class="number">-0.7393</span>, <span class="number">-0.0539</span>, <span class="number">-0.0580</span>],</span><br><span class="line">        [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,  <span class="number">0.2141</span>,</span><br><span class="line">         <span class="number">-0.0274</span>,  <span class="number">0.0076</span>,  <span class="number">0.0448</span>, <span class="number">-0.0817</span>,  <span class="number">0.1408</span>, <span class="number">-0.4683</span>, <span class="number">-0.0382</span>, <span class="number">-0.0285</span>],</span><br><span class="line">        [<span class="number">-0.1470</span>,  <span class="number">0.3710</span>, <span class="number">-0.0771</span>, <span class="number">-0.0605</span>, <span class="number">-0.2693</span>,  <span class="number">0.2962</span>, <span class="number">-0.0684</span>,  <span class="number">0.2291</span>,</span><br><span class="line">          <span class="number">0.0724</span>,  <span class="number">0.0204</span>,  <span class="number">0.0651</span>, <span class="number">-0.0356</span>,  <span class="number">0.3568</span>, <span class="number">-0.7885</span>, <span class="number">-0.0341</span>, <span class="number">-0.0385</span>],</span><br><span class="line">        [<span class="number">-0.1142</span>,  <span class="number">0.3956</span>, <span class="number">-0.0744</span>, <span class="number">-0.0727</span>, <span class="number">-0.2434</span>,  <span class="number">0.2935</span>, <span class="number">-0.1226</span>,  <span class="number">0.2366</span>,</span><br><span class="line">          <span class="number">0.0636</span>,  <span class="number">0.0164</span>,  <span class="number">0.0477</span>, <span class="number">-0.0497</span>,  <span class="number">0.3136</span>, <span class="number">-0.8199</span>, <span class="number">-0.0208</span>, <span class="number">-0.0245</span>],</span><br><span class="line">        [<span class="number">-0.0854</span>,  <span class="number">0.4126</span>, <span class="number">-0.0669</span>, <span class="number">-0.0829</span>, <span class="number">-0.2140</span>,  <span class="number">0.2871</span>, <span class="number">-0.1699</span>,  <span class="number">0.2389</span>,</span><br><span class="line">          <span class="number">0.0496</span>,  <span class="number">0.0118</span>,  <span class="number">0.0343</span>, <span class="number">-0.0641</span>,  <span class="number">0.2542</span>, <span class="number">-0.8307</span>, <span class="number">-0.0124</span>, <span class="number">-0.0151</span>],</span><br><span class="line">        [<span class="number">-0.0847</span>,  <span class="number">0.4176</span>, <span class="number">-0.0620</span>, <span class="number">-0.0877</span>, <span class="number">-0.2053</span>,  <span class="number">0.2863</span>, <span class="number">-0.2063</span>,  <span class="number">0.2468</span>,</span><br><span class="line">          <span class="number">0.0281</span>,  <span class="number">0.0090</span>,  <span class="number">0.0254</span>, <span class="number">-0.0784</span>,  <span class="number">0.1827</span>, <span class="number">-0.7871</span>, <span class="number">-0.0100</span>, <span class="number">-0.0116</span>],</span><br><span class="line">        [<span class="number">-0.0620</span>,  <span class="number">0.4284</span>, <span class="number">-0.0550</span>, <span class="number">-0.0931</span>, <span class="number">-0.1827</span>,  <span class="number">0.2785</span>, <span class="number">-0.2409</span>,  <span class="number">0.2438</span>,</span><br><span class="line">         <span class="number">-0.0278</span>,  <span class="number">0.0044</span>,  <span class="number">0.0120</span>, <span class="number">-0.1073</span>,  <span class="number">0.0924</span>, <span class="number">-0.6543</span>, <span class="number">-0.0047</span>, <span class="number">-0.0063</span>]],</span><br><span class="line">       grad_fn=&lt;CatBackward&gt;), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">h_n shape:</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>])</span><br><span class="line">c_n shape:</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><p>output由一个tuple组成，第一个元素就是输出，这里的维度为torch.Size([18, 16])，第二个元素和我们之前pack时的batch_size参数一样。</p><p>h_n和c_n分别为两个三维张量，维度如上，因为是双向，第一维为2.</p><p>到这里是不是就结束了呢，当然不是，pack完了之后经过LSTM输出了结果，看上去很费劲，因为序列连一起了，batch都没了。所以我们还原，这时候就用到pack_padded_sequence的好基友pad_packed_sequence了。</p><h3 id="pad-packed-sequence"><a href="#pad-packed-sequence" class="headerlink" title="pad_packed_sequence"></a>pad_packed_sequence</h3><p>pad_packed_sequence可以看成是解压缩操作。从上面可以看到，h_n, c_n已经是正常维度的张量了，没有pack，当然也用不着pad。我们只需对output做pad。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">padded_output, length = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'padded_output\n'</span>, padded_output)</span><br><span class="line">print(<span class="string">'padded_output shape\n'</span>, padded_output.shape)</span><br><span class="line">print(<span class="string">'length\n'</span>,length)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">padded_output</span><br><span class="line"> tensor([[[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,</span><br><span class="line">           <span class="number">0.0291</span>,  <span class="number">0.0619</span>,  <span class="number">0.0213</span>,  <span class="number">0.1517</span>,  <span class="number">0.0241</span>,  <span class="number">0.2986</span>, <span class="number">-0.2594</span>,</span><br><span class="line">          <span class="number">-0.1432</span>, <span class="number">-0.1742</span>],</span><br><span class="line">         [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,</span><br><span class="line">           <span class="number">0.0918</span>,  <span class="number">0.0730</span>,  <span class="number">0.0247</span>,  <span class="number">0.1526</span>,  <span class="number">0.0160</span>,  <span class="number">0.3559</span>, <span class="number">-0.4301</span>,</span><br><span class="line">          <span class="number">-0.1375</span>, <span class="number">-0.1429</span>],</span><br><span class="line">         [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,</span><br><span class="line">           <span class="number">0.1487</span>,  <span class="number">0.0789</span>,  <span class="number">0.0259</span>,  <span class="number">0.1354</span>,  <span class="number">0.0050</span>,  <span class="number">0.3851</span>, <span class="number">-0.5676</span>,</span><br><span class="line">          <span class="number">-0.1110</span>, <span class="number">-0.1117</span>],</span><br><span class="line">         [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,</span><br><span class="line">           <span class="number">0.1887</span>,  <span class="number">0.0803</span>,  <span class="number">0.0254</span>,  <span class="number">0.1112</span>, <span class="number">-0.0077</span>,  <span class="number">0.3929</span>, <span class="number">-0.6681</span>,</span><br><span class="line">          <span class="number">-0.0804</span>, <span class="number">-0.0828</span>],</span><br><span class="line">         [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,</span><br><span class="line">           <span class="number">0.2141</span>,  <span class="number">0.0779</span>,  <span class="number">0.0234</span>,  <span class="number">0.0866</span>, <span class="number">-0.0215</span>,  <span class="number">0.3831</span>, <span class="number">-0.7393</span>,</span><br><span class="line">          <span class="number">-0.0539</span>, <span class="number">-0.0580</span>],</span><br><span class="line">         [<span class="number">-0.1470</span>,  <span class="number">0.3710</span>, <span class="number">-0.0771</span>, <span class="number">-0.0605</span>, <span class="number">-0.2693</span>,  <span class="number">0.2962</span>, <span class="number">-0.0684</span>,</span><br><span class="line">           <span class="number">0.2291</span>,  <span class="number">0.0724</span>,  <span class="number">0.0204</span>,  <span class="number">0.0651</span>, <span class="number">-0.0356</span>,  <span class="number">0.3568</span>, <span class="number">-0.7885</span>,</span><br><span class="line">          <span class="number">-0.0341</span>, <span class="number">-0.0385</span>],</span><br><span class="line">         [<span class="number">-0.1142</span>,  <span class="number">0.3956</span>, <span class="number">-0.0744</span>, <span class="number">-0.0727</span>, <span class="number">-0.2434</span>,  <span class="number">0.2935</span>, <span class="number">-0.1226</span>,</span><br><span class="line">           <span class="number">0.2366</span>,  <span class="number">0.0636</span>,  <span class="number">0.0164</span>,  <span class="number">0.0477</span>, <span class="number">-0.0497</span>,  <span class="number">0.3136</span>, <span class="number">-0.8199</span>,</span><br><span class="line">          <span class="number">-0.0208</span>, <span class="number">-0.0245</span>],</span><br><span class="line">         [<span class="number">-0.0854</span>,  <span class="number">0.4126</span>, <span class="number">-0.0669</span>, <span class="number">-0.0829</span>, <span class="number">-0.2140</span>,  <span class="number">0.2871</span>, <span class="number">-0.1699</span>,</span><br><span class="line">           <span class="number">0.2389</span>,  <span class="number">0.0496</span>,  <span class="number">0.0118</span>,  <span class="number">0.0343</span>, <span class="number">-0.0641</span>,  <span class="number">0.2542</span>, <span class="number">-0.8307</span>,</span><br><span class="line">          <span class="number">-0.0124</span>, <span class="number">-0.0151</span>],</span><br><span class="line">         [<span class="number">-0.0847</span>,  <span class="number">0.4176</span>, <span class="number">-0.0620</span>, <span class="number">-0.0877</span>, <span class="number">-0.2053</span>,  <span class="number">0.2863</span>, <span class="number">-0.2063</span>,</span><br><span class="line">           <span class="number">0.2468</span>,  <span class="number">0.0281</span>,  <span class="number">0.0090</span>,  <span class="number">0.0254</span>, <span class="number">-0.0784</span>,  <span class="number">0.1827</span>, <span class="number">-0.7871</span>,</span><br><span class="line">          <span class="number">-0.0100</span>, <span class="number">-0.0116</span>],</span><br><span class="line">         [<span class="number">-0.0620</span>,  <span class="number">0.4284</span>, <span class="number">-0.0550</span>, <span class="number">-0.0931</span>, <span class="number">-0.1827</span>,  <span class="number">0.2785</span>, <span class="number">-0.2409</span>,</span><br><span class="line">           <span class="number">0.2438</span>, <span class="number">-0.0278</span>,  <span class="number">0.0044</span>,  <span class="number">0.0120</span>, <span class="number">-0.1073</span>,  <span class="number">0.0924</span>, <span class="number">-0.6543</span>,</span><br><span class="line">          <span class="number">-0.0047</span>, <span class="number">-0.0063</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.0087</span>,  <span class="number">0.0716</span>,  <span class="number">0.0069</span>, <span class="number">-0.0040</span>, <span class="number">-0.1375</span>,  <span class="number">0.0404</span>,  <span class="number">0.0757</span>,</span><br><span class="line">           <span class="number">0.0291</span>,  <span class="number">0.0486</span>,  <span class="number">0.0195</span>,  <span class="number">0.1454</span>,  <span class="number">0.0138</span>,  <span class="number">0.2596</span>, <span class="number">-0.2467</span>,</span><br><span class="line">          <span class="number">-0.1491</span>, <span class="number">-0.1596</span>],</span><br><span class="line">         [<span class="number">-0.0895</span>,  <span class="number">0.1504</span>, <span class="number">-0.0069</span>, <span class="number">-0.0137</span>, <span class="number">-0.2248</span>,  <span class="number">0.1365</span>,  <span class="number">0.1002</span>,</span><br><span class="line">           <span class="number">0.0918</span>,  <span class="number">0.0517</span>,  <span class="number">0.0197</span>,  <span class="number">0.1431</span>, <span class="number">-0.0005</span>,  <span class="number">0.2929</span>, <span class="number">-0.4016</span>,</span><br><span class="line">          <span class="number">-0.1401</span>, <span class="number">-0.1218</span>],</span><br><span class="line">         [<span class="number">-0.1621</span>,  <span class="number">0.2254</span>, <span class="number">-0.0318</span>, <span class="number">-0.0240</span>, <span class="number">-0.2693</span>,  <span class="number">0.2207</span>,  <span class="number">0.0843</span>,</span><br><span class="line">           <span class="number">0.1487</span>,  <span class="number">0.0439</span>,  <span class="number">0.0172</span>,  <span class="number">0.1198</span>, <span class="number">-0.0221</span>,  <span class="number">0.2856</span>, <span class="number">-0.5105</span>,</span><br><span class="line">          <span class="number">-0.1089</span>, <span class="number">-0.0852</span>],</span><br><span class="line">         [<span class="number">-0.1886</span>,  <span class="number">0.2883</span>, <span class="number">-0.0563</span>, <span class="number">-0.0352</span>, <span class="number">-0.2872</span>,  <span class="number">0.2700</span>,  <span class="number">0.0431</span>,</span><br><span class="line">           <span class="number">0.1887</span>,  <span class="number">0.0194</span>,  <span class="number">0.0129</span>,  <span class="number">0.0852</span>, <span class="number">-0.0521</span>,  <span class="number">0.2364</span>, <span class="number">-0.5524</span>,</span><br><span class="line">          <span class="number">-0.0720</span>, <span class="number">-0.0538</span>],</span><br><span class="line">         [<span class="number">-0.1767</span>,  <span class="number">0.3363</span>, <span class="number">-0.0719</span>, <span class="number">-0.0475</span>, <span class="number">-0.2858</span>,  <span class="number">0.2911</span>, <span class="number">-0.0109</span>,</span><br><span class="line">           <span class="number">0.2141</span>, <span class="number">-0.0274</span>,  <span class="number">0.0076</span>,  <span class="number">0.0448</span>, <span class="number">-0.0817</span>,  <span class="number">0.1408</span>, <span class="number">-0.4683</span>,</span><br><span class="line">          <span class="number">-0.0382</span>, <span class="number">-0.0285</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-0.1019</span>,  <span class="number">0.1606</span>, <span class="number">-0.0482</span>, <span class="number">-0.0519</span>, <span class="number">-0.1932</span>,  <span class="number">0.2632</span>, <span class="number">-0.0423</span>,</span><br><span class="line">           <span class="number">0.0774</span>,  <span class="number">0.0434</span>,  <span class="number">0.0142</span>,  <span class="number">0.0915</span>, <span class="number">-0.0455</span>,  <span class="number">0.2970</span>, <span class="number">-0.6665</span>,</span><br><span class="line">          <span class="number">-0.0578</span>, <span class="number">-0.0521</span>],</span><br><span class="line">         [<span class="number">-0.1855</span>,  <span class="number">0.2300</span>, <span class="number">-0.0625</span>, <span class="number">-0.0508</span>, <span class="number">-0.2488</span>,  <span class="number">0.2741</span>, <span class="number">-0.0638</span>,</span><br><span class="line">           <span class="number">0.1441</span>,  <span class="number">0.0239</span>,  <span class="number">0.0137</span>,  <span class="number">0.0786</span>, <span class="number">-0.0524</span>,  <span class="number">0.2311</span>, <span class="number">-0.5685</span>,</span><br><span class="line">          <span class="number">-0.0673</span>, <span class="number">-0.0485</span>],</span><br><span class="line">         [<span class="number">-0.1346</span>,  <span class="number">0.3093</span>, <span class="number">-0.0775</span>, <span class="number">-0.0678</span>, <span class="number">-0.2616</span>,  <span class="number">0.2946</span>, <span class="number">-0.1167</span>,</span><br><span class="line">           <span class="number">0.1699</span>, <span class="number">-0.0280</span>,  <span class="number">0.0067</span>,  <span class="number">0.0333</span>, <span class="number">-0.0924</span>,  <span class="number">0.1308</span>, <span class="number">-0.5332</span>,</span><br><span class="line">          <span class="number">-0.0237</span>, <span class="number">-0.0201</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br><span class="line">padded_output shape</span><br><span class="line"> torch.Size([<span class="number">3</span>, <span class="number">10</span>, <span class="number">16</span>])</span><br><span class="line">length</span><br><span class="line"> tensor([<span class="number">10</span>,  <span class="number">5</span>,  <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>是不是又回来了？16是cell_size*2, 因为是双向。</p><p>可以看到，没参与计算后来被补上来的部分都成为了0！</p><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line">mport os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> util.embedding_util <span class="keyword">import</span> get_embedding</span><br><span class="line"><span class="keyword">from</span> util.plot_util <span class="keyword">import</span> loss_acc_plot</span><br><span class="line"><span class="keyword">from</span> util.lr_util <span class="keyword">import</span> lr_update</span><br><span class="line"><span class="keyword">import</span> config.lstm_config <span class="keyword">as</span> config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BILSTM</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cell_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 checkpoint_dir,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                 model_type,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word2id,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedding_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 keep_prob)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line">        self.history = &#123;&#125;</span><br><span class="line">        self.keep_prob = keep_prob</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding layer</span></span><br><span class="line">        weights = get_embedding(model_type=model_type,</span><br><span class="line">                                word2id=word2id,</span><br><span class="line">                                embedding_dim=embedding_dim)</span><br><span class="line">        <span class="keyword">if</span> model_type == <span class="string">'static'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'non-static'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'rand'</span>:</span><br><span class="line">            self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=weights, trainable=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> model_type == <span class="string">'multichannel'</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'unknown model type'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># BILSTM layer</span></span><br><span class="line">        self.fw_cell = tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">            tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=<span class="number">0.7</span>)</span><br><span class="line">        self.bw_cell = tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">            tf.nn.rnn_cell.LSTMCell(num_units=cell_size), output_keep_prob=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">        self.Dense = tf.layers.Dense(units=num_classes, activation=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, seq_length, training)</span>:</span></span><br><span class="line">        embedded_words = self.embeddings(inputs)</span><br><span class="line">        outputs, final_state = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">            self.fw_cell,</span><br><span class="line">            self.bw_cell,</span><br><span class="line">            inputs=embedded_words,</span><br><span class="line">            sequence_length=seq_length,</span><br><span class="line">            dtype=tf.float32,</span><br><span class="line">            time_major=<span class="keyword">False</span>)</span><br><span class="line">        outputs = tf.concat(outputs, axis=<span class="number">2</span>)</span><br><span class="line">        final_output = outputs[<span class="number">-1</span>]</span><br><span class="line">        logits = self.Dense(final_output)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        preds = self.call(inputs, seq_length, training)</span><br><span class="line">        <span class="comment"># L2正则化</span></span><br><span class="line">        loss_L2 = tf.add_n([tf.nn.l2_loss(v)</span><br><span class="line">                            <span class="keyword">for</span> v <span class="keyword">in</span> self.trainable_variables</span><br><span class="line">                            <span class="keyword">if</span> <span class="string">'bias'</span> <span class="keyword">not</span> <span class="keyword">in</span> v.name]) * <span class="number">0.001</span></span><br><span class="line">        loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)</span><br><span class="line">        loss = loss + loss_L2</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grads_fn</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            loss = self.loss_fn(inputs, target, seq_length, training)</span><br><span class="line">        <span class="keyword">return</span> tape.gradient(loss, self.variables)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self, model)</span>:</span></span><br><span class="line">        <span class="string">""" Function to save trained model.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        checkpoint = tf.train.Checkpoint(model=model)</span><br><span class="line">        checkpoint_prefix = os.path.join(self.checkpoint_dir, <span class="string">'ckpt'</span>)</span><br><span class="line">        checkpoint.save(file_prefix=checkpoint_prefix)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Run the model once to initialize variables</span></span><br><span class="line">        dummy_input = tf.constant(tf.zeros((<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">        dummy_length = tf.constant(<span class="number">1</span>, shape=(<span class="number">1</span>,))</span><br><span class="line">        self(dummy_input, dummy_length, <span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># Restore the variables of the model</span></span><br><span class="line">        saver = tf.contrib.Saver(self.variables)</span><br><span class="line">        saver.restore(tf.train.latest_checkpoint</span><br><span class="line">                      (self.checkpoint_directory))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span><span class="params">(self, inputs, target, seq_length, training)</span>:</span></span><br><span class="line">        y = self.call(inputs, seq_length, training)</span><br><span class="line">        y_pred = tf.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        correct = tf.where(tf.equal(y_pred, target)).numpy().shape[<span class="number">0</span>]</span><br><span class="line">        total = target.numpy().shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> correct/total</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, training_data, eval_data, pbar, num_epochs=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            early_stopping_rounds=<span class="number">5</span>, verbose=<span class="number">1</span>, train_from_scratch=True)</span>:</span></span><br><span class="line">        <span class="string">"""train the model"""</span></span><br><span class="line">        <span class="keyword">if</span> train_from_scratch <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            self.restore_model()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize best loss. This variable will store the lowest loss on the</span></span><br><span class="line">        <span class="comment"># eval dataset.</span></span><br><span class="line">        best_loss = <span class="number">2018</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize classes to update the mean loss of train and eval</span></span><br><span class="line">        train_loss = []</span><br><span class="line">        eval_loss = []</span><br><span class="line">        train_accuracy = []</span><br><span class="line">        eval_accuracy = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize dictionary to store the loss history</span></span><br><span class="line">        self.history[<span class="string">'train_loss'</span>] = []</span><br><span class="line">        self.history[<span class="string">'eval_loss'</span>] = []</span><br><span class="line">        self.history[<span class="string">'train_accuracy'</span>] = []</span><br><span class="line">        self.history[<span class="string">'eval_accuracy'</span>] = []</span><br><span class="line"></span><br><span class="line">        count = early_stopping_rounds</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Begin training</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># 在每个epoch训练之初初始化optimizer，决定是否使用学习率衰减</span></span><br><span class="line">            learning_rate = lr_update(i+<span class="number">1</span>, mode=config.lr_mode)</span><br><span class="line">            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Training with gradient descent</span></span><br><span class="line">            start = time.time()</span><br><span class="line">            <span class="keyword">for</span> index, (sequence, label, seq_length) <span class="keyword">in</span> enumerate(training_data):</span><br><span class="line">                <span class="comment"># cpu需要类型转换，不然会报错：Could not find valid device</span></span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                grads = self.grads_fn(sequence, label, seq_length, training=<span class="keyword">True</span>)</span><br><span class="line">                optimizer.apply_gradients(zip(grads, self.variables))</span><br><span class="line">                pbar.show(index, use_time=time.time()-start)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the loss on the training data after one epoch</span></span><br><span class="line">            <span class="keyword">for</span> sequence, label, seq_length <span class="keyword">in</span> training_data:</span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                train_los = self.loss_fn(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                train_acc = self.get_accuracy(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                train_loss.append(train_los)</span><br><span class="line">                train_accuracy.append(train_acc)</span><br><span class="line">            self.history[<span class="string">'train_loss'</span>].append(np.mean(train_loss))</span><br><span class="line">            self.history[<span class="string">'train_accuracy'</span>].append(np.mean(train_accuracy))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the loss on the eval data after one epoch</span></span><br><span class="line">            <span class="keyword">for</span> sequence, label, seq_length <span class="keyword">in</span> eval_data:</span><br><span class="line">                sequence = tf.cast(sequence, dtype=tf.float32)</span><br><span class="line">                label = tf.cast(label, dtype=tf.int64)</span><br><span class="line">                eval_los = self.loss_fn(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                eval_acc = self.get_accuracy(sequence, label, seq_length, training=<span class="keyword">False</span>)</span><br><span class="line">                eval_loss.append(eval_los)</span><br><span class="line">                eval_accuracy.append(eval_acc)</span><br><span class="line">            self.history[<span class="string">'eval_loss'</span>].append(np.mean(eval_loss))</span><br><span class="line">            self.history[<span class="string">'eval_accuracy'</span>].append(np.mean(eval_accuracy))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print train and eval losses</span></span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span>) | ((i + <span class="number">1</span>) % verbose == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">'Epoch %d - train_loss: %4f - eval_loss: %4f - train_acc:%4f - eval_acc:%4f'</span></span><br><span class="line">                      % (i + <span class="number">1</span>,</span><br><span class="line">                         self.history[<span class="string">'train_loss'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'train_accuracy'</span>][<span class="number">-1</span>],</span><br><span class="line">                         self.history[<span class="string">'eval_accuracy'</span>][<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check for early stopping</span></span><br><span class="line">            <span class="keyword">if</span> self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>] &lt; best_loss:</span><br><span class="line">                best_loss = self.history[<span class="string">'eval_loss'</span>][<span class="number">-1</span>]</span><br><span class="line">                count = early_stopping_rounds</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 画出loss_acc曲线</span></span><br><span class="line">        loss_acc_plot(history=self.history)</span><br></pre></td></tr></table></figure><h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_packed_sequence, pack_padded_sequence</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config.config <span class="keyword">as</span> config</span><br><span class="line"><span class="keyword">from</span> util.embedding_util <span class="keyword">import</span> get_embedding</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">2018</span>)</span><br><span class="line">torch.cuda.manual_seed(<span class="number">2018</span>)</span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">2018</span>)</span><br><span class="line">np.random.seed(<span class="number">2018</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICE"</span>] = <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 word_embedding_dimension,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, bi_flag,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layer,</span></span></span><br><span class="line"><span class="function"><span class="params">                 labels,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cell_type,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout,</span></span></span><br><span class="line"><span class="function"><span class="params">                 checkpoint_dir)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.num_label = len(labels)</span><br><span class="line">        self.num_layer = num_layer</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.checkpoint_dir = checkpoint_dir</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            self.device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, word_embedding_dimension)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.embedding.parameters():</span><br><span class="line">            p.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.embedding.weight.data.copy_(torch.from_numpy(get_embedding(vocab_size, word_embedding_dimension)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cell_type == <span class="string">"LSTM"</span>:</span><br><span class="line">            self.rnn_cell = nn.LSTM(input_size=word_embedding_dimension,</span><br><span class="line">                                    hidden_size=hidden_size,</span><br><span class="line">                                    num_layers=num_layer,</span><br><span class="line">                                    batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                    dropout=dropout,</span><br><span class="line">                                    bidirectional=bi_flag)</span><br><span class="line">        <span class="keyword">elif</span> cell_type == <span class="string">"GRU"</span>:</span><br><span class="line">            self.rnn_cell = nn.GRU(input_size=word_embedding_dimension,</span><br><span class="line">                                   hidden_size=hidden_size,</span><br><span class="line">                                   num_layers=num_layer,</span><br><span class="line">                                   batch_first=<span class="keyword">True</span>,</span><br><span class="line">                                   dropout=dropout,</span><br><span class="line">                                   bidirectional=bi_flag)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"RNN: Unknown rnn cell type"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否双向</span></span><br><span class="line">        self.bi_num = <span class="number">2</span> <span class="keyword">if</span> bi_flag <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(hidden_size*self.bi_num, self.num_label)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, length)</span>:</span></span><br><span class="line">        batch_size = inputs.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 初始化态h和C,默认为zeros</span></span><br><span class="line">        h_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float()</span><br><span class="line">        c_0 = torch.zeros(self.num_layer*self.bi_num, batch_size, self.hidden_size).float()</span><br><span class="line"></span><br><span class="line">        embeddings = self.embedding(inputs, length)  <span class="comment"># (batch_size, time_steps, embedding_dim)</span></span><br><span class="line">        <span class="comment"># 去除padding元素</span></span><br><span class="line">        <span class="comment"># embeddings_packed: (batch_size*time_steps, embedding_dim)</span></span><br><span class="line">        embeddings_packed = pack_padded_sequence(embeddings, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        output, (h_n, c_n) = self.rnn_cell(embeddings_packed, (h_0, c_0))</span><br><span class="line">        <span class="comment"># padded_output: (batch_size, time_steps, hidden_size * bi_num)</span></span><br><span class="line">        <span class="comment"># h_n|c_n: (num_layer*bi_num, batch_size, hidden_size)</span></span><br><span class="line">        padded_output, _ = pad_packed_sequence(output, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 取最后一个有效输出作为最终输出（0为无效输出）</span></span><br><span class="line">        last_output = padded_output[torch.LongTensor(range(batch_size)), length]</span><br><span class="line">        last_output = F.dropout(last_output, p=self.dropout, training=self.training)</span><br><span class="line">        output = self.linear(last_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.load_state_dict(torch.load(self.checkpoint_dir))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self)</span>:</span></span><br><span class="line">        torch.save(self.state_dict(), self.checkpoint_dir)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, y_pred, y_true)</span>:</span></span><br><span class="line">        _, y_pred = torch.max(y_pred.data, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> config.use_cuda:</span><br><span class="line">            y_true = y_true.cpu().numpy()</span><br><span class="line">            y_pred = y_pred.cpu().numpy()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_true = y_true.numpy()</span><br><span class="line">            y_pred = y_pred.numpy()</span><br><span class="line">        f1 = f1_score(y_true, y_pred, labels=self.labels, average=<span class="string">"macro"</span>)</span><br><span class="line">        correct = np.sum((y_true==y_pred).astype(int))</span><br><span class="line">        acc = correct/y_pred.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> (acc, f1)</span><br></pre></td></tr></table></figure><p>各位晚安~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;本文介绍pytorch RNN网络搭建，主要包括LSTM和G
      
    
    </summary>
    
      <category term="pytorch" scheme="http://state-of-art.top/categories/pytorch/"/>
    
    
      <category term="pytorch，lstm，nlp" scheme="http://state-of-art.top/tags/pytorch%EF%BC%8Clstm%EF%BC%8Cnlp/"/>
    
  </entry>
  
  <entry>
    <title>torchtext读取文本数据集</title>
    <link href="http://state-of-art.top/2018/11/28/torchtext%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://state-of-art.top/2018/11/28/torchtext读取文本数据集/</id>
    <published>2018-11-28T15:21:08.000Z</published>
    <updated>2018-11-28T14:15:32.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文主要介绍如何使用Torchtext读取文本数据集。</p><p>Torchtext是非官方的、一种为pytorch提供文本数据处理能力的库， 类似于图像处理库Torchvision。</p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><ol><li>下载地址：<a href="https://github.com/text" target="_blank" rel="noopener">https://github.com/text</a></li><li>安装：pip install text-master.zip</li><li>测试安装是否成功： import torchtext</li></ol><h2 id="How-To-Use"><a href="#How-To-Use" class="headerlink" title="How To Use"></a>How To Use</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/20181128/1.png" alt="image text"></p><p>先上一张图。使用tortext的目的是将文本转换成Batch，方便后面训练模型时使用。过程如下:</p><ul><li>使用Field对象进行文本预处理， 生成example</li><li>使用Dataset类生成数据集dataset</li><li>使用Iterator生成迭代器</li></ul><p>从图中还可以看到，torchtext可以生成词典vocab和词向量embedding，但个人比较喜欢将这两步放在数据预处理和模型里面进行，所以这两个功能不在本文之列。</p><h3 id="常用的类"><a href="#常用的类" class="headerlink" title="常用的类"></a>常用的类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, Example, TabularDataset</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> BucketIterator</span><br></pre></td></tr></table></figure><p>Field：用来定义字段以及文本预处理方法</p><p>Example: 用来表示一个样本，通常为“数据+标签”</p><p>TabularDataset: 用来从文件中读取数据，生成Dataset， Dataset是Example实例的集合</p><p>BucketIterator：迭代器，用来生成batch， 类似的有Iterator，Buckeiterator的功能较强大点，支持排序，动态padding等</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>见我上篇博文&lt;文本预处理&gt;。使用生成的train.tsv和valid.tsv。</p><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><h4 id="创建Field对象"><a href="#创建Field对象" class="headerlink" title="创建Field对象"></a>创建Field对象</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">x_tokenize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 如果加载进来的是已经转成id的文本</span></span><br><span class="line">    <span class="comment"># 此处必须将字符串转换成整型</span></span><br><span class="line">    <span class="comment"># 否则必须将use_vocab设为True</span></span><br><span class="line">    <span class="keyword">return</span> [int(c) <span class="keyword">for</span> c <span class="keyword">in</span> x.split()]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_tokenize</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(y)</span><br><span class="line"></span><br><span class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                     use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                     fix_length=self.fix_length, </span><br><span class="line">                     eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                     include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line">LABEL = Field(sequential=<span class="keyword">False</span>, tokenize=y_tokenize, use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><ul><li>sequential     类型boolean,  作用：是否为序列，一般文本都为True，标签为False</li><li>tokenize    类型: function， 作用: 文本处理，默认为str.split(), 这里对x和y分别自定义了处理函数。</li><li>use_vocab： 类型: boolean， 作用：是否建立词典</li><li>batch_first：类型: boolean， 作用：为True则返回Batch维度为(batch_size， 文本长度), False 则相反</li><li>fix_length：类型: int, 作用：固定文本的长度，长则截断，短则padding，可认为是静态padding；为None则按每个Batch内的最大长度进行动态padding。</li><li>eos_token：类型：str, 作用: 句子结束字符</li><li>init_token：类型：str, 作用: 句子开始字符</li><li>include_lengths：类型: boolean， 作用：是否返回句子的原始长度，一般为True，方便RNN使用。</li><li>pad_token：padding的字符，默认为”<pad>“, 这里因为原始数据已经转成了int类型，所以使用0。注意这里的pad_token要和你的词典vocab里的“<pad>”的Id保持一致，否则会影响后面词向量的读取。</pad></pad></li></ul><h4 id="读取文件生成数据集"><a href="#读取文件生成数据集" class="headerlink" title="读取文件生成数据集"></a>读取文件生成数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fields = [</span><br><span class="line">    (<span class="string">"label"</span>, LABEL), (<span class="string">"text"</span>, TEXT)]</span><br><span class="line"></span><br><span class="line">train, valid = TabularDataset.splits(</span><br><span class="line">    path=config.ROOT_DIR,</span><br><span class="line">    train=self.train_path, validation=self.valid_path,</span><br><span class="line">    format=<span class="string">'tsv'</span>,</span><br><span class="line">    skip_header=<span class="keyword">False</span>,</span><br><span class="line">    fields=fields)</span><br><span class="line"><span class="keyword">return</span> train, valid</span><br></pre></td></tr></table></figure><h4 id="生成迭代器"><a href="#生成迭代器" class="headerlink" title="生成迭代器"></a>生成迭代器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_iter, val_iter = BucketIterator.splits((train, valid),</span><br><span class="line">                                             batch_sizes=(self.batch_size, self.batch_size),</span><br><span class="line">                                             device = torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">                                             sort_key=<span class="keyword">lambda</span> x: len(x.text), <span class="comment"># field sorted by len</span></span><br><span class="line">                                             sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                             repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>这里要注意的是sort_with_batch要设置为True，并指定排序的key为文本长度，方便后面pytorch RNN进行pack和pad。</p><p>我们来看下train_iter和val_iter里放了什么东西。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=<span class="number">1</span>, fix_length=<span class="keyword">None</span>)</span><br><span class="line">train, valid  = bi.create_dataset()</span><br><span class="line">train_iter, valid_iter = bi.get_iterator(train, valid)</span><br><span class="line">batch = next(iter(train_iter))</span><br><span class="line">print(train_iter)</span><br><span class="line">print(<span class="string">'batch:\n'</span>, batch)</span><br><span class="line">print(<span class="string">'batch_text:\n'</span>, batch.text)</span><br><span class="line">print(<span class="string">'batch_label:\n'</span>, batch.label)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;torchtext.data.iterator.BucketIterator object at <span class="number">0x7f04a9d845f8</span>&gt;</span><br><span class="line">batch:</span><br><span class="line">[torchtext.data.batch.Batch of size <span class="number">1</span>]</span><br><span class="line">[.label]:[torch.LongTensor of size <span class="number">1</span>]</span><br><span class="line">[.text]:(<span class="string">'[torch.LongTensor of size 1x125]'</span>, <span class="string">'[torch.LongTensor of size 1]'</span>)</span><br><span class="line">batch_text:</span><br><span class="line"> (tensor([[<span class="number">11149</span>,  <span class="number">7772</span>, <span class="number">13752</span>, <span class="number">13743</span>, <span class="number">13773</span>, <span class="number">13793</span>, <span class="number">13791</span>, <span class="number">13591</span>, <span class="number">12478</span>, <span class="number">13759</span>,</span><br><span class="line">         <span class="number">13783</span>, <span class="number">13492</span>, <span class="number">13793</span>, <span class="number">13745</span>, <span class="number">13754</span>, <span class="number">13612</span>,  <span class="number">7452</span>, <span class="number">12185</span>, <span class="number">13789</span>, <span class="number">13784</span>,</span><br><span class="line">         <span class="number">13765</span>, <span class="number">12451</span>, <span class="number">12112</span>, <span class="number">13620</span>, <span class="number">12240</span>, <span class="number">13073</span>, <span class="number">13790</span>, <span class="number">13738</span>, <span class="number">13637</span>, <span class="number">13759</span>,</span><br><span class="line">         <span class="number">13776</span>, <span class="number">13793</span>, <span class="number">13739</span>, <span class="number">13783</span>, <span class="number">13787</span>, <span class="number">13793</span>, <span class="number">12702</span>, <span class="number">13790</span>, <span class="number">13698</span>, <span class="number">13774</span>,</span><br><span class="line">         <span class="number">13792</span>, <span class="number">13768</span>, <span class="number">13715</span>, <span class="number">13641</span>, <span class="number">13761</span>, <span class="number">13713</span>, <span class="number">13682</span>, <span class="number">13712</span>, <span class="number">13786</span>, <span class="number">13749</span>,</span><br><span class="line">         <span class="number">13097</span>, <span class="number">13734</span>, <span class="number">13702</span>, <span class="number">13735</span>, <span class="number">13257</span>, <span class="number">13642</span>, <span class="number">13700</span>, <span class="number">13793</span>, <span class="number">13684</span>, <span class="number">13755</span>,</span><br><span class="line">         <span class="number">13488</span>, <span class="number">13789</span>, <span class="number">13750</span>, <span class="number">13484</span>, <span class="number">13494</span>, <span class="number">13793</span>, <span class="number">13624</span>, <span class="number">13670</span>, <span class="number">13786</span>, <span class="number">13655</span>,</span><br><span class="line">         <span class="number">13768</span>, <span class="number">13687</span>, <span class="number">13774</span>, <span class="number">13792</span>, <span class="number">13791</span>, <span class="number">13591</span>, <span class="number">13546</span>, <span class="number">13777</span>, <span class="number">13658</span>, <span class="number">13740</span>,</span><br><span class="line">         <span class="number">13577</span>, <span class="number">13790</span>, <span class="number">13684</span>, <span class="number">13755</span>, <span class="number">13793</span>, <span class="number">13572</span>, <span class="number">12891</span>, <span class="number">13793</span>, <span class="number">13368</span>, <span class="number">13713</span>,</span><br><span class="line">         <span class="number">13682</span>, <span class="number">13712</span>, <span class="number">13786</span>, <span class="number">13786</span>, <span class="number">13642</span>, <span class="number">13700</span>, <span class="number">13793</span>, <span class="number">13429</span>, <span class="number">13520</span>, <span class="number">13613</span>,</span><br><span class="line">         <span class="number">13792</span>, <span class="number">13368</span>, <span class="number">13790</span>, <span class="number">13750</span>, <span class="number">13699</span>, <span class="number">13764</span>, <span class="number">13590</span>, <span class="number">13675</span>, <span class="number">13742</span>, <span class="number">13691</span>,</span><br><span class="line">         <span class="number">13688</span>, <span class="number">13742</span>, <span class="number">13782</span>, <span class="number">13538</span>, <span class="number">13742</span>, <span class="number">13783</span>, <span class="number">13787</span>, <span class="number">13774</span>, <span class="number">13645</span>, <span class="number">13742</span>,</span><br><span class="line">         <span class="number">13791</span>, <span class="number">13740</span>, <span class="number">13744</span>, <span class="number">13750</span>, <span class="number">13792</span>]]), tensor([<span class="number">125</span>]))</span><br><span class="line">batch_label:</span><br><span class="line"> tensor([<span class="number">11</span>])</span><br></pre></td></tr></table></figure><p>可以看到batch有两个属性，分别为label和text, text是一个元组，第一个元素为文本，第二个元素为文本原始长度（这里因为我们在定义TEXT时使用了include_lengths=True，否则这里只返回文本）， label则是标签。</p><p>这里为了方便展示只使用了一个batch，返回的batch维度为（batch_size * length）, 数据格式为LongTensor。如果想看动态padding的效果，可多取几个batch，会发现他们是按照长度进行排序，并且是以0进行padding的。</p><h4 id="对Batch包装一下，方便调用"><a href="#对Batch包装一下，方便调用" class="headerlink" title="对Batch包装一下，方便调用"></a>对Batch包装一下，方便调用</h4><p>通过以上步骤，我们能够得到一个batch。但是很快就发现有个不太方便的地方。我们只能通过batch的属性，即自定义的字段名称，如text和label，来访问数据。这样的话在训练时我们只能这样操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_iter:</span><br><span class="line">        inputs = batch.text[<span class="number">0</span>]</span><br><span class="line">        label = batch.label</span><br><span class="line">        length = batch.text[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>万一这个字段改了，还要去改训练的代码，很麻烦，关键是显得很LOW，姿势不对。</p><p>怎么办呢？</p><p>我们对获得的iter进行包装一下，就可以避免这个问题了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""对batch做个包装，方便调用，可选择性使用"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                temp = [getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars]</span><br><span class="line">                label = torch.cat(temp, dim=<span class="number">1</span>).long()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'BatchWrapper: invalid label'</span>)</span><br><span class="line">            text = x[<span class="number">0</span>]</span><br><span class="line">            length = x[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> (text, label, length)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br></pre></td></tr></table></figure><p>我们这样使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br></pre></td></tr></table></figure><p>这样你就会发现batch不再有text和label属性了，而是一个三元组（text， label， length），调用时</p><p>就可以采用如下优雅一点的姿势：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> inputs, label, length <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>data_loader.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""将id格式的输入转换成dataset，并做动态padding"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, TabularDataset</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> BucketIterator</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">x_tokenize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 如果加载进来的是已经转成id的文本</span></span><br><span class="line">    <span class="comment"># 此处必须将字符串转换成整型</span></span><br><span class="line">    <span class="keyword">return</span> [int(c) <span class="keyword">for</span> c <span class="keyword">in</span> x.split()]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">y_tokenize</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(y)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchIterator</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_path, valid_path,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size, fix_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 x_var=<span class="string">"text"</span>, y_var=[<span class="string">"label"</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 format=<span class="string">'tsv'</span>)</span>:</span></span><br><span class="line">        self.train_path = train_path</span><br><span class="line">        self.valid_path = valid_path</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.fix_length = fix_length</span><br><span class="line">        self.format = format</span><br><span class="line">        self.x_var = x_var</span><br><span class="line">        self.y_vars = y_var</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line">        TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=x_tokenize,</span><br><span class="line">                     use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>,</span><br><span class="line">                     fix_length=self.fix_length,   <span class="comment">#  如需静态padding,则设置fix_length, 但要注意要大于文本最大长度</span></span><br><span class="line">                     eos_token=<span class="keyword">None</span>, init_token=<span class="keyword">None</span>,</span><br><span class="line">                     include_lengths=<span class="keyword">True</span>, pad_token=<span class="number">0</span>)</span><br><span class="line">        LABEL = Field(sequential=<span class="keyword">False</span>, tokenize=y_tokenize, use_vocab=<span class="keyword">False</span>, batch_first=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        fields = [</span><br><span class="line">            (<span class="string">"label"</span>, LABEL), (<span class="string">"text"</span>, TEXT)]</span><br><span class="line"></span><br><span class="line">        train, valid = TabularDataset.splits(</span><br><span class="line">            path=config.ROOT_DIR,</span><br><span class="line">            train=self.train_path, validation=self.valid_path,</span><br><span class="line">            format=<span class="string">'tsv'</span>,</span><br><span class="line">            skip_header=<span class="keyword">False</span>,</span><br><span class="line">            fields=fields)</span><br><span class="line">        <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_iterator</span><span class="params">(self, train, valid)</span>:</span></span><br><span class="line">        train_iter, val_iter = BucketIterator.splits((train, valid),</span><br><span class="line">                                                     batch_sizes=(self.batch_size, self.batch_size),</span><br><span class="line">                                                     device = torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">                                                     sort_key=<span class="keyword">lambda</span> x: len(x.text), <span class="comment"># field sorted by len</span></span><br><span class="line">                                                     sort_within_batch=<span class="keyword">True</span>,</span><br><span class="line">                                                     repeat=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">        train_iter = BatchWrapper(train_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">        val_iter = BatchWrapper(val_iter, x_var=self.x_var, y_vars=self.y_vars)</span><br><span class="line">        <span class="comment">### batch = iter(train_iter)</span></span><br><span class="line">        <span class="comment">### batch： ((text, length), y)</span></span><br><span class="line">        <span class="keyword">return</span> train_iter, val_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""对batch做个包装，方便调用，可选择性使用"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                temp = [getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars]</span><br><span class="line">                y = torch.cat(temp, dim=<span class="number">1</span>).long()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'BatchWrapper: invalid label'</span>)</span><br><span class="line">            text = x[<span class="number">0</span>]</span><br><span class="line">            length = x[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> (text, y, length)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    bi = BatchIterator(config.TRAIN_FILE, config.VALID_FILE, batch_size=<span class="number">1</span>, fix_length=<span class="keyword">None</span>)</span><br><span class="line">    train, valid  = bi.create_dataset()</span><br><span class="line">    train_iter, valid_iter = bi.get_iterator(train, valid)</span><br><span class="line">    batch = next(iter(train_iter))</span><br><span class="line">    print(train_iter)</span><br><span class="line">    print(<span class="string">'batch:\n'</span>, batch)</span><br><span class="line">    print(<span class="string">'batch_text:\n'</span>, batch.text)</span><br><span class="line">    print(<span class="string">'batch_label:\n'</span>, batch.label)</span><br></pre></td></tr></table></figure><p>config.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_FILE = <span class="string">'outputs/intermediate/train.tsv'</span></span><br><span class="line">VALID_FILE = <span class="string">'outputs/intermediate/valid.tsv'</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;本文主要介绍如何使用Torchtext读取文本数据集。&lt;/p
      
    
    </summary>
    
      <category term="NLP" scheme="http://state-of-art.top/categories/NLP/"/>
    
    
      <category term="torchtext，pytorch" scheme="http://state-of-art.top/tags/torchtext%EF%BC%8Cpytorch/"/>
    
  </entry>
  
  <entry>
    <title>文本预处理</title>
    <link href="http://state-of-art.top/2018/11/28/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://state-of-art.top/2018/11/28/文本预处理/</id>
    <published>2018-11-28T15:21:08.000Z</published>
    <updated>2018-11-28T14:14:22.908Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本文介绍如何利用有限内存进行大规模数据处理，主要包括：</p><ul><li>建立词典</li><li>将单词转换为id</li><li>训练集验证集切分</li></ul><h2 id="How-To-Do-IT"><a href="#How-To-Do-IT" class="headerlink" title="How To Do IT"></a>How To Do IT</h2><h3 id="原始数据集"><a href="#原始数据集" class="headerlink" title="原始数据集"></a>原始数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">11</span>@成都 高新技术 产业 开发区 人民 检察院 指控 ， <span class="number">2015</span>年 <span class="number">3</span> 月 <span class="number">29</span>日 <span class="number">23</span>时 许 ， 被告人 刘某 某 饮 酒后 驾驶川 A ＊ ＊ ＊ <span class="number">84</span> 北京 现代牌 小型 轿车 ， 从 成都市 桐梓林 附近 出发 上 人民 南 路 出 城 ， 当 车 行驶 至 成都 高新区 天府 大道 与 府城 大道 交叉 路口处 时 ，  公诉 机关 认为 ， 被告人 刘 某某 在 道路 上 醉 酒 驾驶 机动车 ， 危害 公共 安全 ， 其 行为 应当 以 ×× 追究 其 刑事 责任 。</span><br><span class="line"></span><br><span class="line"><span class="number">11</span>@黑龙江省 尚志市 人民 检察院 指控 ： ×× <span class="number">2014</span>年 <span class="number">9</span> 月 <span class="number">22</span>日 <span class="number">20</span>时 许 ， 被告人 矫 <span class="number">2</span> 某 在 尚志市 苇河镇 阿里郎歌厅 对面 停放 的 货车 的 副 驾驶 座位 上 ， 将 被害人 李某 甲 的 蓝色 女式 拎 包 盗 走 ， 包 内 有 人民币 <span class="number">57000</span> 元 ， 红色 钱包 一个 ， 农业 银行卡 一 张 ， 身份证 一 张 、 驾驶证 一 本 、 账本 一 册 。 案 发 前 ， 被告人 矫 <span class="number">2</span> 某 将 盗走 的 财物 返还 被害人 。  在 ×× 到 五 年 幅度 内 量刑 ， 并 处 罚金 ； 对 所 犯 的 ×× 在 ×× 到 六 个 月 幅度 内 量刑 ， 并 处 罚金 。 针对 上述 指控 ， 公诉 机关 提供 了 相应 的 证据 。</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这里以<a href="http://cail.cipsc.org.cn/" target="_blank" rel="noopener">法研杯</a>比赛的文本数据集为例。格式为 标签@文本</p><p>其中，文本已经过分词处理，使用空格分隔。</p><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent_label_split</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    句子处理成单词</span></span><br><span class="line"><span class="string">    :param line: 原始行</span></span><br><span class="line"><span class="string">    :return: 单词， 标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    line = line.strip(<span class="string">'\n'</span>).split(<span class="string">'@'</span>)</span><br><span class="line">    label = line[<span class="number">0</span>]</span><br><span class="line">    sent = line[<span class="number">1</span>].split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">return</span> sent, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_to_id</span><span class="params">(word, word2id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单词--&gt;ID</span></span><br><span class="line"><span class="string">    :param word: 单词</span></span><br><span class="line"><span class="string">    :param word2id: word2id @type: dict</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> word2id[word] <span class="keyword">if</span> word <span class="keyword">in</span> word2id <span class="keyword">else</span> word2id[<span class="string">'unk'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_vocab</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_word_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    建立词典</span></span><br><span class="line"><span class="string">    :param vocab_size: 词典大小</span></span><br><span class="line"><span class="string">    :param min_freq: 最小词频限制</span></span><br><span class="line"><span class="string">    :param stop_list: 停用词 @type：file_path</span></span><br><span class="line"><span class="string">    :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试</span></span><br><span class="line"><span class="string">    :return: word2id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    count = Counter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        logger.info(<span class="string">'Building vocab'</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'Build vocab'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            count.update(words)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> stop_word_list:</span><br><span class="line">        stop_list = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fr):</span><br><span class="line">                    word = line.strip(<span class="string">'\n'</span>)</span><br><span class="line">                    <span class="keyword">if</span> stop_list.get(word) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                        stop_list[word] = i</span><br><span class="line">        count = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> count.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> stop_list&#125;</span><br><span class="line">    count = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 词典</span></span><br><span class="line">    vocab = [w[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> count <span class="keyword">if</span> w[<span class="number">1</span>] &gt;= min_freq]</span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; len(vocab):</span><br><span class="line">        vocab = vocab[:vocab_size]</span><br><span class="line">    vocab = config.flag_words + vocab</span><br><span class="line">    logger.info(<span class="string">'vocab_size is %d'</span>%len(vocab))</span><br><span class="line">    <span class="comment"># 词典到编号的映射</span></span><br><span class="line">    word2id = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> zip(vocab, range(<span class="number">0</span>, len(vocab)))&#125;</span><br><span class="line">    <span class="keyword">assert</span> word2id[<span class="string">'&lt;pad&gt;'</span>] == <span class="number">0</span>, <span class="string">"ValueError: '&lt;pad&gt;' id is not 0"</span></span><br><span class="line">    print(word2id)</span><br><span class="line">    <span class="keyword">with</span> open(config.WORD2ID_FILE, <span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        pickle.dump(word2id, fw)</span><br><span class="line">    <span class="keyword">return</span> word2id</span><br></pre></td></tr></table></figure><h3 id="文本映射到Id"><a href="#文本映射到Id" class="headerlink" title="文本映射到Id"></a>文本映射到Id</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text2id</span><span class="params">(word2id, maxlen=None, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集文本转ID</span></span><br><span class="line"><span class="string">    :param valid_size: 验证集大小</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))</span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Text to id file existed'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    logger.info(<span class="string">'Text to id'</span>)</span><br><span class="line">    sentences, labels, lengths = [], [], []</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'text_to_id'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            sent = [word_to_id(word=word, word2id=word2id) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">            <span class="keyword">if</span> maxlen:</span><br><span class="line">                sent = sent[:maxlen]</span><br><span class="line">            length = len(sent)</span><br><span class="line">            sentences.append(sent)</span><br><span class="line">            labels.append(label)</span><br><span class="line">            lengths.append(length)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    train, valid = train_val_split(sentences, labels,</span><br><span class="line">                                   valid_size=valid_size,</span><br><span class="line">                                   random_state=random_state,</span><br><span class="line">                                   shuffle=shuffle)</span><br><span class="line">    <span class="keyword">del</span> sentences, labels, lengths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing train to file done'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.VALID_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing valid to file done'</span>)</span><br></pre></td></tr></table></figure><h3 id="训练集验证集分割"><a href="#训练集验证集分割" class="headerlink" title="训练集验证集分割"></a>训练集验证集分割</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_split</span><span class="params">(X, y, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集验证集分割</span></span><br><span class="line"><span class="string">    :param X: sentences</span></span><br><span class="line"><span class="string">    :param y: labels</span></span><br><span class="line"><span class="string">    :param random_state: 随机种子</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger.info(<span class="string">'train val split'</span>)</span><br><span class="line">    data = [(data_x, data_y) <span class="keyword">for</span> data_x, data_y <span class="keyword">in</span> zip(X, y)]</span><br><span class="line">    N = len(data)</span><br><span class="line">    test_size = int(N * valid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.seed(random_state)</span><br><span class="line">        random.shuffle(data)</span><br><span class="line"></span><br><span class="line">    valid = data[:test_size]</span><br><span class="line">    train = data[test_size:]</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">from</span> Logginger <span class="keyword">import</span> init_logger</span><br><span class="line"></span><br><span class="line">logger = init_logger(<span class="string">"torch"</span>, logging_path=config.LOG_PATH)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent_label_split</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    句子处理成单词</span></span><br><span class="line"><span class="string">    :param line: 原始行</span></span><br><span class="line"><span class="string">    :return: 单词， 标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    line = line.strip(<span class="string">'\n'</span>).split(<span class="string">'@'</span>)</span><br><span class="line">    label = line[<span class="number">0</span>]</span><br><span class="line">    sent = line[<span class="number">1</span>].split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">return</span> sent, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_to_id</span><span class="params">(word, word2id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    单词--&gt;ID</span></span><br><span class="line"><span class="string">    :param word: 单词</span></span><br><span class="line"><span class="string">    :param word2id: word2id @type: dict</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> word2id[word] <span class="keyword">if</span> word <span class="keyword">in</span> word2id <span class="keyword">else</span> word2id[<span class="string">'unk'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_vocab</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_word_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    建立词典</span></span><br><span class="line"><span class="string">    :param vocab_size: 词典大小</span></span><br><span class="line"><span class="string">    :param min_freq: 最小词频限制</span></span><br><span class="line"><span class="string">    :param stop_list: 停用词 @type：file_path</span></span><br><span class="line"><span class="string">    :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试</span></span><br><span class="line"><span class="string">    :return: word2id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    count = Counter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        logger.info(<span class="string">'Building vocab'</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'Build vocab'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            count.update(words)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> stop_word_list:</span><br><span class="line">        stop_list = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fr):</span><br><span class="line">                    word = line.strip(<span class="string">'\n'</span>)</span><br><span class="line">                    <span class="keyword">if</span> stop_list.get(word) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                        stop_list[word] = i</span><br><span class="line">        count = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> count.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> stop_list&#125;</span><br><span class="line">    count = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 词典</span></span><br><span class="line">    vocab = [w[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> count <span class="keyword">if</span> w[<span class="number">1</span>] &gt;= min_freq]</span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; len(vocab):</span><br><span class="line">        vocab = vocab[:vocab_size]</span><br><span class="line">    vocab = config.flag_words + vocab</span><br><span class="line">    logger.info(<span class="string">'vocab_size is %d'</span>%len(vocab))</span><br><span class="line">    <span class="comment"># 词典到编号的映射</span></span><br><span class="line">    word2id = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> zip(vocab, range(<span class="number">0</span>, len(vocab)))&#125;</span><br><span class="line">    <span class="keyword">assert</span> word2id[<span class="string">'&lt;pad&gt;'</span>] == <span class="number">0</span>, <span class="string">"ValueError: '&lt;pad&gt;' id is not 0"</span></span><br><span class="line">    print(word2id)</span><br><span class="line">    <span class="keyword">with</span> open(config.WORD2ID_FILE, <span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        pickle.dump(word2id, fw)</span><br><span class="line">    <span class="keyword">return</span> word2id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_split</span><span class="params">(X, y, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集验证集分割</span></span><br><span class="line"><span class="string">    :param X: sentences</span></span><br><span class="line"><span class="string">    :param y: labels</span></span><br><span class="line"><span class="string">    :param random_state: 随机种子</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger.info(<span class="string">'train val split'</span>)</span><br><span class="line">    data = [(data_x, data_y) <span class="keyword">for</span> data_x, data_y <span class="keyword">in</span> zip(X, y)]</span><br><span class="line">    N = len(data)</span><br><span class="line">    test_size = int(N * valid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.seed(random_state)</span><br><span class="line">        random.shuffle(data)</span><br><span class="line"></span><br><span class="line">    valid = data[:test_size]</span><br><span class="line">    train = data[test_size:]</span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text2id</span><span class="params">(word2id, maxlen=None, valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练集文本转ID</span></span><br><span class="line"><span class="string">    :param valid_size: 验证集大小</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))</span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Text to id file existed'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    logger.info(<span class="string">'Text to id'</span>)</span><br><span class="line">    sentences, labels, lengths = [], [], []</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.RAW_DATA), <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(fr, desc=<span class="string">'text_to_id'</span>):</span><br><span class="line">            words, label = sent_label_split(line)</span><br><span class="line">            sent = [word_to_id(word=word, word2id=word2id) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">            <span class="keyword">if</span> maxlen:</span><br><span class="line">                sent = sent[:maxlen]</span><br><span class="line">            length = len(sent)</span><br><span class="line">            sentences.append(sent)</span><br><span class="line">            labels.append(label)</span><br><span class="line">            lengths.append(length)</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> is_debug:</span><br><span class="line">                limit_train_size = <span class="number">10000</span></span><br><span class="line">                <span class="keyword">if</span> size &gt; limit_train_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    train, valid = train_val_split(sentences, labels,</span><br><span class="line">                                   valid_size=valid_size,</span><br><span class="line">                                   random_state=random_state,</span><br><span class="line">                                   shuffle=shuffle)</span><br><span class="line">    <span class="keyword">del</span> sentences, labels, lengths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.TRAIN_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing train to file done'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(config.VALID_FILE, <span class="string">'w'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        <span class="keyword">for</span> sent, label <span class="keyword">in</span> train:</span><br><span class="line">            sent = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> sent]</span><br><span class="line">            line = <span class="string">"\t"</span>.join[str(label), <span class="string">" "</span>.join(sent)]</span><br><span class="line">            fw.write(line + <span class="string">'\n'</span>)</span><br><span class="line">        logger.info(<span class="string">'Writing valid to file done'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能整合，提供给外部调用的函数接口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_helper</span><span class="params">(vocab_size, min_freq=<span class="number">3</span>, stop_list=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                valid_size=<span class="number">0.3</span>, random_state=<span class="number">2018</span>, shuffle=True, is_debug=False)</span>:</span></span><br><span class="line">    <span class="comment"># 判断文件是否已存在</span></span><br><span class="line">    <span class="keyword">if</span> len(glob(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE))) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.info(<span class="string">'Word to id file existed'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE), <span class="string">'rb'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">            word2id = pickle.load(fr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        word2id = bulid_vocab(vocab_size=vocab_size, min_freq=min_freq, stop_word_list=stop_list,</span><br><span class="line">                is_debug=is_debug)</span><br><span class="line">    text2id(word2id, valid_size=valid_size, random_state=random_state, shuffle=shuffle, is_debug=is_debug)</span><br></pre></td></tr></table></figure><p>config.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------PATH------------</span></span><br><span class="line">ROOT_DIR = <span class="string">'/home/daizelin/pytorch/'</span></span><br><span class="line">RAW_DATA = <span class="string">'data/data_for_test.csv'</span></span><br><span class="line">TRAIN_FILE = <span class="string">'outputs/intermediate/train.tsv'</span></span><br><span class="line">VALID_FILE = <span class="string">'outputs/intermediate/valid.tsv'</span></span><br><span class="line">LOG_PATH = <span class="string">'outputs/logs'</span></span><br><span class="line">is_debug = <span class="keyword">False</span></span><br><span class="line">flag_words = [<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;unk&gt;'</span>]</span><br></pre></td></tr></table></figure><p>Logginger.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> logging <span class="keyword">import</span> Logger</span><br><span class="line"><span class="keyword">from</span> logging.handlers <span class="keyword">import</span> TimedRotatingFileHandler</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">使用方式</span></span><br><span class="line"><span class="string">from you_logging_filename.py import init_logger</span></span><br><span class="line"><span class="string">logger = init_logger("dataset",logging_path='')</span></span><br><span class="line"><span class="string">def you_function():</span></span><br><span class="line"><span class="string">logger.info()</span></span><br><span class="line"><span class="string">logger.error()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">日志模块</span></span><br><span class="line"><span class="string">1. 同时将日志打印到屏幕跟文件中</span></span><br><span class="line"><span class="string">2. 默认值保留近7天日志文件</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_logger</span><span class="params">(logger_name, logging_path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> logger_name <span class="keyword">not</span> <span class="keyword">in</span> Logger.manager.loggerDict:</span><br><span class="line">        logger  = logging.getLogger(logger_name)</span><br><span class="line">        logger.setLevel(logging.DEBUG)</span><br><span class="line">        handler = TimedRotatingFileHandler(filename=logging_path+<span class="string">"/all.log"</span>,when=<span class="string">'D'</span>,backupCount = <span class="number">7</span>)</span><br><span class="line">        datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">        format_str = <span class="string">'[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'</span></span><br><span class="line">        formatter = logging.Formatter(format_str,datefmt)</span><br><span class="line">        handler.setFormatter(formatter)</span><br><span class="line">        handler.setLevel(logging.INFO)</span><br><span class="line">        logger.addHandler(handler)</span><br><span class="line">        console= logging.StreamHandler()</span><br><span class="line">        console.setLevel(logging.INFO)</span><br><span class="line">        console.setFormatter(formatter)</span><br><span class="line">        logger.addHandler(console)</span><br><span class="line"></span><br><span class="line">        handler = TimedRotatingFileHandler(filename=logging_path+<span class="string">"/error.log"</span>,when=<span class="string">'D'</span>,backupCount=<span class="number">7</span>)</span><br><span class="line">        datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">        format_str = <span class="string">'[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'</span></span><br><span class="line">        formatter = logging.Formatter(format_str,datefmt)</span><br><span class="line">        handler.setFormatter(formatter)</span><br><span class="line">        handler.setLevel(logging.ERROR)</span><br><span class="line">        logger.addHandler(handler)</span><br><span class="line">    logger = logging.getLogger(logger_name)</span><br><span class="line">    <span class="keyword">return</span> logger</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本
      
    
    </summary>
    
      <category term="NLP" scheme="http://state-of-art.top/categories/NLP/"/>
    
    
      <category term="文本预处理" scheme="http://state-of-art.top/tags/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>周期性学习率(Cyclical Learning Rate)技术</title>
    <link href="http://state-of-art.top/2018/10/28/%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%8A%80%E6%9C%AF/"/>
    <id>http://state-of-art.top/2018/10/28/周期性学习率技术/</id>
    <published>2018-10-28T15:21:08.000Z</published>
    <updated>2018-10-30T14:25:09.279Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍神经网络训练中的周期性学习率技术。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>学习率(learning_rate, LR)是神经网络训练过程中最重要的超参数之一，它对于快速、高效地训练神经网络至关重要。简单来说，LR决定了我们当前的权重参数朝着降低损失的方向上改变多少。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_weight = exsiting_weight - learning_rate * gradient</span><br></pre></td></tr></table></figure><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-neural_network-7.png" alt="image text"></p><p><em><center>Fig.: A simple neural network where the w’s and b’s are to be learnt (Img Credit: Matt Mazur)</center></em></p><p>这看上去很简单。但是正如许多研究显示的那样，单单通过提升这一步就会对我们的训练产生深远的影响，并且尚有很大的优化空间。</p><p>本文介绍了一种叫做周期性学习率（CLR）的技术，它是一种非常新的、简单的想法，用来设置和控制训练过程中LR的大小。该技术在<a href="https://twitter.com/jeremyphoward" target="_blank" rel="noopener">jeremyphoward</a>今年的<a href="http://www.fast.ai/" target="_blank" rel="noopener">fast.ai course</a>课程中提及过。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>神经网络用来完成某项任务需要对大量参数进行训练。参数训练意味着寻找合适的一些参数，使得在每个batch训练完成后损失（loss）达到最小，而参数更新的方式则与LR密切相关。</p><p>通常来说，有两种广泛使用的方法用来设置训练过程中的LR。</p><h3 id="One-LR-for-all-parameters"><a href="#One-LR-for-all-parameters" class="headerlink" title="One LR for all parameters"></a>One LR for all parameters</h3><p>一个典型的例子是<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">SGD</a>， 在训练开始时设置一个LR常量，并且设定一个LR衰减策略（如step，exponential等）。这个单一的LR用来更新所有的参数。在每个epochs中，LR按预先设定随时间逐渐衰减，当我们临近最小损失时， 通过衰减可以减缓更新，以防止我们越过最小值。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-learningrates.jpeg" alt="image text"></p><p><em><center>Fig. Effect of various learning rates on convergence (Img Credit: cs231n)</center></em></p><p>该方法存在如下挑战(<a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">refer</a>)：</p><ol><li>难以选择初始的LR达到想要的效果（如上图所示）；</li><li>LR衰减策略同样难以设定，他们很难自适应动态变化的数据；</li><li>所有的参数使用相同的LR进行更新，而这些参数可能学习速率不完全相同；</li><li>很容易陷入马鞍点不能自拔</li></ol><h3 id="Adaptive-LR-for-each-parameter"><a href="#Adaptive-LR-for-each-parameter" class="headerlink" title="Adaptive LR for each parameter"></a>Adaptive LR for each parameter</h3><p>一些改进的优化器如<em>AdaGrad</em>, <em>AdaDelta</em>, <em>RMSprop</em> and <em>Adam</em> 很大程度上缓解了上述困难，方法是对每个参数采用不同的自适应学习率。比如AdaDelta，它的更新机制甚至不需要我们主动设置默认的学习率。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/%E8%87%AA%E9%80%82%E5%BA%94SGD%E6%96%B9%E6%B3%95.gif" alt="image text"></p><p><em><center>Fig: Animation comparing optimization algorithms (Img Credit: Alec Radford)</center></em></p><h2 id="Cycling-Learning-Rate"><a href="#Cycling-Learning-Rate" class="headerlink" title="Cycling Learning Rate"></a>Cycling Learning Rate</h2><p>CLR是Leslie Smith于2015年提出的。这是一种调节LR的方法，在该方法中，设定一个LR上限和下限，LR的值在上限和下限的区间里周期性地变化。看上去，LCR似乎是自适应LR技术和SGD的竞争者，事实上，CLR技术是可以和上述提到的改进的优化器一起使用来进行参数更新的。</p><p>而在计算上，CLR比上述提到的改进的优化器更容易实现，正如文献[1]所述：</p><p><em>Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates, as shown in Section 4.1. In addition, CLR policies are computationally simpler than adaptive learning rates. CLR is likely most similar to the SGDR method that appeared recently.</em></p><h3 id="Why-it-works"><a href="#Why-it-works" class="headerlink" title="Why it works"></a>Why it works</h3><p>直觉上看，随着训练进度的增加我们应该保持学习率一直减小以便于在某一时刻达到收敛。</p><p>然而，事实恰与直觉相反，使用一个在给定区间里周期性变化的LR可能更有用处。原因是周期性高的学习率能够使模型跳出在训练过程中遇到的局部最低点和马鞍点。事实上，Dauphin等[3]指出相比于局部最低点，马鞍点更加阻碍收敛。如果马鞍点正好发生在一个巧妙的平衡点，小的学习率通常不能产生足够大的梯度改变使其跳过该点（即使跳过，也需要花费很长时间）。这正是周期性高学习率的作用所在，它能够更快地跳过马鞍点。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/%E9%A9%AC%E9%9E%8D%E7%82%B9.gif" alt="image text"></p><p><em><center>Fig.: A saddle point in the error surface (Img Credit: safaribooksonline)</center></em></p><p>另外一个好处是，最优的LR肯定落在最小值和最大值之间。换言之，我们确实在迭代过程中使用了最好的LR。</p><h4 id="Epoch，iterations-cycles-and-stepsize"><a href="#Epoch，iterations-cycles-and-stepsize" class="headerlink" title="Epoch，iterations, cycles and stepsize"></a>Epoch，iterations, cycles and stepsize</h4><p>首先介绍几个术语，理解这些术语可以更好地理解下面描述的算法和公式。</p><p>我们现在考虑一个包含50000个样本的训练集。</p><p>一个epoch是至将整个训练集训练一轮。如果我们将batch_size, 我们在一个epoch里会得到500个batch或者叫iteration。iteration的数目随着epoch的增加不断积累，在第二个epoch，对应着501到1000次iteration，后面的以此类推。</p><p>一个cycle定义为学习率从低到高，然后从高到低走一轮所用的iteration数。而stepsize指的是cycle迭代步数的一半。注意，cycle不一定必须和epoch相同，但实践上通常将cycle和epoch对应相同的iteration。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-triangle.png" alt="image text"></p><p><em><center>Fig: Triangular LR policy. (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在上图中，两条红线分别表示学习率最小值（base lr）和学习率最大值（max lr）。蓝色的线是学习率随着iteration改变的方式。蓝线上下一次表示一个cycle，stepsize则是其一半。</p><h3 id="Calculating-the-LR"><a href="#Calculating-the-LR" class="headerlink" title="Calculating the LR"></a>Calculating the LR</h3><p>综上所述，接下来我们需要参数作为该算法的输入：</p><ul><li><p>stepsize</p></li><li><p>base_lr</p></li><li><p>max_lr</p></li></ul><p>下面是LR更新的一段代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_triangular_lr</span><span class="params">(iteration, stepsize, base_lr, max_lr)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given the inputs, calculates the lr that should be</span></span><br><span class="line"><span class="string">    applicable for this iteration</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cycle = np.floor(<span class="number">1</span> + iteration/(<span class="number">2</span>  * stepsize))</span><br><span class="line">    x = np.abs(iteration/stepsize - <span class="number">2</span> * cycle + <span class="number">1</span>)</span><br><span class="line">    lr = base_lr + (max_lr - base_lr) * np.maximum(<span class="number">0</span>, (<span class="number">1</span>-x))</span><br><span class="line">    <span class="keyword">return</span> lr</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># Demo of how the LR varies with iterations</span></span><br><span class="line">    num_iterations = <span class="number">10000</span></span><br><span class="line">    stepsize = <span class="number">1000</span></span><br><span class="line">    base_lr = <span class="number">0.0001</span></span><br><span class="line">    max_lr = <span class="number">0.001</span></span><br><span class="line">    lr_trend = list()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        lr = get_triangular_lr(iteration, stepsize, base_lr, max_lr)</span><br><span class="line">        <span class="comment"># Update your optimizer to use this learning rate in this iteration</span></span><br><span class="line">        lr_trend.append(lr)</span><br><span class="line">    </span><br><span class="line">    plt.plot(lr_trend)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如下图所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-graph.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration. We are using the triangular profile.</center></em></p><h3 id="Deriving-the-optimal-base-lr-and-max-lr"><a href="#Deriving-the-optimal-base-lr-and-max-lr" class="headerlink" title="Deriving the optimal base lr and max lr"></a>Deriving the optimal base lr and max lr</h3><p>对于给定的数据集，怎么确定合理的base lr 和max lr呢？</p><p>答案是先跑几个epoch，并且让学习率线性增加，观察准确率的变化，从中选出合适的base 和max lr。</p><p>我们让学习率按照上面的斜率进行增长，跑了几轮，结果如下图所示。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-deciding-baselr-maxlr.png" alt="image text"></p><p><em><center>Fig: Plot of accuracy vs learning rate (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>可以看出，开始的时候，准确率随着学习率的增加而增加，然后进入平缓起期，然后又开始减小，出现震荡。注意图中准确率开始增长的那一点和达到平衡的那一点（图中红色箭头所示）。这两个点可以作为比较好的base lr 和 max lr。当然，你也可以选择平衡点旁边的准确率峰值点作为max lr， 把base lr 设为其1/3 或者1/4。</p><p>好了，三个参数中已经有两个确定了，那么怎么确定stepsize呢？</p><p>已经有论文做过实验，他们将stepsize设成一个epoch包含的iteration数量的2-10倍。拿我们之前举的例子来说，我们一个epoch包含500个iteration，那么stepsize就设成1000-5000。该论文实验表明，stepsize设成2倍或者10倍，两者结果并没有太大的不同。</p><h3 id="Variants"><a href="#Variants" class="headerlink" title="Variants"></a>Variants</h3><p>上面我们实现的算法中，学习率是按照三角的规律周期性变化。除了这种以外，还有其他几种不同的函数形式。</p><p><strong><em>traiangular2：</em></strong>这里max lr 按cycle进行对半衰减。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-triangular2.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration for the triangular2 approach (Img Credit: Brad Kenstler)</center></em></p><p><strong><em>exp_range：</em></strong>这里max lr按iteration进行指数衰减。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-exp_range.png" alt="image text"></p><p><em><center>Fig: Graph showing the variation of lr with iteration for the exp-range approach (Img Credit: Brad Kenstler)</center></em></p><p>这些与固定学习率的指数衰减（exponential decay）相比，有论文表明效果都得到了明显的提升。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>如下图所示，在某神经网络上，CLR提供了一个快速的收敛，因此它的确值得一试。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-cifar10.png" alt="image text"></p><p><em><center>Fig. CLR tested on CIFAR 10 (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在上图的试验中，CLR花了25K次迭代达到了81%的准确率，传统的LR更新方法大约需要70K才能达到同样的水平。</p><p><img src="https://blog-1257937792.cos.ap-chengdu.myqcloud.com/blog_2/15-clr-adam.png" alt="image text"></p><p><em><center>Fig. CLR used with Nesterov and Adam. Much faster convergence with Nesterov (Nesterov is an improvement over SGD) (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a>)</center></em></p><p>在另一项试验中，如上图所示，CLR + Nesterov优化器比著名的Adam收敛的还要快。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>CLR带来了一种新的方案来控制学习率的更新，它可以与SGD以及一些更加高级的优化器上一起使用。CLR应该成为每一个深度学习实践者工具箱里的一项技术。</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol><li><a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">Cyclical Learning Rates for Training Neural Networks, Smith</a></li><li><a href="https://arxiv.org/pdf/1609.04747.pdf" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms, Rudder</a></li><li>Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization.</li><li><a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">SGDR: Stochastic Gradient Descent with Warm Restarts, Loshchilov, Hutter</a></li><li><a href="https://github.com/bckenstler/CLR" target="_blank" rel="noopener">https://github.com/bckenstler/CLR</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文介绍神经网络训练中的周期性学习率技术。&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;学
      
    
    </summary>
    
      <category term="深度学习" scheme="http://state-of-art.top/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="超参数" scheme="http://state-of-art.top/tags/%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    
      <category term="学习率" scheme="http://state-of-art.top/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>图像数据-TFrecord在动态图中的使用</title>
    <link href="http://state-of-art.top/2018/10/27/TFrecord%E5%9C%A8%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://state-of-art.top/2018/10/27/TFrecord在动态图中的使用/</id>
    <published>2018-10-27T15:21:08.000Z</published>
    <updated>2018-10-28T08:50:46.251Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍图片数据使用TFrecord和tf.data.dataset进行存储和读取。</p><p>Tensorflow 提供了四种数据读取方式：</p><ol><li><p>Preloaded data: 用一个tf.constant常量将数据集加载进来，主要用于很小的数据集；</p></li><li><p>Feeding: 使用python代码供给数据，将所有数据加载进内存，然后一个batch一个batch地输入到计算图中， 适用于小数据集；</p></li><li>QueueRunner: 基于队列的输入通道，读取TFrecord静态图使用；</li><li>tf.data API: 能够从不同的输入或文件格式中读取、预处理数据，并且对数据应用一些变换（例如，batching、shuffling、mapping function over the dataset），tf.data API 是旧的 feeding、QueueRunner的升级。值得注意的是， Eager模式必须使用该API来构建输入通道， 一般结合TFrecord使用。该API相比于Queue更容易使用。</li></ol><h2 id="What‘s-TFrecord"><a href="#What‘s-TFrecord" class="headerlink" title="What‘s TFrecord"></a>What‘s TFrecord</h2><p>TFrecord是Tensorflow提供的一种二进制存储格式，可将数据和标签统一存储。从上述读取方式中可以看出，TFrecord在QueueRunner和tf.data API读取中均扮演了重要的角色。</p><h2 id="Why-TFrecord"><a href="#Why-TFrecord" class="headerlink" title="Why TFrecord"></a>Why TFrecord</h2><p>与其他方案相比， 使用TFrecord读取的优点在于：</p><ol><li>可处理大规模数据量，而不会造成其他方案所带来的内存不够用的问题；</li><li>在Feeding方案中，batch读取的IO操作势必会阻塞训练，前一个batch加载完成后，神经网络必须等待下一个batch加载完成后才能继续训练，效率较低。</li></ol><h2 id="How-To-Use"><a href="#How-To-Use" class="headerlink" title="How To Use"></a>How To Use</h2><p>TFrecord的使用主要有两块：一是图片数据转TFrecord格式存储，二是解析存储好的TFrecord文件。下面逐一介绍。</p><h3 id="图片转TFrecord"><a href="#图片转TFrecord" class="headerlink" title="图片转TFrecord"></a>图片转TFrecord</h3><p>本文使用的数据集是Kaggle猫狗数据集。</p><p>该数据集包含train和test两个文件夹， 分别为训练集和测试集，下面以train集为例操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls |wc -w</span><br><span class="line"></span><br><span class="line">25000</span><br></pre></td></tr></table></figure><p>训练集包含25000张图片，猫狗各一半。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls </span><br><span class="line"></span><br><span class="line">cat.124.jpg    cat.3750.jpg  cat.6250.jpg  cat.8751.jpg  dog.11250.jpg  dog.2500.jpg   dog.5000.jpg  dog.7501.jpg</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>图片文件以jpg格式存储，以cat， dog作为文件名开头。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_tfrecord_encode</span><span class="params">(classes, tfrecord_filename, data_path, is_training=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    功能：读取图片转换成tfrecord格式的文件</span></span><br><span class="line"><span class="string">    @params: classes: 标签类别  @type：classes: dict</span></span><br><span class="line"><span class="string">    @params: tfrecord_filename: tfrecord文件保存文件</span></span><br><span class="line"><span class="string">    @type：tfrecord_filename: str</span></span><br><span class="line"><span class="string">    @params: data_path: 原始训练集存储路径</span></span><br><span class="line"><span class="string">    @is_training: 是否为训练集，用来区分训练集和测试集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 初始化一个writer</span></span><br><span class="line">    writer = tf.python_io.TFRecordWriter(tfrecord_filename)</span><br><span class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> tqdm(os.listdir(path)):</span><br><span class="line">        name = img_name.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 使用tf.gfile.FastFile读取图片要比PIL.Image读取处理得到的</span></span><br><span class="line">        <span class="comment"># 最终TFrecod文件小得多，在本案例中，IMAGE方式读取得到的TFrecord大小约为3.7G</span></span><br><span class="line">        <span class="comment"># 而tf.gfile.FastFile得到的约为548M</span></span><br><span class="line">        <span class="keyword">with</span> tf.gfile.FastGFile(os.path.join(path, img_name), <span class="string">'rb'</span>) <span class="keyword">as</span> gf:</span><br><span class="line">            img = gf.read()</span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            <span class="comment"># 构造特征</span></span><br><span class="line">            feature = &#123;</span><br><span class="line">                <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[classes[name]])),</span><br><span class="line">                <span class="string">'img_raw'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),</span><br><span class="line">                <span class="string">'file_name'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()]))</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature = &#123;</span><br><span class="line">                <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[<span class="number">-1</span>])),</span><br><span class="line">                <span class="string">'img_raw'</span>:tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),</span><br><span class="line">                <span class="string">'file_name'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_name.encode()]))</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="comment"># example 对象将label和image特征进行封装</span></span><br><span class="line">        example = tf.train.Example(features=tf.train.Features(feature=feature))  </span><br><span class="line">        writer.write(example.SerializeToString())   <span class="comment"># 序列化为字符串</span></span><br><span class="line">    writer.close()</span><br><span class="line">    print(<span class="string">'tfrecord writen done!'</span>)</span><br></pre></td></tr></table></figure><p>调用上述函数，可得到猫狗训练集的TFrecord格式文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    classes = &#123;<span class="string">'cat'</span>: <span class="number">0</span>, <span class="string">'dog'</span>: <span class="number">1</span>&#125;</span><br><span class="line">    tfrecord_filename = <span class="string">'cat_and_dog.tfrecord'</span></span><br><span class="line">    data_path = <span class="string">'train/'</span></span><br><span class="line">    img_tfrecord_encode(classes, tfrecord_filename, data_path, is_training=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>上述程序运行大约需要2min。</p><h3 id="使用tf-data读取TFrecord"><a href="#使用tf-data读取TFrecord" class="headerlink" title="使用tf.data读取TFrecord"></a>使用tf.data读取TFrecord</h3><p>在动态图（eager）模式下，QueueRunner不可用，必须使用tf.data进行TFrecord的读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_tfrecord_parse</span><span class="params">(tfrecord_filename, epochs, batch_size, shape,</span></span></span><br><span class="line"><span class="function"><span class="params">                       padded_shapes=None, shuffle=True, buffer_size=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param: tfrecord_filename:tfrecord文件列表   @type:list</span></span><br><span class="line"><span class="string">    @param: epoch:训练轮数（repeating次数）       @type:int</span></span><br><span class="line"><span class="string">    @param：batch_size:批数据大小                @type:int</span></span><br><span class="line"><span class="string">    @param: shape:图片维度                      @type:tuple</span></span><br><span class="line"><span class="string">    @param: padded_shapes:不定长padding        @type:tuple</span></span><br><span class="line"><span class="string">    @param: shuffle:是否打乱                   @type:boolean</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"><span class="comment"># 解析单个example，特征与encode一一对应。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_example</span><span class="params">(serialized_example)</span>:</span></span><br><span class="line">        features = tf.parse_single_example(serialized_example,</span><br><span class="line">                                           features=&#123;</span><br><span class="line">                                               <span class="string">'label'</span>: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">                                               <span class="string">'img_raw'</span>: tf.FixedLenFeature([], tf.string),</span><br><span class="line">                                               <span class="string">'file_name'</span>: tf.FixedLenFeature([], tf.string)</span><br><span class="line">                                           &#125;)</span><br><span class="line">        <span class="comment"># 解码</span></span><br><span class="line">        image = tf.image.decode_jpeg(features[<span class="string">'img_raw'</span>])</span><br><span class="line">        <span class="comment"># 设置shape</span></span><br><span class="line">        image = tf.image.resize_images(image, shape, method=<span class="number">1</span>)</span><br><span class="line">        label = tf.cast(features[<span class="string">'label'</span>], tf.int64)</span><br><span class="line">        file_name = tf.cast(features[<span class="string">'file_name'</span>], tf.string)</span><br><span class="line">        <span class="keyword">return</span> image, label, file_name</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 解析TFrecord</span></span><br><span class="line">    dataset = tf.data.TFRecordDataset(tfrecord_filename).map(parse_example)</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="keyword">if</span> padded_shapes:</span><br><span class="line">            dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).padded_batch(batch_size, padded_shapes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = dataset.repeat(epochs).shuffle(buffer_size=buffer_size).batch(batch_size)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> padded_shapes:</span><br><span class="line">            dataset = dataset.repeat(epochs).padded_batch(batch_size, padded_shapes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = dataset.repeat(epochs).batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><p>调用上述函数，解析TFrecord得到dataset。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>():</span><br><span class="line">    tfrecord_filename = <span class="string">'cat_and_dog.tfrecord'</span></span><br><span class="line">    epochs = <span class="number">100</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    shape = (<span class="number">227</span>, <span class="number">227</span>)</span><br><span class="line">    dataset = img_tfrecord_parse(tfrecord_filename=tfrecord_filename,</span><br><span class="line">                                epochs=epochs,</span><br><span class="line">                                batch_size=batch_size,</span><br><span class="line">                                shape=shape)</span><br><span class="line">    <span class="comment"># 查看dataset</span></span><br><span class="line">    iterator = dataset.make_one_hot_iterator()</span><br><span class="line">    image, label, file_name = iterator.get_next()</span><br><span class="line">    print(image[<span class="number">0</span>])</span><br><span class="line">    print(label[<span class="number">0</span>])</span><br><span class="line">    print(file_name[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文介绍图片数据使用TFrecord和tf.data.dataset进行存储和读取。&lt;/p&gt;
&lt;p&gt;Tensorflow 提供了四种数据读取方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Preloaded data: 用一个tf.constant常量将数据集加载进来，主要用于很小
      
    
    </summary>
    
      <category term="Tensorflow" scheme="http://state-of-art.top/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://state-of-art.top/tags/Tensorflow/"/>
    
  </entry>
  
</feed>
